<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[七月在线-深度学习]]></title>
    <url>%2F2019%2F03%2F25%2F20190325%E4%B8%83%E6%9C%88%E5%9C%A8%E7%BA%BF-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[​ 线性与非线性线性：随机梯度下降，卷积函数非线性：修正线性单元（Relu） Dropout,Boosting,Bagging,Stacking,MappingBagging和DropoutBagging能实现跟神经网络中Dropout类似的效果。Dropout是将许多单独训练的子网络集成起来，某些权值是共享的。Bagging是将许多单独训练的学习机集成起来；Dropout和Bagging这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。 Bagging和Boosting的区别：Boosting并不是单独训练的，而是按照有一定的顺序训练的，具有相互依赖关系。 1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。2）样例权重：Bagging：使用均匀取样，每个样例的权重相等。Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。3）预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。4）并行计算：Bagging：各个预测函数可以并行生成Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果 Stacking是通过两层学习机完成的学习。 处理过拟合的方法Dropout； 调整超参数来最小化代价函数（cost functon）的技术：网格搜索，随机搜索，贝叶斯（bayesian）优化，居于梯度的优化。 批规范化（Batch Normalization）的好处：增加反向传播速度，避免梯度消失；加速网络收敛；减轻参数初始化的影响。批规范化（Batch Normalization）不能处理过拟合，因为同一个数据在不同批中被归一化后的值会有差别，相当于做了数据增强（data augmentation）。]]></content>
      <categories>
        <category>七月在线</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>七月在线</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习2]]></title>
    <url>%2F2018%2F08%2F07%2F20180807Tensorflow%E5%85%A5%E9%97%A82%2F</url>
    <content type="text"><![CDATA[这篇博文主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。 然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。 深度学习模型TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。 基本设置在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。 加载MNIST数据集TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True) 这里mnist是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。 开启TensorFlow会话TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。 这里我们使用了一个更为方便的类，InteractiveSession，这能让你在构建代码时更加灵活。InteractiveSession允许你做一些交互操作，通过创建一个计算流图(computation graph)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用InteractiveSession，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。 下面是如何创建一个InteractiveSession： 12import tensorflow as tfsess = tf.InteractiveSession() 计算流图(Computation Graph)为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了Theano和Torch上。 所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见这里。 Softmax Regression模型见这篇博文。 CNN模型Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。 权值初始化为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的噪声以破坏参数整体的对称性，同时避免梯度为0.由于我们使用ReLU激活函数（详细介绍)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数： 1234567def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 卷积(Convolution)和池化(Pooling)TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式： 123456def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 其中，padding=&#39;SAME&#39;表示通过填充0，使得输入和输出的形状一致。 第一层：卷积层第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。 12W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32]) 为了使得图片与计算层匹配，我们首先reshape输入图像x为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（-1就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几） 1x_image = tf.reshape(x, [-1,28,28,1]) 然后，使用weight tensor对x_image进行卷积计算，加上bias，再应用到一个ReLU激活函数，最终采用最大池化。 12h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1) 第二层：卷积层为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。 12345W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2) 全连接层到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个ReLu函数。 12345W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 整个CNN的网络结构如下图： Dropout为了减少过拟合程度，在输出层之前应用dropout技术（即丢弃某些神经元的输出结果）。我们创建一个placeholder来表示一个神经元的输出在dropout时不被丢弃的概率。Dropout能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的tf.nn.dropout操作能够利用mask技术处理各种规模的神经元输出。 12keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最终，我们用一个softmax层，得到类别上的概率分布。（与之前的Softmax Regression模型相同）。 1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 模型训练和测试为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前Softmax Regression模型中的训练、测试过程类似。区别在于： 用更复杂的ADAM最优化方法代替了之前的梯度下降； 增了额外的参数keep_prob在feed_dict中，以控制dropout的几率； 在训练过程中，增加了log输出功能（每100次迭代输出一次）。 下面是程序： 123456789101112131415cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuracy)) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)) 最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的Softmax Regression模型。 完整代码及运行结果利用CNN模型实现手写体识别的完整代码如下： 123456789101112131415161718__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef weight_varible(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")sess = tf.InteractiveSession()# parasW_conv1 = weight_varible([5, 5, 1, 32])b_conv1 = bias_variable([32])# conv layer-1x = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, [-1, 28, 28, 1])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# conv layer-2W_conv2 = weight_varible([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# full connectionW_fc1 = weight_varible([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)# dropoutkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# output layer: softmaxW_fc2 = weight_varible([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)y_ = tf.placeholder(tf.float32, [None, 10])# model trainingcross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuacy = accuracy.eval(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuacy)) train_step.run(feed_dict = &#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)# accuacy on testprint("test accuracy %g"%(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;))) 运行结果如下图： 参考资料 Tensorflow 实战 Google 深度学习框架TensorFlow——Mnist手写数字识别实战教程]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习]]></title>
    <url>%2F2018%2F08%2F04%2F20180804Tensorflow%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow 简介TensorFlow是Google在2015年11月份开源的人工智能框架（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音、图像、水平等多个领域。官网上对TensorFlow的介绍是，一个使用数据流图(Data Flow Graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点之间的边，代表多维数据(Tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph)数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。这些tensors的flow也就是TensorFlow的命名来源。节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 TensorFlow的特性灵活性TensorFlow不是一个严格的神经网络工具包，如果你可以使用数据流图来描述你的计算过程，那么你可以使用TensorFlow做任何事情。你还可以十分方便地根据需要来构建数据流图，使用简单的Python语言来实现高层次的功能。 可移植性TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。 提高开发效率TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。 支持语言选项目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口） 充分利用硬件资源，最大化计算性能基本使用你需要理解在TensorFlow中，是如何： 将计算流程表示成图； 通过Sessions来执行图计算； 将数据表示为tensors； 使用Variables来保持状态信息； 分别使用feeds和fetches来填充数据和抓取任意的操作结果； 概览TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(Operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。一个Tensor是一个多维数组，例如，你可以将一批图像表示为一个四维的数组[batch, height, width, channels]，数组中的数值类型均为浮点数。TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPU和GPU)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor. 图计算TensorFlow程序中图的创建类似于一个施工阶段，而在执行阶段则利用一个session来执行图中的节点。很常见的情况是，在施工阶段创建一个图来表示和训练神经网络，而在执行阶段在图中重复执行一系列的训练操作。 创建图在TensorFlow中，Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解Graph Class文档。 见下面的示例代码： 123456789101112131415import tensorflow as tf# Create a Constant op that produces a 1x2 matrix. The op is# added as a node to the default graph.# The value returned by the constructor represents the output# of the Constant op.matrix1 = tf.constant([[3., 3.]])# Create another Constant that produces a 2x1 matrix.matrix2 = tf.constant([[2.],[2.]])# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.# The returned value, 'product', represents the result of the matrix# multiplication.product = tf.matmul(matrix1, matrix2) 默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。 在Session中执行图计算见下面的示例代码，更多可了解Session Class： 1234567891011121314151617181920# Launch the default graph.sess = tf.Session()# To run the matmul op we call the session 'run()' method, passing 'product'# which represents the output of the matmul op. This indicates to the call# that we want to get the output of the matmul op back.## All inputs needed by the op are run automatically by the session. They# typically are run in parallel.## The call 'run(product)' thus causes the execution of threes ops in the# graph: the two constants and matmul.## The output of the op is returned in 'result' as a numpy `ndarray` object.result = sess.run(product)print(result)# ==&gt; [[ 12.]]# Close the Session when we're done.sess.close() Sessions最后需要关闭，以释放相关的资源；也可以使用with模块，session在with模块中自动会关闭： 123with tf.Session() as sess: result = sess.run([product]) print(result) TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果想在第二块多余的GPU上执行(GPU默认标记为0)： 123456with tf.Session() as sess: with tf.device("/gpu:1"): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... device中的各个字符串含义如下： “/cpu:0”: 你机器的CPU； “/gpu:0”: 你机器的第一个GPU； “/gpu:1”: 你机器的第二个GPU； 关于Tensorflow中GPU的使用见这里。 交互环境下的使用以上的python示例中，使用了Session和Session.run()来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用InteractiveSession类，以及Tensor.eval()、Operation.run()等方法。例如，在交互的Python环境下执行以下代码： 1234567891011121314151617# Enter an interactive TensorFlow Session.import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# Initialize 'x' using the run() method of its initializer op.x.initializer.run()# Add an op to subtract 'a' from 'x'. Run it and print the resultsub = tf.sub(x, a)print(sub.eval())# ==&gt; [-2. -1.]# Close the Session when we're done.sess.close() TensorsTensorflow中使用tensor数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个tensor具有固定的类型、级别和大小，更加深入理解这些概念可参考Rank, Shape, and Type。 变量(Variables)变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色： 123456789101112131415161718192021222324252627282930# Create a Variable, that will be initialized to the scalar value 0.state = tf.Variable(0, name="counter")# Create an Op to add one to `state`.one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# Variables must be initialized by running an `init` Op after having# launched the graph. We first have to add the `init` Op to the graph.init_op = tf.initialize_all_variables()# Launch the graph and run the ops.with tf.Session() as sess: # Run the 'init' op sess.run(init_op) # Print the initial value of 'state' print(sess.run(state)) # Run the op that updates 'state' and print 'state'. for _ in range(3): sess.run(update) print(sess.run(state))# output:# 0# 1# 2# 3 赋值函数assign()和add()函数类似，直到session的run()之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。 抓取(Fetches)为了抓取ops的输出，需要先执行session的run()函数。然后，通过print函数打印状态信息。 123456789101112input1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.mul(input1, intermed)with tf.Session() as sess: result = sess.run([mul, intermed]) print(result)# output:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 所有tensors的输出都是一次性连贯执行的。 填充(Feeds)TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序： 123456789input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;))# output:# [array([ 14.], dtype=float32)] 如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考MNIST fully-connected feed tutorial (source code)。 示例：曲线拟合下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 简化调用库名import tensorflow as tfimport numpy as np# 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3x_data = np.random.rand(100).astype("float32")y_data = x_data * 0.1 + 0.3# 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值）W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))b = tf.Variable(tf.zeros([1]))y = W * x_data + b# 最小化均方误差loss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# 初始化TensorFlow参数init = tf.initialize_all_variables()# 运行数据流图（注意在这一步才开始执行计算过程）sess = tf.Session()sess.run(init)# 观察多次迭代计算时，w和b的拟合值for step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(W), sess.run(b))# 最好的情况是w和b分别接近甚至等于0.1和0.3# output:# 0 [ 0.59550095] [ 0.06236772]# 20 [ 0.20834503] [ 0.24582449]# 40 [ 0.12388583] [ 0.28805643]# 60 [ 0.10526589] [ 0.29736692]# 80 [ 0.10116094] [ 0.29941952]# 100 [ 0.10025596] [ 0.29987204]# 120 [ 0.10005643] [ 0.29997179]# 140 [ 0.10001245] [ 0.29999378]# 160 [ 0.10000274] [ 0.29999864]# 180 [ 0.10000062] [ 0.29999971]# 200 [ 0.10000014] [ 0.29999995] MNIST手写体识别任务下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。 MNIST数据集介绍MNIST是一个简单的图片数据集（数据集下载地址），包含了大量的数字手写体图片。下面是一些示例图片： MNIST数据集是含标注信息的，以上图片分别代表5，0，4，1。 由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True) MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。 MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用xs和ys分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。 一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下： 但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。 所以MNIST的训练数据集可以是一个形状为55000 * 784位的tensor，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（tensor中的像素值在0到1之间）。如下图： MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示3。所以，mnist.train.labels是一个55000 * 10的二维数组。如下： 以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。 Softmax Regression模型数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9. Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。 Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）： Step 1: add up the evidence of our input being in certain classes;Step 2: convert that evidence into probabilities. 翻译为： 第1步：将我们输入的证据（数据的意思？）加在某些类别中;第2步：将证据（数据的意思？）转换为概率。 为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）： 最后，我们在一个图片类别的evidence中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为 其中，Wi 和 bi 分别为类别 i 的权值和偏置， j 是输入图片 x 的像素索引。然后，我们将得到的evidence值通过一个softmax函数转化为概率值 y: 这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下： 具体计算方式如下 这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程： 如果我们将这个过程公式化，将得到 实际的计算中，我们通常采用矢量计算的方式，如下 也可以简化成 Softmax Regression的程序实现为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。 为了使用TensorFlow，我们需要引用该库函数 1import tensorflow as tf 我们利用一些符号变量来描述交互计算的过程，创建如下 1x = tf.placeholder(tf.float32, [None, 784]) 这里的 x不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。 我们使用Variable(变量)来表示模型中的权值和偏置，这些参数是可变的。如下， 12W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) 这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。 实现softmax regression模型仅需要一行代码，如下 1y = tf.nn.softmax(tf.matmul(x, W) + b) 其中，matmul函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。 模型的训练在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下： 这里y是所预测的概率分布，而 y’ 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数。 为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值， 1y_ = tf.placeholder(tf.float32, [None, 10]) 然后得到交叉熵，即 1cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。 接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用后向传播算法来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下， 1train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多其他的优化算法，仅需要一行代码即可实现调用。 TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化： 1init = tf.initialize_all_variables() 我们可以在一个Session里面运行模型，并且进行初始化： 12sess = tf.Session()sess.run(init) 接下来，进行模型的训练 123for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) 每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。 以上过程称为随机梯度下降，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。 模型的评价怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。 首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下： 1correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。 1accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 最后，我们来获取模型在测试集上的准确率， 1print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。 完整代码及运行结果利用Softmax模型实现手写体识别的完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")x = tf.placeholder(tf.float32, [None, 784])# parasW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))y = tf.nn.softmax(tf.matmul(x, W) + b)y_ = tf.placeholder(tf.float32, [None, 10])# loss funccross_entropy = -tf.reduce_sum(y_ * tf.log(y))train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)# initinit = tf.initialize_all_variables()sess = tf.Session()sess.run(init)# trainfor i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))print("Accuarcy on Test-dataset: ", sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 运行结果如下图：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群上Docker的简单使用]]></title>
    <url>%2F2018%2F07%2F27%2F20180727Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。由于深度学习众多框架如果同时使用会互相影响，在集群上使用docker可以很好地解决这种情况。 用户管理 (管理员权限)添加docker用户组:1sudo groupadd -g 344 docker 添加用户到用户组：1sudo usermod -a -G 用户组 用户 即给用户docker的使用权限，放开权限后，需要重启docker服务。 1service docker restart 从用户组中删除用户1gpasswd -d 用户 用户组 镜像的基本操作列出本地镜像1docker images 各个选项说明: REPOSITORY：表示镜像的仓库源（不唯一） TAG：镜像的标签(不唯一，可以自己设定) IMAGE ID：镜像ID（唯一） CREATED：镜像创建时间 SIZE：镜像大小同一个镜像ID可以有多个仓库源和标签，如图中红框所示。 查找镜像 我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：https://hub.docker.com/我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。 1docker search httpd NAME:镜像仓库源的名称 DESCRIPTION:镜像的描述 OFFICIAL:是否docker官方发布 下载镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。 1docker pull 镜像名(:标签) 设置镜像标签1docker tag 原始镜像名 新镜像名:标签 发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。 删除镜像 当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。 1.若一个镜像有多个标签，我们只想删除已经没用的标签 1docker rmi 仓库源(liufan): 镜像标签(lf) 删除前后 我们发现liufan:lf已经被删除 2.彻底删除镜像 1docker rmi –f 镜像ID（以8c811b4aec35为例）（不建议-f强制删除） 我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签） 3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。（容器会在下面章节有所讲解） 1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除： 若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中） 导入导出镜像导出1docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar) 已经在对应目录生成压缩文件先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像 1docker rmi busybox &amp;&amp; docker images 导入1docker load &lt; (镜像存储位置)/home/lf/aa.tar 我们发现busybox镜像已经成功导入。 注意：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2种方式：1） 通过Dockerfile文件制作镜像（较难）2） 基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单） 容器基本操作 容器是镜像的一个运行实例，它是基于镜像创建的。 新建容器12docker create -it --name lf tensorflowdocker ps -a 可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。使用docker create 命令新建的容器一开始是处于停止状态的，需要用如下命令来启动并进入它。 12docker start lfdocker attach lf 启动容器 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。 1docker run -it --name liufan ubuntu/numpy /bin/bash 上述命令等价于先执行docker create,再执行docker start和docker attach命令。 上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。命令中： -i：表示让容器的标准输入保持打开， -t：让docker分配一个伪终端并绑定到容器的标准输入上， /bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出） 我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。 查看终止删除容器1docker ps // 查看所有正在运行容器 1docker ps -a // 查看所有容器 1docker ps -a -q // 查看所有容器ID 1docker stop containerId // 停止容器运行，containerId 是容器的ID或者名字，一个或多个 1docker rm containerId // 删除容器，containerId 是容器的ID或者名字，一个或多个 可以看到lf、wh这两个容器已经被删除 1docker stop $(docker ps -a -q) // stop停止所有容器 1docker rm $(docker ps -a -q) // 删除所有容器 注意：删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。 进入容器1.attach命令 使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。 2.exec命令 docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash 导入和导出容器1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它导出： 1docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar) 我们将liufan这个容器导出本地并压缩命名为aa.tar文件。 导入：先将liufan容器删除在尝试导入 12docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -adocker import /home/lf/aa.tar test/ubuntu:lf 我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个镜像到本地镜像库。 两者的区别是： docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。docker load：将保存完整记录，体积较大。 代码实例（以Tensorflow为例） 上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。 创建容器1docker run -it --name liufan bluesliuf/tensorflow /bin/bash 我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。 问题：我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。Docker提供了一种方法：挂载。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。挂载本地目录到容器注意：不可先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。创建容器有2种方式1.未调用GPU，在终端输入命令：1docker run -it -v /home/lf/lf/catdogs:/var/catdogs --name liufan bluesliuf/tensorflow /bin/bash2.调用GPU，在终端输入命令：1docker run -it -v /home/lf/lf/catdogs:/var/catdogs -v /usr/local/docker-inspur/nvidia-volumes/volume:/usr/local/nvidia:ro --volume-driver=nvidia-docker --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools --device=/dev/nvidia0 --device=/dev/nvidia1 --device=/dev/nvidia2 --device=/dev/nvidia3 --device=/dev/nvidia4 --device=/dev/nvidia5 --name liufan bluesliuf/tensorflow /bin/bash-v:挂载的命令参数冒号前：本地目录的绝对路径冒号后：容器挂载本地目录的绝对路径调用GPU比不调用GPU多出的部分：表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU，可以不加此部分name: 创建的容器名bluesliuf/tensorflow:基于的镜像不调用GPU（本机）：调用GPU（集群）：可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。运行代码（本机）：注:本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。再去容器内部看，本地的修改已经同步到容器内了。在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。在终端输入命令：python 代码文件名（此处我是training.py） 不调用GPU（本机）： 可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按【ctrl+p】【ctrl+q】，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如： 调用GPU：在后台运行和上面一样，也是利用【ctrl+p】【ctrl+q】。 数据卷挂载Docker针对挂载目录还提供了一种高级的用法。叫数据卷。 数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下：1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。1docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash 2.再创建一个新的容器，来使用这个数据卷。1docker run -it --volumes-from wuhao --name lf bluesliuf/tensorflow /bin/bash –volumes-from用来指定要从哪个数据卷来挂载数据。我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。 通过数据卷挂载目录更具有优势。1） 我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。2） 当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可（类似一个全局变量），而无须一个个修改其他容器的本地挂载目录。 挂载成功后。运行代码步骤与上面一样。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型-GAN]]></title>
    <url>%2F2018%2F07%2F05%2F20180705%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN%2F</url>
    <content type="text"><![CDATA[GAN的来源 2014年Goodfellow提出Generative Adversarial Nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。 GANs简单的想法就是用两个模型， 一个生成模型，一个判别模型。判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。 GAN的基本组成 GAN 模型中的两位博弈方分别由生成式模型（Generative Model）和判别式模型（Discriminative Model）充当。 生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好； 判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。 可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。 上图是GAN网络的流程图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能让判别器输出值为1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，使得判别器输出假数据的值为1是生成器的目标，而对于这些假数据，判别器要尽可能输出0。GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示： 其本质上是两个优化问题，把拆解就如同下面两个公式，上面是优化D的，下面是优化G的。 当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。 GAN的训练过程 下图为GAN的训练过程。生成式对抗网络主要由生成器G和判别器D组成，训练过程如下所述： 输入噪声（隐藏变量）Z 通过生成器G得到x_fake=G(z) 从真实数据集中获取一部分真实数据x_real 将两者混合x=x_fake+x_real 将数据喂入判别部分D，给定标签x_fake=0,x_real=1,这一过程就是简单的二分类 按照分类结果，回传loss 在整个过程中，D要尽可能的使D(G(z))=0,D(x_real)=1（火眼金睛，不错杀也不漏杀）。而G则要使得D(G(z))=1(即让生成的图片以假乱真) GAN的算法流程和动态求解过程如下图所示： 一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。 GAN的网络结构判别器(卷积) 卷积层大家应该都很熟悉了,为了方便说明，定义如下： 二维的离散卷积（N=2） 方形的特征输入（i1=i2=i） 方形的卷积核尺寸（k1=k2=k ） 每个维度相同的步长（s1=s2=s） 每个维度相同的padding (p1=p2=p) 下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)。 从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为： 生成器(反卷积) 在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。对于上述卷积运算，我们把上图所示的3x3卷积核展开成一个如下图所示的[4.16]的稀疏矩阵C，其中非0元素Wi,j表示卷积核的第i行和第j列。 我们再把4x4的输入特征展开成[16,1]的矩阵X，那么Y=CX则是一个[4,1]的输出特征矩阵，把它重新排列成2x2的输出特征就得到最终的结果，从上述分析可以看出卷积层的计算其实是可以转化成矩阵相乘的。值得注意的是，在一些深度学习网络的开源框架中，并不是通过这种转换方法来计算卷积的，因为这个转换会存在很多无用的0乘操作，caffe中具体实现卷积计算的方法可以参考impleming convolution as a matrix multiplication。 通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么我们很容易得到卷积层的反向传播就是和C的转置相乘。 反卷积和卷积的关系如下 反卷积又称transposed（转置） convolution，我们可以看出其实卷积层的前向传播过程就是反卷积层的反向传播过程，卷积层的反向传播过程就是反卷积层的前向传播过程。因为卷积层的前向反向计算分别为乘C和CT,而反卷积层的前向反向计算分别为乘CT和(CT)T,所以他们的前向传播和反向传播刚好交换过来。同样为了说明，定义反卷积操作参数如下： 二维的离散卷积（N=2） 方形的特征输入（i1‘=i2‘=i‘） 方形的卷积核尺寸（k1‘=k2‘=k‘） 每个维度相同的步长（s1‘=s2‘=s‘） 每个维度相同的padding (p1‘=p2‘=p‘)上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为： o′=i′-k′+2p′+1=i′+(k-1)-2p GAN的优点 GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 相比其他所有模型, GAN可以产生更加清晰，真实的样本 GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域 GAN的缺点 训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的 GAN不适合处理离散形式的数据，比如文本 GAN存在训练不稳定、梯度消失、模式崩溃的问题 实例DCGAN网络网络结构 (判别器) 网络结构 (生成器) 二次元动漫人脸（共50个epoch）数据集：51223张动漫人脸，图左为原始数据集，图右为训练过程 训练过程生成效果图如下： 真实人脸（共100个epoch）数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下： 训练过程生成效果图如下]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的发展]]></title>
    <url>%2F2018%2F06%2F28%2F20180628%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[深度学习的发展历程 人工智能（爷爷） 机器学习（爸爸） 深度学习（儿子） 人工智能 远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。 机器学习 机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。 深度学习 深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素： 大数据，用于训练数据的增加； 计算机的算力大大增加，更快的CPU、通用GPU 的出现 上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。输入层输入图片，即像素矩阵；对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。 深度学习常见的编程框架 观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。 另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。 深度学习的应用无人驾驶 深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。 在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。 从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程就是用深度学习完成。 AlphaGo阿尔法狗 阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。 AlphaGo主要包括三个组成部分： 蒙特卡洛搜索树（MonteCarlo tree search，MCTS） 估值网络（Value network） 策略网络（Policy notebook） AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。 人脸识别 人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.将3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。 学习深度学习所需的基础知识 高数（链式求导，偏导，微积分） 线代（各种矩阵变换、线性方程） 概率论（各种统计分布函数，贝叶斯，傅里叶变换） 信息论（熵，相对熵，最大熵模型） 数理统计和参数估计（中心极值定理，矩阵计算，最大似然估计） 机器学习算法（KNN,决策树，SVM） 编程语言（最好是python）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习常见模型-CNN]]></title>
    <url>%2F2018%2F06%2F21%2F20180621%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN%2F</url>
    <content type="text"><![CDATA[CNN的来源 CNN由纽约大学的Yann LeCun于1989年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。 一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。 CNN的网络架构 卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。 卷积层（Conv） 这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”名字来源。在卷积层中，有两个关键操作：局部关联和窗口滑动。局部关联将每个神经元看作一个滤波器（filter），窗口滑动则使filter对局部数据进行计算。除了这两个操作外，还有两个名词：步长和填充值。步长（stride）为窗口一次滑动的长度，而填充值请看下图的例子。比如有一个5x5像素大小的图片，步长取2，那么则有一个像素没有办法获取到，那应该怎么办呢？ 再举一个例子，图片中输入的是一个3x4的矩阵，卷积核是一个2x2的矩阵。我们假设卷积是一次移动一个像素来操作的，那么我们首先对左上角2x2局部矩阵与卷积核进行卷积操作，即各个位置的元素相乘再相加，得到的输出矩阵S的S00的元素值为aw+bx+ey+fz。然后我们将卷积核向右平移一个像素，现在是（b,c,f,g）四个元素构成的矩阵和卷积核来卷进，得到了输出矩阵S的S01的元素，以此类推，可以得到矩阵S的S02，S10，S11，S12的元素，具体过程如下图所示。 再举一个卷积过程的例子如下：我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3*3矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 这里实际输入的是3个5x5的矩阵，在原来输入的周围加上值为0的一层padding，则输入变为如图所示的7x7的矩阵。例子里面使用了两个卷积核，我们先关注与卷积核W0。由于输入的是3个7x7的矩阵，也可以说成7x7x3的张量，所以我们对应的卷积核W0的最后一个参数也必须是3的张量，这里卷积核W0的单独子矩阵维度为3x3.那么卷积核W0实际为一个3x3x3的张量。同时和上面的例子不同的是，这里的步长为2，即每次卷积后卷积核会向后移动2个像素的位置。蓝色矩阵（输入图像） 对 粉丝矩阵（filter） 进行矩阵内积计算并将三个内积运算的结果与偏移量b像加，比如上图中，3+0+0+0=3，计算后的值，即绿色矩阵 中的一个元素。 池化层（Pooling） 池化层，又称为降采样层，使用的原因为：根据图像局部相关性的原理，对图像进行子采样可能减少计算量，同时保持图像的旋转不变性。相比卷积层的复杂，池化层简单的多，所谓的池化，个人理解就是对输入张量的各个子矩阵进行压缩。假如是2x2的池化滤波，那么就将子矩阵的每个2x2个元素变为一个元素；如果为3x3的池化滤波，就将子矩阵每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。如果想将矩阵中每NxN个元素变成一个元素，则需要一个共同的池化标准。常见的池化标准有2个：MAX和Average。即取对应区域的最大值或者平均值作为池化后的元素值。下图的例子中采用的是最大池化方法，2x2的池化滤波，步长为2.首先对红色2x2区域进行池化，此区域中最大值对6，则对应池化输出的值为6。然后滤波进行移动，由于步长为2，则移动至图中绿色区域，输出最大值为8，以此类推，最终，输入的4x4的矩阵经过池化过程后，变为2x2的矩阵，得到了压缩。 全连接层（Full Connecting） 每层之间的神经元都有权重连接，通常全连接层在卷积神经网络的尾部，是同传统神经网络神经元的连接方式是一样的。全连接层和卷积层比较相似，但全连接层的输出是一个Nx1大小的向量，并通过几个全连接层对向量进行将为操作，一般采用softmax全连接。 总结 一般CNN的结构依次为 input ((conv –&gt; relu) x N–&gt;pool?) x M (fc –&gt; relu) x K fc 卷积神经网络的训练算法 与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距； 找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。 卷积神经网络的优缺点优点 共享卷积核，便于处理高维数据； 不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。 缺点 需要大量训练样本和好的硬件支持（GPU、TPU…）; 物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。 卷积神经网络的典型结构 LeNet,最早用于手写体数字识别的卷积神经网络。 AlexNet，2012年ILSVRC比赛中获得第一名，远超过第二名，比LeNet更深，用多层小卷积层进行叠加替换大卷积层。 ZFNet，2013年ILSVRC比赛冠军 GoogleNet，2014年ILSVRC比赛冠军 VGGNet，2014年ILSVRC比赛中的模型，图像识别上略差于GoogleNet，但是在很多图像转化学习问题（比如object detection）上效果很好。 实战演练猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型 训练集（共25000张图片，猫狗各12500张）测试集（共3000张图片，猫狗各1500张） 我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2F2018%2F06%2F08%2F20180608zookeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。zookeeper主要是写分布式程序，可总结为：一致、有头、数据数。 总述 使用zookeeper开发自己的分布式系统要注意的问题： 解决数据一致性的问题 协调各种“动物”hadoop 小象impala 黑斑羚shark 鲨鱼hive 蜂巢mahout 象夫zookeeper 动物园管理员 google三论文 GFS → HDFSBigTable → HBaseMapReduce → HadoopMRchubby → zookeeper zookeeper是什么？ NoSQL数据库CAP原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memcached安装及简单命令]]></title>
    <url>%2F2018%2F06%2F05%2F20180605memcached%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[memcached是一套分布式的高速缓存系统，由LiveJournal的Brad Fitzpatrick开发，但目前被许多网站使用。 ubuntu16.0.4下memcached的安装 linux系统安装memcached首先要安装libevent库 12sudo apt-get install libevent-devesudo apt-get install memcached 若linux系统为centos则命令为linux系统安装memcached首先要安装libevent库 12yum install libevent libevent-deveyum install memcached memcached的连接与关闭启动memcached连接 找到memcached的安装目录，自动安装memcached在/usr/local/bin/memcached路径下linux系统安装memcached首先要安装libevent库 1memcached -u root -d -m 128m -p 11211 连接memcached语法为：telnet HOST PORT本实例的memcached服务运行的主机为127.0.0.1(本机)，端口为11211linux系统安装memcached首先要安装libevent库 1telnet 127.0.0.1 11211 连接成功如下图所示： 退出命令 linux系统安装memcached首先要安装libevent库 1quit 关闭memcached 与windows直接输入memcached.exe -d stop关闭memcached不同linux需先知道memcached的进程号，再将其杀死查看进程号linux系统安装memcached首先要安装libevent库 1stats 或者 ps -ef|grep memcached 知道了memcached对应的进程号pid后，使用kill命令杀死进程即可。注意：杀死进程前必须quit退出连接linux系统安装memcached首先要安装libevent库 1kill 5070 杀死进城后再次连接memcached失败，说明memcached已经被关闭。 memcached的命令存储命令set set用于将value存储于key中，若set的key已经存在，该命令可以更新key所对应的原来的数据。语法格式如下：linux系统安装memcached首先要安装libevent库 12set key flag exptime bytes [noreply]value key：键值对中的key，用于查找缓存值flag：可以包含键值对的整型参数，客户机使用它存储关于键值对的额外信息exptime：再缓存中保存键值对的时间长度，以秒为单位，0表示永远bytes：在缓存中存储的字节数noreply：可选参数，该参数告知服务器不需要返回数据value：存储的值，始终位于第二行 add add用于将value存储在指定的key中，如果add的key已经存在，则不会更新数据，与之前的值仍然保持相同，会得到NOT_STORED的响应，但是过期的key会更新。语法格式如下：linux系统安装memcached首先要安装libevent库 12add key flags exptime bytes [noreply]value replace replace用于替换已经存在的key的value，如果可以不存在，则替换失败，并且得到NOT_STORED的响应语法格式如下：linux系统安装memcached首先要安装libevent库 12replace key flags exptime bytes [noreply]value append append用于向已经存在key的value后面追加数据语法格式如下：linux系统安装memcached首先要安装libevent库 12append key flags exptime bytes [noreply]value prepend prepend命令用于向已经存在key的value前面追加数据语法格式如下：linux系统安装memcached首先要安装libevent库 12prepend key flags exptime bytes [noreply]value cas cas用于执行一个“检查并设置”的操作，它仅在当前客户端最后一次取值后，该key对应的值没有被其他客户端修改的情况下才能够将值写入。检查是通过cas_token参数进行的，这个参数是memcached指定给已经存在的元素的一个唯一的64位值。语法格式如下：linux系统安装memcached首先要安装libevent库 12cas key flags exptime bytes unique_cas_token [noreply]value unique_cas_token是通过gets命令获取的一个唯一的64位值 查找命令get get用于获取存储在key中的value，如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。语法格式如下：linux系统安装memcached首先要安装libevent库 12get keyget key1 key2 key3 gets gets用于获取CAS令牌存的value,如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。语法格式如下：linux系统安装memcached首先要安装libevent库 12gets keygets key1 key2 key3 delete delete命令用于删除已经存在的key。语法格式如下：linux系统安装memcached首先要安装libevent库 1delete key [noreply] incr/decr incr和decr用于对已经存在的key的数字进行自增或自减操作。但是惭怍的数据必须是十进制的32位无符号整数，若key不存在，返回NOT_FOUND，若键的值不为数字，则返回CLIENT_ERROR，其他错误返回ERROR。语法格式如下：linux系统安装memcached首先要安装libevent库 12incr key increment_values [noreply]decr key decrement_values [noreply] increment_values为增加的数值decrement_values为减少的数值 flush_all flush_all用于清理缓存中的所有的键值对，该命令提供了一个可选参数time，用于在制定的时间后执行清理缓存操作。语法格式如下：linux系统安装memcached首先要安装libevent库 1flush_all [time] [noreply]]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis安装及简单命令]]></title>
    <url>%2F2018%2F06%2F02%2F20180602redis%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[redis是一个key-value存储系统。它支持存储的value类型相对更很多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。Redis是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。 Redis：REmote DIctionary ServerRedis(远程字典服务器) 是完全开源免费的，用C语言编写，是一个高性能的（key/value）分布式内存数据库，基于内存运行并支持持久化的NoSQL数据库，是当前最热门的NoSQL数据库之一，也被人们称之为结构数据服务器。 Redis逐步取代memcached的原因 1.redis支持数据持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用2.redis不仅仅直迟简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储3.redis支持数据的备份，即master-slave魔术的数据备份 Redis能做什么？ 1.内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务2.取最新N个数据的操作，如：可以将最新的10条评论的ID放在redis的List集合里面3.模拟类似于HttpSession这种需要设定过期时间的功能4.发布、订阅消息系统5.定时器、计数器 与memcached区别 都是key-value存储memcached一旦服务关闭，数据会全部没有redis服务关闭重启后数据还在 安装Redis 我们的环境：VMware WorkplaceCentOS-6.5redis-4.0.9下载地址： Http://redis.io将下载好的redis-4.0.9放入虚拟机中，并解压 1tar -zxvf redis-4.9.0.tar.gz redis内包含的文件 输入命令 1make 运行运行makefile文件，要有GCC，没有则会报错。安装gcc： 1yum install gcc-c++ 安装完成之后要进行二次make，但是之前要将上一次make不成功的残余文件清理，之后再make 12make discleanmake make完成后，执行 1make install 出现下图所示即安装成功 配置redis.conf 进入redis-4.0.9,将redis.conf拷贝一份，在拷贝后的上面进行修改将redis.conf拷贝至myredis文件夹下 1vim redis.conf 将no修改为yes 启动redis服务1cd /usr/local/bin 1redis-server /home/myy/hadoop/myredis/redis.config 进入客户端 客户端默认端口为6379 判断是否与服务端连接成功 查看服务状态 关闭连接 关闭后查看服务就没有了 基本命令默认库 Redis默认有16个库，进入时默认在0号库，角标从0开始，某些任务找一号库，某些找二号库，任务逻辑更清晰redis.conf中有说明 转换库的命令，eg：切换为8号库 flushdb和flushall的区别 flushdb是删除当前库，flushall是删除全部库 Benchmark查看本机状态 常用五大数据类型string字符串 String是redis最基本的类型，可以理解为与memcached一摸一样的类型，一个key对应一个value。String类型是二进制安全的，即redis的string可以包含任何数据，比如jpg图片或者序列化对象string是redis最基本的数据类型，一个redis中字符串最多可以是512M hash哈希 redis hash是一个键值对集合，是一个string类型的filed和value的映射表，适合用于存储对象。类似java中的Map&lt;String,Object&gt; list列表 列表是简单的字符串列表，按照插入顺利排序。可以添加一个元素到列表的头部（左边）或者尾部（右边），它的底层实际是个链表 set集合 set是string类型的无序集合。是通过HashTable实现的。 Zset有序集合 Zset（sorted set）zset和set一样也是string类型元素的集合，且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的承宣是唯一的，但分数（score）是可以重复的。 常用关键字set/get/exists/keys/move/mset/mget set设置键值，也可以覆盖原来的值get获取对应键的值exists查看某个key是否存在move移动到别的库内mset/mget批量设置/获取键值 expire/ttl/setex expire设置秒数，过期后自动消失ttl 查看某个key还有多久时间setex设置值时同时设置时间 append/strlen/getrange/setrange/incr/decr/incrby/decrby append补充字符串strlen字符串的长度getrange获取指定区域范围内的值，0到-1表示全部setrange设置指定区域范围内的值incr递增加1decr递减少1incrby decrby自定义数量string类型此命令不可用 lpush/lrange/lpop/rpop/lidex/llen lpush和rpush查看后顺序不同lrange查看listlpop栈顶出去rpop栈底出去lidex索引llen查看list长度 lren/ltrim/rpoplpush/lset/linsert lrem删除n和valueltrim截取指定范围内的值再赋值给listrpoplpush将list01栈底给list02栈顶lset替换某位置的值linsert某值之前或之后插入某值 sadd/smembers/sismember/scard/srem/srandmember sadd设置set集合（重复自动留一个）smembers查看set集合sismember查看set内是否有某值scard获取集合内元素个数srem删除集合内某值srandmember集合中随机出几个数 spop/smove/sdiff/sinter/sunion spop随机出栈smove将一set中某一值赋给另一setsdiff取两集合的差集：在第一个中而不在第二个中sinter取两集合的交集sunion取两集合的并集 zrange/zrevrange/zcount zrange同list相同zrevrange从高到低排序zincrby修改某个值的分数zcount返回指定分数范围内值的个数 hash相关的关键字 kv模式不变，但v是一个键值对]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 分布式搭建]]></title>
    <url>%2F2018%2F05%2F30%2F20180530hadoop-%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。下面我们一起来搭建吧。 准备工具 linux环境下搭建hadoop集群需要准备：VMware-workstation-10.0.1注册机CentOS-6.5-x86_64-bin-DVD1jdk-7u79-linux-x64hadoop-2.6.4.tar 新建虚拟机 解压VMware-workstation-10.0.1注册机,打开VMware Workstation主页,点击新建虚拟机,选择典型,如下:点击下一步,选择安装程序光盘映像文件,浏览找到你下载CentOS-6.5-x86_64-bin-DVD1的压缩包文件,如下:继续点击下一步,填写用户名和密码(尽量简单),填好后点击下一步,为即将创建的虚拟机命名并选择安装路径(最好不要安装在C盘),如下所示:继续点击下一步至如下界面:点击自定义硬件可以修改虚拟机的各项参数,如果电脑内存小于等于4GB,需要将内存改至512MB,否则严重卡顿。修改完成后点击完成，虚拟机就创建成功，打开后界面如下：若要批量创建虚拟机，可以在创建好的虚拟机的基础上进行克隆操作， 安装jdk 打开一个虚拟机，右键单击桌面选择Open in Terminal，进入编辑界面： 假设用户名为wxx获取root权限123su cd /etcvi sudoers i 进入编辑状态，在 1root ALL=(ALL) ALL的下一行编辑 1wxx ALL=(ALL) ALL 按ESC键，退出编辑格式按Shift + :输入wq!保存并退出 创建hadoop文件夹12cdmkdir hadoop 将jdk-7u79-linux-x64安装包复制到hadoop文件目录下（与windows环境下类似）。 解压jdk-7u79-linux-x64.gz文件123cdcd hadooptar-zxvf jdk-7u79-linux-x64.gz 设置jdk环境变量1234cdcd hadoopsugedit /etc/profile 进入后在最后一行添加以下指令： 123export JAVA_HOME=/home/by/hadoop/jdk1.8.0_11export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 点击保存后关闭，输入以下指令使jdk生效： 1source /etc/profile 检查jdk是否安装成功1java -version 成功后显示如下信息： 123java version "1.7.0_79"Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 创建集群克隆虚拟机 将已经安装好jdk的虚拟机克隆两个，创建三个虚拟机的集群。 修改hostname12suvi /etc/sysconfig/network 将三个虚拟机分别命名master、slave1、slave2如图： 完成后重启虚拟机reboot 将三个虚拟机的ip地址相互连接 首先必须确保虚拟机联网，如果NET模式连不上网，则选中桥接模式。网络通畅后执行以下操作:1.查看三台虚拟机IP,分别对三个虚拟机执行指令ifconfig，查看各虚拟机ip地址 2.在master中执行以下指令 123sucd/etcgedit /etc/hosts 192.168.142.142 192.168.142.143 进入编辑界面后按“IP地址 hostname”填写信息，如图： 填写完后按Save按钮，关闭编辑页。 3.将配置好的文件复制到slave1、slave2中,在master中执行以下指令： 12scp /etc/hosts root@slave1:/etc/scp /etc/hosts root@slave2:/etc/ 4.检查各虚拟机是否互联,在master中执行以下指令： 12ping slave1ping slave2 连通即完成 配置SSH无密钥登录 1.关闭防火墙,对每个虚拟机进行如下操作： 12suchkconfig iptables off 执行后重启虚拟机： 1reboot 2.关闭防火墙后在master下执行以下指令： 1234567cdssh-keygen –t rsacd .sshcat id_rsa.pub &gt;&gt; authorized_keyschmod 600 authorized_keys scp authorized_keys wxx@slave1:~/.ssh/scp authorized_keys wxx@slave2:~/.ssh/ 3.检查无密钥登录是否成功 123ssh slave1ssh slave2ssh master 成功后显示如下： 安装并配置hadoop-2.6.4(在master中) 1.将hadoop-2.6.4.tar.gz安装包复制到hadoop文件目录下（与windows环境下类似）。 2.解压hadoop-2.6.4.tar.gz 123cdcd hadooptar -zxvf hadoop-2.6.4.tar.gz 3.配置hadoop-2.6.4的各项文件 1234cdcd hadoop/hadoop-2.7.4cd etc/hadoopgedit hadoop-env.sh 在最后一行添加: 1export JAVA_HOME=/home/by/hadoop/ jdk1.8.0_11 编辑core-site.xml1gedit core-site.xml 添加代码： 1234567891011121314&lt;property&gt;&lt;name&gt;fs.default.name&lt;/name&gt;&lt;value&gt;hdfs://master:9000&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/by/hadoop/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;ds.default.name&lt;/name&gt;&lt;value&gt;hdfs://master:54310&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt; 编辑hdfs-site.xml1gedit hdfs-site.xml 添加代码： 1234567891011121314&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:/home/by/hadoop/dfs/name&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:/home/by/hadoop/dfs/data&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;2&lt;/value&gt;&lt;/property&gt; 编辑mapred-site.xml1gedit mapred-site.xml 注意：必须先复制mapred-site.xml.template文件更名为mapred-site.xml添加代码： 123456789101112&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;master:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;master:19888&lt;/value&gt;&lt;/property&gt; 编辑yarn-site.xml1gedit yarn-site.xml 添加代码： 1234567891011121314151617181920212223242526272829303132&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;master&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;&lt;value&gt;master:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;&lt;value&gt;master:8030&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;&lt;value&gt;master:8031&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;&lt;value&gt;master:8033&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;&lt;value&gt;master:8088&lt;/value&gt;&lt;/property&gt; 编辑master1gedit master 添加代码： 1master 编辑slaves1gedit slaves 添加代码： 123masterslave1slave2 4.将配置好的文件复制到slave1、slave2中 1234cd cd hadoopscp -r hadoop-2.7.4 slave1:~/hadoopscp -r hadoop-2.7.4 slave2:~/hadoop 5.启动集群 123456cdcd hadoop/hadoop-2.7.4bin/hdfs namenode -formatsbin/start-dfs.shsbin/start-yarn.shsbin/hadoop-daemon.sh start secondarynamenode 6.检查集群情况 1jps 三台虚拟机如下所示：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX入门]]></title>
    <url>%2F2018%2F05%2F27%2F20180527LaTeX%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[LaTeX是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天，甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。我们在投稿论文的时候，会经常使用LaTex根据期刊的要求对文章进行排版，所以作为一名研究生学习这个是十分必要的。 Hello World 打开WinEdt，建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8。 1234\documentclass&#123;article&#125; \begin&#123;document&#125; hello, world \end&#123;document&#125; 然后在WinEdt的工具栏中找到编译按钮（在垃圾桶和字母B中间），在下拉菜单中选择XeTeX，并点击编译。如果顺利的话，就可以顺利生成出第一个pdf文件，点击工具栏中的放大镜按钮就可以快速打开生成的pdf文件。 标题、作者、章节和段落 建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。 1234567\documentclass&#123;article&#125; \author&#123;My Name&#125; \title&#123;The Title&#125; \begin&#123;document&#125; \maketitle hello, world % This is comment \end&#123;document&#125; 效果图如下： 加入目录 建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。 123456789\documentclass&#123;article&#125; \begin&#123;document&#125; \tableofcontents \section&#123;Hello China&#125; China is in East Asia. \subsection&#123;Hello Beijing&#125; Beijing is the capital of China. \subsubsection&#123;Hello Dongcheng District&#125; \paragraph&#123;Hello Tian'anmen Square&#125;is in the center of Beijing \subparagraph&#123;Hello Chairman Mao&#125; is in the center of Tian'anmen Square \end&#123;document&#125; 效果图如下： 段落和换行123456789101112131415\documentclass&#123;article&#125; \begin&#123;document&#125; Beijing is the capital of China. New York is the capital of America. Amsterdam is \\ the capital \\ of Netherlands. \end&#123;document&#125; 效果图如下： 数学公式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758\documentclass&#123;article&#125; \usepackage&#123;amsmath&#125; \usepackage&#123;amssymb&#125; \begin&#123;document&#125; The Newton's second law is F=ma. The Newton's second law is $F=ma$. The Newton's second law is F=ma The Newton's second law is F=ma Greek Letters $\eta$ and $\mu$ Fraction $\frac&#123;a&#125;&#123;b&#125;$ Power $a^b$ Subscript $a_b$ Derivate $\frac&#123;\partial y&#125;&#123;\partial t&#125; $ Vector $\vec&#123;n&#125;$ Bold $\mathbf&#123;n&#125;$ To time differential $\dot&#123;F&#125;$ Matrix (lcr here means left, center or right for each column) \[ \left[ \begin&#123;array&#125;&#123;lcr&#125; a1 &amp; b22 &amp; c333 \\ d444 &amp; e555555 &amp; f6 \end&#123;array&#125; \right] \] Equations(here \&amp; is the symbol for aligning different rows) \begin&#123;align&#125; a+b&amp;=c\\ d&amp;=e+f+g \end&#123;align&#125; \[ \left\&#123; \begin&#123;aligned&#125; &amp;a+b=c\\ &amp;d=e+f+g \end&#123;aligned&#125; \right. \] \end&#123;document&#125; 效果图如下： 插入图片 先搜索到一个将图片转成eps文件的软件，很容易找的，然后将图片保存为一个名字如figure1.eps。建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，放在和图片文件同一个文件夹里，编译并观察现象。 12345\documentclass&#123;article&#125; \usepackage&#123;graphicx&#125; \begin&#123;document&#125; \includegraphics[width=4.00in,height=3.00in]&#123;figure1.eps&#125; \end&#123;document&#125; 简单表格123456789101112131415161718192021222324\documentclass&#123;article&#125;\begin&#123;document&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;a &amp; b \\c &amp; d\\\end&#123;tabular&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;\hlinea &amp; b \\\hlinec &amp; d\\\hline\end&#123;tabular&#125;\begin&#123;center&#125;\begin&#123;tabular&#125;&#123;|c|c|&#125;\hlinea &amp; b \\ \hlinec &amp; d\\\hline\end&#123;tabular&#125;\end&#123;center&#125;\end&#123;document&#125; 效果图如下：]]></content>
      <categories>
        <category>LaTeX</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
        <tag>文档编写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战--决策树]]></title>
    <url>%2F2018%2F05%2F25%2F20180525%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据缺点：可能会产生过度匹配问题使用数据类型：数值型和标称型专家系统中，经常使用决策树 trees.py12from math import log import operator createDataSet() 创建数据集 trees.py12345678910def createDataSet(): # 数据集中两个特征'no surfacing','flippers', 数据的两个类标签'yes','no #dataSet是个list dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing','flippers'] return dataSet, labels calcShannonEnt(dataSet) 计算给定数据集的熵 1234567891011121314151617def calcShannonEnt(dataSet): numEntries = len(dataSet) #计算数据集中实例的总数 labelCounts = &#123;&#125; #创建空字典 for featVec in dataSet: #提取数据集每一行的特征向量 currentLabel = featVec[-1] #获取特征向量最后一列的标签 # 检测字典的关键字key中是否存在该标签，如果不存在keys()关键字，将当前标签/0键值对存入字典中,并赋值为0 #print(labelCounts.keys()) if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 #print(labelCounts) labelCounts[currentLabel] += 1 #否则将当前标签对应的键值加1 #print("%s="%currentLabel,labelCounts[currentLabel]) shannonEnt = 0.0 #初始化熵为0 for key in labelCounts: prob = float(labelCounts[key])/numEntries #计算各值出现的频率 shannonEnt -= prob * log(prob,2) #以2为底求对数再乘以出现的频率，即信息期望值 #print("%s="%labelCounts[key],shannonEnt) return shannonEnt splitDataSet(dataSet, axis, value) 按照给定特征划分数据集得到熵之后，还需划分数据集，以便判断当前是否正确地划分了数据集，三个输入参数分别为：带划分的数据集，划分数据集的特征，需要返回的特征得值，挑选出dataSet中axis位置值为value的剩余部分。 123456789101112def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: #筛选出dataSet中axis位置值为value #列表的索引中冒号的作用，a[1: ]表示该列表中的第1个元素到最后一个元素，而a[ : n]表示从第0歌元素到第n个元素(不包括n) reducedFeatVec = featVec[:axis] #取出特定位置前面部分并赋值给reducedFeatVec #print(featVec[axis+1:]) #print(reducedFeatVec) reducedFeatVec.extend(featVec[axis+1:]) #取出特定位置后面部分并赋值给reducedFeatVec retDataSet.append(reducedFeatVec) #print(retDataSet) return retDataSet chooseBestFeatureToSplit(dataSet) 选择最好的数据集划分方式选取特征，划分数据集，计算得出最好的划分数据集的特征 123456789101112131415161718192021def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 #计算特征数量，即每一列表元素具有的列数，再减去最后一列为标签，故需减去1 baseEntropy = calcShannonEnt(dataSet) #计算信息熵，此处值为0.9709505944546686，此值将与划分之后的数据集计算的信息熵进行比较 bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] #创建标签列表 #print(featList) uniqueVals = set(featList) #确定某一特征下所有可能的取值,set集合类型中的每个值互不相同 #print(uniqueVals) newEntropy = 0.0 for value in uniqueVals: #计算每种划分方式的信息熵 subDataSet = splitDataSet(dataSet, i, value) #抽取该特征的每个取值下其他特征的值组成新的子数据集 prob = len(subDataSet)/float(len(dataSet)) #计算该特征下的每一个取值对应的概率（或者说所占的比重） newEntropy += prob * calcShannonEnt(subDataSet) #计算该特征下每一个取值的子数据集的信息熵，并求和 infoGain = baseEntropy - newEntropy #计算每个特征的信息增益 #print("第%d个特征是的取值是%s，对应的信息增益值是%f"%((i+1),uniqueVals,infoGain)) if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i #print("第%d个特征的信息增益最大，所以选择它作为划分的依据，其特征的取值为%s,对应的信息增益值是%f"%((i+1),uniqueVals,infoGain)) return bestFeature majorityCnt(classList) 递归构建决策树，返回出现次数最多的分类名称 1234567def majorityCnt(classList): classCount=&#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] createTree(dataSet,labels) 创建树,参数为数据集和标签列表 123456789101112131415161718192021def createTree(dataSet,labels): classList = [example[-1] for example in dataSet] #提取dataset中的最后一列——种类标签 #print(classList) if classList.count(classList[0]) == len(classList): #计算classlist[0]出现的次数,如果相等，说明都是属于一类，不用继续往下划分 return classList[0] #递归结束的第一个条件是所有的类标签完全相同，则直接返回该类标签 #print(dataSet[0]) if len(dataSet[0]) == 1: #看还剩下多少个属性，如果只有一个属性，但是类别标签有多个，就直接用majoritycnt()进行整理，选取类别最多的作为返回值 return majorityCnt(classList) #递归结束的第二个条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组，则返回出现次数最多的类别 bestFeat = chooseBestFeatureToSplit(dataSet) #选取信息增益最大的特征作为下一次分类的依据 bestFeatLabel = labels[bestFeat] #选取特征对应的标签 #print(bestFeatLabel) myTree = &#123;bestFeatLabel:&#123;&#125;&#125; #创建tree字典，下一个特征位于第二个大括号内，循环递归 del(labels[bestFeat]) #删除使用过的特征 featValues = [example[bestFeat] for example in dataSet] #特征值对应的该栏数据 #print(featValues) uniqueVals = set(featValues) #找到featvalues所包含的所有元素，去重复 for value in uniqueVals: subLabels = labels[:] #将使用过的标签删除更新后，赋值给新的列表，进行迭代 #print(subLabels) myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat,value),subLabels) #循环递归生成树 return myTree classify(inputTree,featLabels,testVec): 测试算法，使用决策树执行分类 12345678910111213def classify(inputTree,featLabels,testVec): firstStr = list(inputTree.keys())[0] #找到树的第一个分类特征，或者说根节点'no surfacing' #print(firstStr) secondDict = inputTree[firstStr] #从树中得到该分类特征的分支，有0和1 #print(secondDict) featIndex = featLabels.index(firstStr) #根据分类特征的索引找到对应的标称型数据值，'no surfacing'对应的索引为0 #print(featIndex) key = testVec[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): classLabel = classify(valueOfFeat, featLabels, testVec) else: classLabel = valueOfFeat return classLabel storeTree(inputTree,filename) 决策树的存储，使用pickle序列化对象，可在磁盘中保存对象。 12345678910def storeTree(inputTree,filename): import pickle fw = open(filename,'wb') #二进制写入'wb' pickle.dump(inputTree,fw) #pickle的dump函数将决策树写入文件中 fw.close() def grabTree(filename): import pickle fr = open(filename,'rb') #对应于二进制方式写入数据，'rb'采用二进制形式读出数据 return pickle.load(fr) trees_main.py123import treesfrom imp import reloadimport treePlotter 创建数据集1234myDat,labels=trees.createDataSet()#print(myDat)#print(labels)#print(trees.calcShannonEnt(myDat)) 熵增大的原因 熵越高，混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大 123#myDat[1][-1]='maybe'#更改list中某一元素的值（除yes和no外的值），即为添加更多的分类，中括号中为对应元素行列的位置#print(myDat)#print(trees.calcShannonEnt(myDat)) #分类变多，熵增大 append()和extend()两类方法的区别123456a=[1,2,3]b=[4,5,6]a.append(b)#print(a)#[1, 2, 3, [4, 5, 6]]a.extend(b)#print(a)#[1, 2, 3, [4, 5, 6], 4, 5, 6] 按照给定特征划分数据集123#print(myDat)#print(trees.splitDataSet(myDat,0,1))#print(trees.splitDataSet(myDat,0,0)) 选择最好的数据集划分方式12#print(myDat)#print(trees.chooseBestFeatureToSplit(myDat)) 创建树,参数为数据集和标签列表12345678myTree=trees.createTree(myDat,labels)#print(myTree)myDat,labels=trees.createDataSet()myTree1=treePlotter.retrieveTree(0) #print(myTree1)#print(trees.classify(myTree1,labels,[1,0]))#print(trees.classify(myTree,labels,[1,1])) 决策树的存储12trees.storeTree(myTree,'classifierStorage.txt')#print(trees.grabTree('classifierStorage.txt')) 使用决策树预测隐形眼镜类型123456fr=open('lenses.txt')lenses = [inst.strip().split('\t') for inst in fr.readlines()] #将文本数据的每一个数据行按照tab键分割，并依次存入lenseslensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate'] # 创建并存入特征标签列表lensesTree = trees.createTree(lenses, lensesLabels) # 根据继续文件得到的数据集和特征标签列表创建决策树print(lensesTree)treePlotter.createPlot(lensesTree) treePlotter.py python中使用Matplotlib注解绘制树形图 1import matplotlib.pyplot as plt 定义文本框和箭头格式123decisionNode = dict(boxstyle="sawtooth", fc="0.8") # boxstyle为文本框的类型，sawtooth是锯齿形，fc是边框线粗细,pad指的是外边框锯齿形（圆形等）的大小leafNode = dict(boxstyle="round4", fc="0.8") #定义决策树的叶子结点的描述属性，round4表示圆形arrow_args = dict(arrowstyle="&lt;-") #定义箭头属性 plotNode(nodeTxt, centerPt, parentPt, nodeType) 绘制带箭头的注解annotate是关于一个数据点的文本nodeTxt为要显示的文本，centerPt为文本的中心点，箭头所在的点，parentPt为指向文本的点annotate的作用是添加注释，nodetxt是注释的内容nodetype指的是输入的节点（边框）的形状 1234def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction', xytext=centerPt, textcoords='axes fraction', va="center", ha="center", bbox=nodeType, arrowprops=arrow_args ) def createPlot(): 第一版构造树函数，后面会改进，所以这里要注释上 123456#fig = plt.figure(1, facecolor='white')#fig.clf()#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses#plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)#plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)#plt.show() getNumLeafs(myTree) 计算叶子节点的个数构造注解树，需要知道叶节点的个数，以便可以正确确定x轴的长度；要知道树的层数，可以确定y轴的高度。 1234567891011121314151617181920212223def getNumLeafs(myTree): numLeafs = 0 firstStr = list(myTree.keys())[0] #获得myTree的第一个键值，即第一个特征，分割的标签 #print(firstStr) secondDict = myTree[firstStr] #根据键值得到对应的值，即根据第一个特征分类的结果 #print(secondDict) for key in secondDict.keys(): #获取第二个小字典中的key if type(secondDict[key]).__name__=='dict': #判断是否小字典中是否还包含新的字典（即新的分支） numLeafs += getNumLeafs(secondDict[key]) #包含的话进行递归从而继续循环获得新的分支所包含的叶节点的数量 else: numLeafs +=1 #不包含的话就停止迭代并把现在的小字典加一表示这边有一个分支 return numLeafsdef getTreeDepth(myTree): #计算判断节点的个数 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict': thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepth retrieveTree(i) 预先存储树信息 12345def retrieveTree(i): listOfTrees =[&#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;, &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: &#123;'head': &#123;0: 'no', 1: 'yes'&#125;&#125;, 1: 'no'&#125;&#125;&#125;&#125; ] return listOfTrees[i] plotMidText(cntrPt, parentPt, txtString) 作用是计算tree的中间位置，cntrPt起始位置,parentPt终止位置,txtString文本标签信息 12345678910111213141516171819202122def plotMidText(cntrPt, parentPt, txtString): xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] #cntrPt起点坐标，子节点坐标，parentPt结束坐标，父节点坐标 yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] #找到x和y的中间位置 createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)def plotTree(myTree, parentPt, nodeTxt): numLeafs = getNumLeafs(myTree) depth = getTreeDepth(myTree) firstStr = list(myTree.keys())[0] cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) #计算子节点的坐标 plotMidText(cntrPt, parentPt, nodeTxt) #绘制线上的文字 plotNode(firstStr, cntrPt, parentPt, decisionNode) #绘制节点 secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD #每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的 for key in secondDict.keys(): if type(secondDict[key]).__name__=='dict': plotTree(secondDict[key],cntrPt,str(key)) else: plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD createPlot(inTree)1234567891011def createPlot(inTree): fig = plt.figure(1, facecolor='white') #类似于Matlab的figure，定义一个画布，背景为白色 fig.clf() # 把画布清空 axprops = dict(xticks=[], yticks=[]) #subplot定义了一个绘图 createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) #no ticks #createPlot.ax1为全局变量，绘制图像的句柄，111表示figure中的图有1行1列，即1个，最后的1代表第一个图,frameon表示是否绘制坐标轴矩形 plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), '') plt.show() treePlotter_main.py12345678import treePlotter#treePlotter.createPlot()#print(treePlotter.retrieveTree(1))myTree=treePlotter.retrieveTree(0)#print(treePlotter.getNumLeafs(myTree))#print(treePlotter.getTreeDepth(myTree))myTree['no surfacing'][3]='maybe'treePlotter.createPlot(myTree)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>《机器学习实战》</tag>
        <tag>ML</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战--KNN]]></title>
    <url>%2F2018%2F05%2F23%2F20180523%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN%2F</url>
    <content type="text"><![CDATA[这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0 knn 优 点：精度高，对异常值不敏感，无数据输入假定缺 点：计算复杂度高，空间复杂度高，无法给出数据的内在含义使用数据范围：数值型和标称型 ————————————————————————————-下面进入正题————————————————————————————- kNN.py123from numpy import * import operator from os import listdir createDataSet()1234def createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group, labels classify0() inX用于分类的输入向量,是一个向量dataSet输入的训练样本集，是一个矩阵labels标签向量k用于选择最近邻居的数目labels数目与dataSet的行数相同 1234567891011121314151617181920212223242526272829def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] #返回的是dataSet的行数，行数就是样本的数量 diffMat = tile(inX, (dataSetSize,1)) - dataSet #矩阵相减 #inX是个向量，而dataset是个矩阵，两者之间要进行相减的运算，需要把这个向量也补成一个和dataset有相同行数列数的矩阵， #tile()的第二个参数，就是(datasetsize,1)，这个参数的意思就是把inX补成有datasetsize行数的矩阵。 #假如inX是（1，2），datasetsize =3，那么经过tile()转换后产生了一个这样的矩阵（[1,2],[1,2],[1,2]） sqDiffMat = diffMat**2 #平方 sqDistances = sqDiffMat.sum(axis=1) #按行求和 # sqdiffMat是([1,2],[0,1],[3,4])，axis这个参数影响了对矩阵求和时候的顺序，axis=0是按照列求和，结果为([3.1.7]) # axis=1是按照行进行求和，结果是([4,7])。 distances = sqDistances**0.5 #开方，得到欧氏距离 sortedDistIndicies = distances.argsort() #把向量中每个元素进行排序，结果是元素的索引形成的向量 #例子distance([1,4,3])，经过distance.argsort()之后的结果是([0,2,1] classCount=&#123;&#125; #存放最终的分类结果及相应的结果投票数 #投票过程，就是统计前k个最近的样本所属类别包含的样本个数 for i in range(k): # index = sortedDistIndicies[i]是第i个最相近的元素索引，即样本下标 # voteIlabel = labels[index]是样本index对应的分类结果('A' or 'B') voteIlabel = labels[sortedDistIndicies[i]] # classCount.get(voteIlabel, 0)返回voteIlabel的值，如果不存在，则返回0 # 然后将票数增1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 # 把分类结果进行排序，然后返回得票数最多的分类结果 # key=operator.itemgetter(1)的意思是按照字典里的第一个排序 #例子a = [1, 2, 3]，b = operator.itemgetter(1)，b(a)返回为2 #b = operator.itemgetter(1, 0)，b(a)，定义函数b，获取对象的第1个域和第0个的值，返回 (2, 1) # reverse=True是降序排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] #返回类别最多的类别 file2matrix() 将文本记录转换为NumPy将文本记录转换为NumPy的解析程序输入为矩阵，输出为训练样本矩阵和类标签向量 12345678910111213141516def file2matrix(filename): fr = open(filename) #打开文档 numberOfLines = len(fr.readlines()) #得到文件行数 #fr.readlines()读取行数,存在数组中,导入后每行中用\t隔开,两行之间用\n换行得到文件行数 returnMat = zeros((numberOfLines,3)) #创建返回NumPy矩阵，numberoflines行，3列的初始化零的矩阵 classLabelVector = []#定义一个空的数组 fr = open(filename) index = 0 for line in fr.readlines(): line = line.strip() #删除（）中的内容，这里表示删除空格 listFromLine = line.split('\t')#以\t分割 #print(listFromLine) returnMat[index,:] = listFromLine[0:3]#把每行前三个元素存入returnMat矩阵中，每行中存储三个 classLabelVector.append(int(listFromLine[-1]))#存储第四列元素即标签，在数组中append添加，-1表示最后一列 index += 1 return returnMat,classLabelVector autoNorm() 归一化数值，避免某特征值过大，使得权重比例不均匀，对计算结果产生影响。autoNorm可以自动将数字特征值转化为0到1区间 12345678910111213def autoNorm(dataSet): minVals = dataSet.min(0)#一维数组，值为各项特征（列）中的最小值。参数0使得函数从列中选取最小值 #print(minVals) maxVals = dataSet.max(0) #print(maxVals) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) #创建与样本集一样大小的零矩阵 #print(normDataSet) m = dataSet.shape[0]#dataSet的行数 normDataSet = dataSet - tile(minVals, (m,1))#矩阵中所有的值减去最小值 #tile将原来的一个数组minVals，扩充成了m行1列的数组 normDataSet = normDataSet/tile(ranges, (m,1)) #矩阵中所有的值除以最大取值范围进行归一化 return normDataSet, ranges, minVals datingClassTest() 测试算法，样本集中百分之九十的数据用来训练样本，百分之十的样本用来测试分类器kNN.classify0()。 1234567891011121314def datingClassTest(): hoRatio = 0.10 #百分之十的数据用于测试分类器，更改该变量的值可更改参加测试分类器的数据量 datingDataMat,datingLabels = file2matrix('datingTestSet2.txt') #导入数据 normMat, ranges, minVals = autoNorm(datingDataMat) #归一化数值 m = normMat.shape[0] #得到总行数 numTestVecs = int(m*hoRatio) #测试总数据数量，m*hoRatio是一个浮点型，需转化成整形 errorCount = 0.0 #初试错误率为0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) #分类器（需要测试的向量，训练样本集(90%)，标签集合，K） print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, datingLabels[i])) if (classifierResult != datingLabels[i]): errorCount += 1.0 #计数，错误的个数 print("the total error rate is: %f" % (errorCount/float(numTestVecs))) #错误率 print(errorCount) classifyPerson() 约会数据,对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测 123456789101112131415161718192021222324252627def classifyPerson(): resultList=['not at all','in samll doses','in large doses'] #三种感兴趣程度 percentTats=float(input("percentage of time spent playing video games?")) ffMiles=floats=float(input("frequent flier miles earned per year?")) iceCream=float(input("liters of ice cream consuned per year?"))#input键盘输入 datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') # 导入数据 normMat,ranges,minvals=autoNorm(datingDataMat) # 归一化，ranges是归一化的分母 inArr=array([ffMiles,percentTats,iceCream]) # inArr是归一化之前的datingDataMat数组中的行 classifierResult=classify0((inArr-minvals)/ranges,normMat,datingLabels,3)#先归一化，然后调用分类函数 #print(classifierResult) print("you will probably like this person:%s"%resultList[classifierResult-1])``` ## img2vector()&gt; 图片转向量手写体：32*32的黑白图像图片转向量，将32*32的二进制图像矩阵转换为1*1024的向量```pythondef img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename) for i in range(32):#循环读出文件的前32行 lineStr = fr.readline() for j in range(32):#将每行的前32个字符存储在NumPy数组中 returnVect[0,32*i+j] = int(lineStr[j]) return returnVect#返回数组 handwritingClassTest() 手写体测试 123456789101112131415161718192021222324252627def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('trainingDigits') #导入训练数据 #print(trainingFileList) m = len(trainingFileList) #训练数据的总数 #print(m) trainingMat = zeros((m,1024)) #m行1024列的零向量 for i in range(m): fileNameStr = trainingFileList[i] #文件名 fileStr = fileNameStr.split('.')[0] #取文件名.之前的名字 classNumStr = int(fileStr.split('_')[0]) #取文件名_之前的名字 hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr) #将对应数据集下的文件一个个的转为向量 #print(trainingMat[i,:]) testFileList = listdir('testDigits') #测试数据 errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) #利用训练的trainingMat测试 print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)) if (classifierResult != classNumStr): errorCount += 1.0 print("\nthe total number of errors is: %d" % errorCount) print("\nthe total error rate is: %f" % (errorCount/float(mTest))) knn_main.py1234567891011import kNN import matplotlib import matplotlib.pyplot as plt import numpy as np from imp import reload group,labels=kNN.createDataSet() #print(group) #print(labels) #print(kNN.classify0([0,0],group,labels,3)) 散点图123456789fig=plt.figure() #建立画板 ax=fig.add_subplot(111) #添加一个子图，一行一列第一个子块，若括号内为349，则三行四列第9个子块 reload(kNN) datingDataMat,datingLabels=kNN.file2matrix('datingTestSet2.txt') #print(datingDataMat) #ax.scatter(datingDataMat[:,1],datingDataMat[:,2]) # scatter绘制散点图,使用第二列第三列数据 #ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*np.array(datingLabels),15.0*np.array(datingLabels)) ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*np.array(datingLabels),15.0*np.array(datingLabels)) #plt.show() 归一化数值1234normMat,ranges,minVals=kNN.autoNorm(datingDataMat) #print(normMat) #print(ranges) #print(minVals) 测试算法1#kNN.datingClassTest() 约会预测123#对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测， #输入10 10000 0.5 #kNN.classifyPerson() 手写体1234567#trainingDigits包含大约2000个例子，每个数字约有200个样本 #testDigits包含大约900个测试数据 testVector=kNN.img2vector('trainingDigits/0_13.txt') #print(testVector[0,0:31]) #print(testVector[0,32:63]) #print(testVector[0,64:95]) kNN.handwritingClassTest()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>《机器学习实战》</tag>
        <tag>KNN</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伊始]]></title>
    <url>%2F2018%2F05%2F17%2F20180517%E4%BC%8A%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈 简单随便说说建站方法Hexo 就是以Hexo为主，剩下的自行百度吧，毕竟我要回寝室，没时间写了 Github Pages 以github为载体实现的，也百度吧，啊啊啊，实验室就剩我自己了。 配置域名 在博客的根目录下source文件中(例如：C:\hexo\source)新建一个名为CNAME的文件，注意没有任何后缀，用于github进行读取。在文件中添加自己的域名并保存，例如 1amberwu.top 然后，重新生成静态文件并部署。CNAME文件也会被上传到github仓库当中，此时在浏览器中输入自己的域名，回车之后，你会第一次遇见自己的小天地~ Hexo的一些基本命令123hexo clean #用于清除配置文件hexo g #完整命令为hexo generate,用于生成静态文件hexo s #完整命令为hexo server,用于启动服务器，主要用来本地预览 在浏览器地址栏输入http://localhost:4000/, 按下回车键，熟悉的界面又出现了。 12hexo d #完整命令为hexo deploy,用于将本地文件发布到github等git仓库上hexo n "my article" #完整命令为hexo new,用于新建一篇名为“my article”的文章 这样就会在博客目录下source_posts中生成相应的 my article.md文件( 例如 C:\blog\source_posts\my article.md ) Hexo修改及配置主题 hexo初始化之后默认的主题是landscape , 然后你可以去这个地址 https://hexo.io/themes/ 里面找到你想要的主题。在github中搜索你要的主题名称，里面都会有该主题的如何使用的介绍，按着来就好了，反正就是改改改！我选的是next,看起来挺不错，至少是我喜欢的类型。 更改主题需要修改配置文件 更改主题需要修改配置文件，就是根目录下的_config.yml文件，找到 theme 字段，并将其值更改为next即可 1theme: next 配置next主题 next主题共分三种，在站点根目录/themes/next/_congig.yml 文件中修改，找到scheme关键字即可选择。 1234# Schemes#scheme: Muse#scheme: Mistscheme: Pisces 当然，你完全可以进行很多的自定义设置甚至修改源码，定制自己的主题。小女子能力有限，更多的设置请参考官方文档http://theme-next.iissnan.com/getting-started.html 添加背景图片 将背景图片命名为background.jpg并放入主题根目录/source/images文件夹中打开博客根目录/themes/next/source/css/_custom/custom.styl文件加入如下代码： 123456// Custom styles.body &#123; background-image: url(/images/background.jpg); background-attachment: fixed; background-repeat: no-repeat;&#125; 更多资料]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
