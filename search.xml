<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基础网络]]></title>
    <url>%2F2018%2F06%2F07%2F%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[LeNet AlexNet ResNet LeNet1989年LeCun提出的，是卷积神经网络的鼻祖。网络结构示意图如下：第一层卷积核为5 x 5，stride为1，输出通道为20，分辨率为24 x 24（28-5+1=24）的特征响应图。经过2 x 2池化，分辨率降为12 x 12。然后同样的操作，通道数变为50 x 50，分辨率变为4 x 4。最后全连接，第一层单元数为500，第二层输出分类个数10，再接softmax输出最终结果。 AlexNetAlexNet针对的是ILSVRC的分类问题，输入图片是256 x 256的三通道彩色图片。参数有6000多万（其中全连接的参数占了约94%） 数据增强训练Alex的时候采用了数据增加手段，包含随机位置裁剪，具体就是在256 x 256的图片中，随机产生位置裁剪一块224 x 224的子区域。且在卷积过程中使用了零填充。值得注意的是，conv2和conv4，conv5的两个分叉。这样做主要是因为当时Alex的显卡不够强大，为了减少计算量同时方便并行，所以采用了分组计算。另外，fc6和fc7使用了dropout。 GoogleNet将层数推进到了22层，而且创新的提出了构件网络的单元Inception模块。参数总量不到700万。 Network In Network(NIN)1 x 1卷积相当于对所有的输入特征响应图做了一次线性组合，然后输出新的一组特征响应图。 Inception结构Inception模块示意图如下图所示：因为所有卷积的stride都是1，所以在图中没有特意标明，另外对于3 x3卷积，5 x 5卷积和3 x 3池化，为了保持特征响应图大小一直，都用了零填充（3 x3卷积填充为1，5 x 5的填充为2）。在输出前有个concatenate层，直译过来就是“并置”。这个操作的意思是把4组不同类型但是大小相同的特征响应图一张张“并排叠”一起，形成新的一组特征响应图。所以通过上图可以看到，Inception里面主要做了两件事：第一件事是通过1 x 1,3 x 3,5 x 5这三种不同尺度的卷积核，一共四种方式对输入的特征响应图做了特征抽取。第二件事是为了降低计算量，同时让信息通过更少的连接传递以达到更加稀疏的特性，采用1 x 1卷积核进行降维。可以看到，对于计算量略大的3 x 3卷积，把192通道的特征响应图降到了原来的一半96通道，对于更大计算量的5 x 5卷积，则降到了更少的16通道。 网络结构共22层1.3个loss单元，就是训练中计算代价函数的对应单元，目的是为了帮助网咯的收敛。2.最后一个Inception模块输出7 x 7大小的832通道的特征响应图后，并没有像AlexNet那样降维然后过两个全连接，而是直接对每个特征响应图求了平均值，得到了一个1024维的向量，然后再过一个全连接得到和输出数目对应的1000维向量用于分类，从某种程度上讲，这算是舍弃了经典的最后一/二层全连接的做法。大大减少了参数量。 ResNet2015年何恺明构建了残差网络。比较经典的ResNet一般是3种结构：50层、101层、152层https://github.com/KaimingHe/deep-residual-networks 退化问题随着层数的加深到一定程度之后，越深的网络反而效果越差，尽管已经有了很多有效的办法，但梯度的衰减仍然存在。 残差单元残差指的是预测值和观测值之间的差异，误差是值观测值和真实值之间的差异，不要弄混了。为了解决退化问题，提出了残差模块，大体思路为，既然单位映射在梯度下降框架下不起作用，那么索性直接把输入传到输出端，“强行”作为单位映射的部分，让可学习的网络作为另一部分，这就是残差学习的模块。如下图中a所示。可以看到，数据经过了两条路线，一条是和一般网络类似的经过两个卷积层再传递到输出，另一条是实现单位映射的直线连接路线，这个路线被称为shortcut。这样做之后，如果像前面说的那样，前面层的参数已经达到了一个很好的水平，那么再基本构建模块时，输入的信息通过shortcut得以一定程度的保留。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据集介绍]]></title>
    <url>%2F2018%2F06%2F07%2F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[MNIST–深度学习入门者必用Hello Worle级的数据MNIST手写体数据集，全称是Mixed National Institute of Standards and Technology。共70000个样本，其中60000个训练集，10000个测试集。均为分辨率为28*28的灰度图，其中一半是书写比较工整的，另一半是相对潦草的，这两种书写在训练集和测试集中也分别占一半。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2F2018%2F05%2F27%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[zookeeper 主要是写分布式程序可总结为：一致、有头、数据数 总述使用zookeeper开发自己的分布式系统要注意的问题： 解决数据一致性的问题 协调各种“动物”hadoop 小象impala 黑斑羚shark 鲨鱼hive 蜂巢mahout 象夫zookeeper 动物园管理员 google三论文GFS → HDFSBigTable → HBaseMapReduce → HadoopMRchubby → zookeeper zookeeper是什么？NoSQL数据库CAP原理]]></content>
      <categories>
        <category>大数据相关</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[物体检测]]></title>
    <url>%2F2018%2F05%2F27%2F%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[只是了解一点点，就写在这里了，后续陆续更新物体检测 object-detection分类+位置14年开始热门起来 R-CNN基于区域的CNN步骤： 随机扣一块区域，输入至CNN，看是否为想要识别的物体，选择性搜索、出框，一般2000次 将框Resize改变成同样大小的框 输入至CNN中，生成256*256的图像 使用SVM分类 Fast R-CNN进步：将起初的先出框再卷积改为先卷积再出框，减少了卷积次数步骤： 先卷积，再出框，一般为2000个区域 ROI Pooling框出区域，值取max softmax，多类输出 RPI Pooling需要输入： 卷积后的结果 生成区域框的边框，标签为左下角和右上角的坐标 Faster R-CNNSSD单发多框检测器图片 → 卷积 → 出框softmax →n+1分类（n类+背景）→ 卷积 → 出框]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>物体检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memcached安装及简单命令]]></title>
    <url>%2F2018%2F05%2F25%2Fmemcached%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ubuntu16.0.4下memcached的安装 linux系统安装memcached首先要安装libevent库 sudo apt-get install libevent-deve sudo apt-get install memcached 若linux系统为centos则命令为 yum install libevent libevent-deve yum install memcached memcached的连接与关闭启动memcached连接找到memcached的安装目录，自动安装memcached在/usr/local/bin/memcached路径下 memcached -u root -d -m 128m -p 11211 连接memcached语法为：telnet HOST PORT本实例的memcached服务运行的主机为127.0.0.1(本机)，端口为11211 telnet 127.0.0.1 11211 连接成功如下图所示：退出命令 quit 关闭memcached与windows直接输入memcached.exe -d stop关闭memcached不同linux需先知道memcached的进程号，再将其杀死查看进程号 stats 或者 ps -ef|grep memcached 知道了memcached对应的进程号pid后，使用kill命令杀死进程即可。注意：杀死进程前必须quit退出连接 kill 5070 杀死进城后再次连接memcached失败，说明memcached已经被关闭。 memcached的命令存储命令1.setset用于将value存储于key中，若set的key已经存在，该命令可以更新key所对应的原来的数据。语法格式如下： set key flag exptime bytes [noreply] value key：键值对中的key，用于查找缓存值flag：可以包含键值对的整型参数，客户机使用它存储关于键值对的额外信息exptime：再缓存中保存键值对的时间长度，以秒为单位，0表示永远bytes：在缓存中存储的字节数noreply：可选参数，该参数告知服务器不需要返回数据value：存储的值，始终位于第二行 2.addadd用于将value存储在指定的key中，如果add的key已经存在，则不会更新数据，与之前的值仍然保持相同，会得到NOT_STORED的响应，但是过期的key会更新。语法格式如下： add key flags exptime bytes [noreply] value 3.replacereplace用于替换已经存在的key的value，如果可以不存在，则替换失败，并且得到NOT_STORED的响应语法格式如下： replace key flags exptime bytes [noreply] value 4.appendappend用于向已经存在key的value后面追加数据语法格式如下： append key flags exptime bytes [noreply] value 5.prependprepend命令用于向已经存在key的value前面追加数据语法格式如下： prepend key flags exptime bytes [noreply] value 6.cascas用于执行一个“检查并设置”的操作，它仅在当前客户端最后一次取值后，该key对应的值没有被其他客户端修改的情况下才能够将值写入。检查是通过cas_token参数进行的，这个参数是memcached指定给已经存在的元素的一个唯一的64位值。语法格式如下： cas key flags exptime bytes unique_cas_token [noreply] value unique_cas_token是通过gets命令获取的一个唯一的64位值 查找命令1.getget用于获取存储在key中的value，如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。语法格式如下： get key get key1 key2 key3 2.getsgets用于获取CAS令牌存的value,如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。语法格式如下： gets key gets key1 key2 key3 3.deletedelete命令用于删除已经存在的key。语法格式如下： delete key [noreply] 4.incr/decrincr和decr用于对已经存在的key的数字进行自增或自减操作。但是惭怍的数据必须是十进制的32位无符号整数，若key不存在，返回NOT_FOUND，若键的值不为数字，则返回CLIENT_ERROR，其他错误返回ERROR。语法格式如下： incr key increment_values [noreply] decr key decrement_values [noreply] increment_values为增加的数值decrement_values为减少的数值 5.flush_allflush_all用于清理缓存中的所有的键值对，该命令提供了一个可选参数time，用于在制定的时间后执行清理缓存操作。语法格式如下： flush_all [time] [noreply]]]></content>
      <categories>
        <category>大数据相关</category>
      </categories>
      <tags>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis安装及简单命令]]></title>
    <url>%2F2018%2F05%2F25%2Fredis%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Redis：REmote DIctionary Server Redis(远程字典服务器)是完全开源免费的，用C语言编写，是一个高性能的（key/value）分布式内存数据库，基于内存运行并支持持久化的NoSQL数据库，是当前最热门的NoSQL数据库之一，也被人们称之为结构数据服务器。 Redis逐步取代memcached的原因1.redis支持数据持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用2.redis不仅仅直迟简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储3.redis支持数据的备份，即master-slave魔术的数据备份 Redis能做什么？1.内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务2.取最新N个数据的操作，如：可以将最新的10条评论的ID放在redis的List集合里面3.模拟类似于HttpSession这种需要设定过期时间的功能4.发布、订阅消息系统5.定时器、计数器 与memcached区别都是key-value存储memcached一旦服务关闭，数据会全部没有redis服务关闭重启后数据还在 安装Redis我们的环境：VMware WorkplaceCentOS-6.5redis-4.0.9下载地址： Http://redis.io将下载好的redis-4.0.9放入虚拟机中，并解压 tar -zxvf redis-4.9.0.tar.gz redis内包含的文件输入命令 make 运行运行makefile文件，要有GCC，没有则会报错。安装gcc： yum install gcc-c++ 安装完成之后要进行二次make，但是之前要将上一次make不成功的残余文件清理，之后再make make disclean make make完成后，执行 make install 出现下图所示即安装成功 配置redis.conf进入redis-4.0.9,将redis.conf拷贝一份，在拷贝后的上面进行修改将redis.conf拷贝至myredis文件夹下 vim redis.conf 将no修改为yes 启动redis服务cd /usr/local/bin redis-server /home/myy/hadoop/myredis/redis.config 进入客户端客户端默认端口为6379判断是否与服务端连接成功查看服务状态关闭连接关闭后查看服务就没有了 基础了解默认库Redis默认有16个库，进入时默认在0号库，角标从0开始，某些任务找一号库，某些找二号库，任务逻辑更清晰redis.conf中有说明转换库的命令，eg：切换为8号库 flushdb和flushall的区别flushdb是删除当前库，flushall是删除全部库 Benchmark查看本机状态 常用五大数据类型1.string字符串String是redis最基本的类型，可以理解为与memcached一摸一样的类型，一个key对应一个value。String类型是二进制安全的，即redis的string可以包含任何数据，比如jpg图片或者序列化对象string是redis最基本的数据类型，一个redis中字符串最多可以是512M 2.hash哈希redis hash是一个键值对集合，是一个string类型的filed和value的映射表，适合用于存储对象。类似java中的Map&lt;String,Object&gt; 3.list列表列表是简单的字符串列表，按照插入顺利排序。可以添加一个元素到列表的头部（左边）或者尾部（右边），它的底层实际是个链表 4.set集合set是string类型的无序集合。是通过HashTable实现的。 5.Zset有序集合Zset（sorted set）zset和set一样也是string类型元素的集合，且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的承宣是唯一的，但分数（score）是可以重复的。 常用关键字set/get/exists/keys/move/mset/mgetset设置键值，也可以覆盖原来的值get获取对应键的值exists查看某个key是否存在move移动到别的库内mset/mget批量设置/获取键值 expire/ttl/setexexpire设置秒数，过期后自动消失ttl 查看某个key还有多久时间setex设置值时同时设置时间 append/strlen/getrange/setrange/incr/decr/incrby/decrbyappend补充字符串strlen字符串的长度getrange获取指定区域范围内的值，0到-1表示全部setrange设置指定区域范围内的值incr递增加1decr递减少1incrby decrby自定义数量string类型此命令不可用 lpush/lrange/lpop/rpop/lidex/llenlpush和rpush查看后顺序不同lrange查看listlpop栈顶出去rpop栈底出去lidex索引llen查看list长度 lren/ltrim/rpoplpush/lset/linsertlrem删除n和valueltrim截取指定范围内的值再赋值给listrpoplpush将list01栈底给list02栈顶lset替换某位置的值linsert某值之前或之后插入某值 sadd/smembers/sismember/scard/srem/srandmembersadd设置set集合（重复自动留一个）smembers查看set集合sismember查看set内是否有某值scard获取集合内元素个数srem删除集合内某值srandmember集合中随机出几个数 spop/smove/sdiff/sinter/sunionspop随机出栈smove将一set中某一值赋给另一setsdiff取两集合的差集：在第一个中而不在第二个中sinter取两集合的交集sunion取两集合的并集 zrange/zrevrange/zcountzrange同list相同zrevrange从高到低排序zincrby修改某个值的分数zcount返回指定分数范围内值的个数 hash相关的关键字kv模式不变，但v是一个键值对]]></content>
      <categories>
        <category>大数据相关</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 分布式搭建]]></title>
    <url>%2F2018%2F05%2F25%2Fhadoop-%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[linux环境下搭建hadoop集群 准备工具VMware-workstation-10.0.1注册机CentOS-6.5-x86_64-bin-DVD1jdk-7u79-linux-x64hadoop-2.6.4.tar 新建虚拟机解压VMware-workstation-10.0.1注册机,打开VMware Workstation主页,点击新建虚拟机,选择典型,如下:点击下一步,选择安装程序光盘映像文件,浏览找到你下载CentOS-6.5-x86_64-bin-DVD1的压缩包文件,如下:继续点击下一步,填写用户名和密码(尽量简单),填好后点击下一步,为即将创建的虚拟机命名并选择安装路径(最好不要安装在C盘),如下所示:继续点击下一步至如下界面:点击自定义硬件可以修改虚拟机的各项参数,如果电脑内存小于等于4GB,需要将内存改至512MB,否则严重卡顿。修改完成后点击完成，虚拟机就创建成功，打开后界面如下：若要批量创建虚拟机，可以在创建好的虚拟机的基础上进行克隆操作， 安装jdk打开一个虚拟机，右键单击桌面选择Open in Terminal，进入编辑界面： 假设用户名为wxx获取root权限su cd /etc vi sudoers i 进入编辑状态，在 root ALL=(ALL) ALL的下一行编辑 wxx ALL=(ALL) ALL 按ESC键，退出编辑格式按Shift + :输入wq!保存并退出 创建hadoop文件夹cd mkdir hadoop 将jdk-7u79-linux-x64安装包复制到hadoop文件目录下（与windows环境下类似）。 解压jdk-7u79-linux-x64.gz文件cd cd hadoop tar-zxvf jdk-7u79-linux-x64.gz 设置jdk环境变量cd cd hadoop su gedit /etc/profile 进入后在最后一行添加以下指令： export JAVA_HOME=/home/by/hadoop/jdk1.8.0_11 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar点击保存后关闭，输入以下指令使jdk生效： source /etc/profile 检查jdk是否安装成功java -version 成功后显示如下信息： java version “1.7.0_79” Java(TM) SE Runtime Environment (build 1.7.0_79-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode) 创建集群克隆虚拟机将已经安装好jdk的虚拟机克隆两个，创建三个虚拟机的集群。 修改hostnamesu vi /etc/sysconfig/network 将三个虚拟机分别命名master、slave1、slave2如图：完成后重启虚拟机reboot 将三个虚拟机的ip地址相互连接首先必须确保虚拟机联网，如果NET模式连不上网，则选中桥接模式。网络通畅后执行以下操作:1.查看三台虚拟机IP,分别对三个虚拟机执行指令ifconfig，查看各虚拟机ip地址 2.在master中执行以下指令 su cd/etc gedit /etc/hosts 192.168.142.142 192.168.142.143 进入编辑界面后按“IP地址 hostname”填写信息，如图：填写完后按Save按钮，关闭编辑页。 3.将配置好的文件复制到slave1、slave2中,在master中执行以下指令： scp /etc/hosts root@slave1:/etc/ scp /etc/hosts root@slave2:/etc/ 4.检查各虚拟机是否互联,在master中执行以下指令： ping slave1 ping slave2连通即完成 配置SSH无密钥登录1.关闭防火墙,对每个虚拟机进行如下操作： su chkconfig iptables off 执行后重启虚拟机： reboot 2.关闭防火墙后在master下执行以下指令： cd ssh-keygen –t rsa cd .ssh cat id_rsa.pub &gt;&gt; authorized_keys chmod 600 authorized_keys scp authorized_keys wxx@slave1:~/.ssh/ scp authorized_keys wxx@slave2:~/.ssh/ 3.检查无密钥登录是否成功 ssh slave1 ssh slave2 ssh master成功后显示如下： 安装并配置hadoop-2.6.4(在master中)1.将hadoop-2.6.4.tar.gz安装包复制到hadoop文件目录下（与windows环境下类似）。 2.解压hadoop-2.6.4.tar.gz cd cd hadoop tar -zxvf hadoop-2.6.4.tar.gz 3.配置hadoop-2.6.4的各项文件 cd cd hadoop/hadoop-2.7.4 cd etc/hadoop gedit hadoop-env.sh 在最后一行添加: export JAVA_HOME=/home/by/hadoop/ jdk1.8.0_11 编辑core-site.xmlgedit core-site.xml 添加代码： &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/by/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ds.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:54310&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; 编辑hdfs-site.xmlgedit hdfs-site.xml 添加代码： &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/by/hadoop/dfs/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/by/hadoop/dfs/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; 编辑mapred-site.xmlgedit mapred-site.xml 注意：必须先复制mapred-site.xml.template文件更名为mapred-site.xml添加代码： &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; 编辑yarn-site.xmlgedit yarn-site.xml 添加代码： &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; 编辑mastergedit master 添加代码： master 编辑slavesgedit slaves 添加代码： master slave1 slave2 4.将配置好的文件复制到slave1、slave2中 cd cd hadoop scp -r hadoop-2.7.4 slave1:~/hadoop scp -r hadoop-2.7.4 slave2:~/hadoop 5.启动集群 cd cd hadoop/hadoop-2.7.4 bin/hdfs namenode -format sbin/start-dfs.sh sbin/start-yarn.sh sbin/hadoop-daemon.sh start secondarynamenode 6.检查集群情况 jps 三台虚拟机如下所示：]]></content>
      <categories>
        <category>大数据相关</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX入门]]></title>
    <url>%2F2018%2F05%2F25%2FLaTeX%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Hello World打开WinEdt，建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8。 \documentclass{article} \begin{document} hello, world \end{document} 然后在WinEdt的工具栏中找到编译按钮（在垃圾桶和字母B中间），在下拉菜单中选择XeTeX，并点击编译。如果顺利的话，就可以顺利生成出第一个pdf文件，点击工具栏中的放大镜按钮就可以快速打开生成的pdf文件。 标题、作者、章节和段落建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。 \documentclass{article} \author{My Name} \title{The Title} \begin{document} \maketitle hello, world % This is comment \end{document} 效果图如下： 加入目录建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。 \documentclass{article} \begin{document} \tableofcontents \section{Hello China} China is in East Asia. \subsection{Hello Beijing} Beijing is the capital of China. \subsubsection{Hello Dongcheng District} \paragraph{Hello Tian&apos;anmen Square}is in the center of Beijing \subparagraph{Hello Chairman Mao} is in the center of Tian&apos;anmen Square \end{document} 效果图如下： 段落和换行\documentclass{article} \begin{document} Beijing is the capital of China. New York is the capital of America. Amsterdam is \\ the capital \\ of Netherlands. \end{document} 效果图如下： 数学公式\documentclass{article} \usepackage{amsmath} \usepackage{amssymb} \begin{document} The Newton&apos;s second law is F=ma. The Newton&apos;s second law is $F=ma$. The Newton&apos;s second law is F=ma The Newton&apos;s second law is F=ma Greek Letters $\eta$ and $\mu$ Fraction $\frac{a}{b}$ Power $a^b$ Subscript $a_b$ Derivate $\frac{\partial y}{\partial t} $ Vector $\vec{n}$ Bold $\mathbf{n}$ To time differential $\dot{F}$ Matrix (lcr here means left, center or right for each column) \[ \left[ \begin{array}{lcr} a1 &amp; b22 &amp; c333 \\ d444 &amp; e555555 &amp; f6 \end{array} \right] \] Equations(here \&amp; is the symbol for aligning different rows) \begin{align} a+b&amp;=c\\ d&amp;=e+f+g \end{align} \[ \left\{ \begin{aligned} &amp;a+b=c\\ &amp;d=e+f+g \end{aligned} \right. \] \end{document} 效果图如下： 插入图片先搜索到一个将图片转成eps文件的软件，很容易找的，然后将图片保存为一个名字如figure1.eps。建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，放在和图片文件同一个文件夹里，编译并观察现象。 \documentclass{article} \usepackage{graphicx} \begin{document} \includegraphics[width=4.00in,height=3.00in]{figure1.eps} \end{document} 简单表格\documentclass{article} \begin{document} \begin{tabular}{|c|c|} a &amp; b \\ c &amp; d\\ \end{tabular} \begin{tabular}{|c|c|} \hline a &amp; b \\ \hline c &amp; d\\ \hline \end{tabular} \begin{center} \begin{tabular}{|c|c|} \hline a &amp; b \\ \hline c &amp; d\\ \hline \end{tabular} \end{center} \end{document} 效果图如下：]]></content>
      <categories>
        <category>文档编写</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战--决策树]]></title>
    <url>%2F2018%2F05%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据缺点：可能会产生过度匹配问题使用数据类型：数值型和标称型专家系统中，经常使用决策树 trees.pyfrom math import log import operator createDataSet()创建数据集 def createDataSet(): # 数据集中两个特征&apos;no surfacing&apos;,&apos;flippers&apos;, 数据的两个类标签&apos;yes&apos;,&apos;no #dataSet是个list dataSet = [[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]] labels = [&apos;no surfacing&apos;,&apos;flippers&apos;] return dataSet, labels calcShannonEnt(dataSet)计算给定数据集的熵 def calcShannonEnt(dataSet): numEntries = len(dataSet) #计算数据集中实例的总数 labelCounts = {} #创建空字典 for featVec in dataSet: #提取数据集每一行的特征向量 currentLabel = featVec[-1] #获取特征向量最后一列的标签 # 检测字典的关键字key中是否存在该标签，如果不存在keys()关键字，将当前标签/0键值对存入字典中,并赋值为0 #print(labelCounts.keys()) if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 #print(labelCounts) labelCounts[currentLabel] += 1 #否则将当前标签对应的键值加1 #print(&quot;%s=&quot;%currentLabel,labelCounts[currentLabel]) shannonEnt = 0.0 #初始化熵为0 for key in labelCounts: prob = float(labelCounts[key])/numEntries #计算各值出现的频率 shannonEnt -= prob * log(prob,2) #以2为底求对数再乘以出现的频率，即信息期望值 #print(&quot;%s=&quot;%labelCounts[key],shannonEnt) return shannonEnt splitDataSet(dataSet, axis, value)按照给定特征划分数据集得到熵之后，还需划分数据集，以便判断当前是否正确地划分了数据集，三个输入参数分别为：带划分的数据集，划分数据集的特征，需要返回的特征得值，挑选出dataSet中axis位置值为value的剩余部分。 def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: #筛选出dataSet中axis位置值为value #列表的索引中冒号的作用，a[1: ]表示该列表中的第1个元素到最后一个元素，而a[ : n]表示从第0歌元素到第n个元素(不包括n) reducedFeatVec = featVec[:axis] #取出特定位置前面部分并赋值给reducedFeatVec #print(featVec[axis+1:]) #print(reducedFeatVec) reducedFeatVec.extend(featVec[axis+1:]) #取出特定位置后面部分并赋值给reducedFeatVec retDataSet.append(reducedFeatVec) #print(retDataSet) return retDataSet chooseBestFeatureToSplit(dataSet)选择最好的数据集划分方式选取特征，划分数据集，计算得出最好的划分数据集的特征 def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 #计算特征数量，即每一列表元素具有的列数，再减去最后一列为标签，故需减去1 baseEntropy = calcShannonEnt(dataSet) #计算信息熵，此处值为0.9709505944546686，此值将与划分之后的数据集计算的信息熵进行比较 bestInfoGain = 0.0;bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] #创建标签列表 #print(featList) uniqueVals = set(featList) #确定某一特征下所有可能的取值,set集合类型中的每个值互不相同 #print(uniqueVals) newEntropy = 0.0 for value in uniqueVals: #计算每种划分方式的信息熵 subDataSet = splitDataSet(dataSet, i, value) #抽取该特征的每个取值下其他特征的值组成新的子数据集 prob = len(subDataSet)/float(len(dataSet)) #计算该特征下的每一个取值对应的概率（或者说所占的比重） newEntropy += prob * calcShannonEnt(subDataSet) #计算该特征下每一个取值的子数据集的信息熵，并求和 infoGain = baseEntropy - newEntropy #计算每个特征的信息增益 #print(&quot;第%d个特征是的取值是%s，对应的信息增益值是%f&quot;%((i+1),uniqueVals,infoGain)) if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i #print(&quot;第%d个特征的信息增益最大，所以选择它作为划分的依据，其特征的取值为%s,对应的信息增益值是%f&quot;%((i+1),uniqueVals,infoGain)) return bestFeature majorityCnt(classList)递归构建决策树，返回出现次数最多的分类名称 def majorityCnt(classList): classCount={} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] createTree(dataSet,labels)创建树,参数为数据集和标签列表 def createTree(dataSet,labels): classList = [example[-1] for example in dataSet] #提取dataset中的最后一列——种类标签 #print(classList) if classList.count(classList[0]) == len(classList): #计算classlist[0]出现的次数,如果相等，说明都是属于一类，不用继续往下划分 return classList[0] #递归结束的第一个条件是所有的类标签完全相同，则直接返回该类标签 #print(dataSet[0]) if len(dataSet[0]) == 1: #看还剩下多少个属性，如果只有一个属性，但是类别标签有多个，就直接用majoritycnt()进行整理，选取类别最多的作为返回值 return majorityCnt(classList) #递归结束的第二个条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组，则返回出现次数最多的类别 bestFeat = chooseBestFeatureToSplit(dataSet) #选取信息增益最大的特征作为下一次分类的依据 bestFeatLabel = labels[bestFeat] #选取特征对应的标签 #print(bestFeatLabel) myTree = {bestFeatLabel:{}} #创建tree字典，下一个特征位于第二个大括号内，循环递归 del(labels[bestFeat]) #删除使用过的特征 featValues = [example[bestFeat] for example in dataSet] #特征值对应的该栏数据 #print(featValues) uniqueVals = set(featValues) #找到featvalues所包含的所有元素，去重复 for value in uniqueVals: subLabels = labels[:] #将使用过的标签删除更新后，赋值给新的列表，进行迭代 #print(subLabels) myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat,value),subLabels) #循环递归生成树 return myTree classify(inputTree,featLabels,testVec):测试算法，使用决策树执行分类 def classify(inputTree,featLabels,testVec): firstStr = list(inputTree.keys())[0] #找到树的第一个分类特征，或者说根节点&apos;no surfacing&apos; #print(firstStr) secondDict = inputTree[firstStr] #从树中得到该分类特征的分支，有0和1 #print(secondDict) featIndex = featLabels.index(firstStr) #根据分类特征的索引找到对应的标称型数据值，&apos;no surfacing&apos;对应的索引为0 #print(featIndex) key = testVec[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): classLabel = classify(valueOfFeat, featLabels, testVec) else: classLabel = valueOfFeat return classLabel storeTree(inputTree,filename)决策树的存储，使用pickle序列化对象，可在磁盘中保存对象。 def storeTree(inputTree,filename): import pickle fw = open(filename,&apos;wb&apos;) #二进制写入&apos;wb&apos; pickle.dump(inputTree,fw) #pickle的dump函数将决策树写入文件中 fw.close() def grabTree(filename): import pickle fr = open(filename,&apos;rb&apos;) #对应于二进制方式写入数据，&apos;rb&apos;采用二进制形式读出数据 return pickle.load(fr) trees_main.pyimport trees from imp import reload import treePlotter 创建数据集myDat,labels=trees.createDataSet() #print(myDat) #print(labels) #print(trees.calcShannonEnt(myDat)) 熵增大的原因熵越高，混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大 #myDat[1][-1]=&apos;maybe&apos;#更改list中某一元素的值（除yes和no外的值），即为添加更多的分类，中括号中为对应元素行列的位置 #print(myDat) #print(trees.calcShannonEnt(myDat)) #分类变多，熵增大 append()和extend()两类方法的区别a=[1,2,3] b=[4,5,6] a.append(b) #print(a)#[1, 2, 3, [4, 5, 6]] a.extend(b) #print(a)#[1, 2, 3, [4, 5, 6], 4, 5, 6] 按照给定特征划分数据集#print(myDat) #print(trees.splitDataSet(myDat,0,1)) #print(trees.splitDataSet(myDat,0,0)) 选择最好的数据集划分方式#print(myDat) #print(trees.chooseBestFeatureToSplit(myDat)) 创建树,参数为数据集和标签列表myTree=trees.createTree(myDat,labels) #print(myTree) myDat,labels=trees.createDataSet() myTree1=treePlotter.retrieveTree(0) #print(myTree1) #print(trees.classify(myTree1,labels,[1,0])) #print(trees.classify(myTree,labels,[1,1])) 决策树的存储trees.storeTree(myTree,&apos;classifierStorage.txt&apos;) #print(trees.grabTree(&apos;classifierStorage.txt&apos;)) 使用决策树预测隐形眼镜类型fr=open(&apos;lenses.txt&apos;) lenses = [inst.strip().split(&apos;\t&apos;) for inst in fr.readlines()] #将文本数据的每一个数据行按照tab键分割，并依次存入lenses lensesLabels = [&apos;age&apos;, &apos;prescript&apos;, &apos;astigmatic&apos;, &apos;tearRate&apos;] # 创建并存入特征标签列表 lensesTree = trees.createTree(lenses, lensesLabels) # 根据继续文件得到的数据集和特征标签列表创建决策树 print(lensesTree) treePlotter.createPlot(lensesTree) treePlotter.pypython中使用Matplotlib注解绘制树形图 import matplotlib.pyplot as plt 定义文本框和箭头格式decisionNode = dict(boxstyle=&quot;sawtooth&quot;, fc=&quot;0.8&quot;) # boxstyle为文本框的类型，sawtooth是锯齿形，fc是边框线粗细,pad指的是外边框锯齿形（圆形等）的大小 leafNode = dict(boxstyle=&quot;round4&quot;, fc=&quot;0.8&quot;) #定义决策树的叶子结点的描述属性，round4表示圆形 arrow_args = dict(arrowstyle=&quot;&lt;-&quot;) #定义箭头属性 plotNode(nodeTxt, centerPt, parentPt, nodeType)绘制带箭头的注解annotate是关于一个数据点的文本nodeTxt为要显示的文本，centerPt为文本的中心点，箭头所在的点，parentPt为指向文本的点annotate的作用是添加注释，nodetxt是注释的内容nodetype指的是输入的节点（边框）的形状 def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=&apos;axes fraction&apos;, xytext=centerPt, textcoords=&apos;axes fraction&apos;, va=&quot;center&quot;, ha=&quot;center&quot;, bbox=nodeType, arrowprops=arrow_args ) def createPlot():第一版构造树函数，后面会改进，所以这里要注释上 #fig = plt.figure(1, facecolor=&apos;white&apos;) #fig.clf() #createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses #plotNode(&apos;a decision node&apos;, (0.5, 0.1), (0.1, 0.5), decisionNode) #plotNode(&apos;a leaf node&apos;, (0.8, 0.1), (0.3, 0.8), leafNode) #plt.show() getNumLeafs(myTree)计算叶子节点的个数构造注解树，需要知道叶节点的个数，以便可以正确确定x轴的长度；要知道树的层数，可以确定y轴的高度。 def getNumLeafs(myTree): numLeafs = 0 firstStr = list(myTree.keys())[0] #获得myTree的第一个键值，即第一个特征，分割的标签 #print(firstStr) secondDict = myTree[firstStr] #根据键值得到对应的值，即根据第一个特征分类的结果 #print(secondDict) for key in secondDict.keys(): #获取第二个小字典中的key if type(secondDict[key]).__name__==&apos;dict&apos;: #判断是否小字典中是否还包含新的字典（即新的分支） numLeafs += getNumLeafs(secondDict[key]) #包含的话进行递归从而继续循环获得新的分支所包含的叶节点的数量 else: numLeafs +=1 #不包含的话就停止迭代并把现在的小字典加一表示这边有一个分支 return numLeafs def getTreeDepth(myTree): #计算判断节点的个数 maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__==&apos;dict&apos;: thisDepth = 1 + getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth: maxDepth = thisDepth return maxDepth retrieveTree(i)预先存储树信息 def retrieveTree(i): listOfTrees =[{&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}}}, {&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: {&apos;head&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}, 1: &apos;no&apos;}}}} ] return listOfTrees[i] plotMidText(cntrPt, parentPt, txtString)作用是计算tree的中间位置，cntrPt起始位置,parentPt终止位置,txtString文本标签信息 def plotMidText(cntrPt, parentPt, txtString): xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] #cntrPt起点坐标，子节点坐标，parentPt结束坐标，父节点坐标 yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] #找到x和y的中间位置 createPlot.ax1.text(xMid, yMid, txtString, va=&quot;center&quot;, ha=&quot;center&quot;, rotation=30) def plotTree(myTree, parentPt, nodeTxt): numLeafs = getNumLeafs(myTree) depth = getTreeDepth(myTree) firstStr = list(myTree.keys())[0] cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff) #计算子节点的坐标 plotMidText(cntrPt, parentPt, nodeTxt) #绘制线上的文字 plotNode(firstStr, cntrPt, parentPt, decisionNode) #绘制节点 secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD #每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的 for key in secondDict.keys(): if type(secondDict[key]).__name__==&apos;dict&apos;: plotTree(secondDict[key],cntrPt,str(key)) else: plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD createPlot(inTree)def createPlot(inTree): fig = plt.figure(1, facecolor=&apos;white&apos;) #类似于Matlab的figure，定义一个画布，背景为白色 fig.clf() # 把画布清空 axprops = dict(xticks=[], yticks=[]) #subplot定义了一个绘图 createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) #no ticks #createPlot.ax1为全局变量，绘制图像的句柄，111表示figure中的图有1行1列，即1个，最后的1代表第一个图,frameon表示是否绘制坐标轴矩形 plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; plotTree(inTree, (0.5,1.0), &apos;&apos;) plt.show() treePlotter_main.pyimport treePlotter #treePlotter.createPlot() #print(treePlotter.retrieveTree(1)) myTree=treePlotter.retrieveTree(0) #print(treePlotter.getNumLeafs(myTree)) #print(treePlotter.getTreeDepth(myTree)) myTree[&apos;no surfacing&apos;][3]=&apos;maybe&apos; treePlotter.createPlot(myTree)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战--KNN]]></title>
    <url>%2F2018%2F05%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN%2F</url>
    <content type="text"><![CDATA[这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0 knn优 点：精度高，对异常值不敏感，无数据输入假定缺 点：计算复杂度高，空间复杂度高，无法给出数据的内在含义使用数据范围：数值型和标称型 —————————————————————–下面进入正题—————————————————————– kNN.pyfrom numpy import * import operator from os import listdir def createDataSet()def createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = [&apos;A&apos;,&apos;A&apos;,&apos;B&apos;,&apos;B&apos;] return group, labels classify0()inX用于分类的输入向量,是一个向量dataSet输入的训练样本集，是一个矩阵labels标签向量k用于选择最近邻居的数目labels数目与dataSet的行数相同 def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] #返回的是dataSet的行数，行数就是样本的数量 diffMat = tile(inX, (dataSetSize,1)) - dataSet #矩阵相减 #inX是个向量，而dataset是个矩阵，两者之间要进行相减的运算，需要把这个向量也补成一个和dataset有相同行数列数的矩阵， #tile()的第二个参数，就是(datasetsize,1)，这个参数的意思就是把inX补成有datasetsize行数的矩阵。 #假如inX是（1，2），datasetsize =3，那么经过tile()转换后产生了一个这样的矩阵（[1,2],[1,2],[1,2]） sqDiffMat = diffMat**2 #平方 sqDistances = sqDiffMat.sum(axis=1) #按行求和 # sqdiffMat是([1,2],[0,1],[3,4])，axis这个参数影响了对矩阵求和时候的顺序，axis=0是按照列求和，结果为([3.1.7]) # axis=1是按照行进行求和，结果是([4,7])。 distances = sqDistances**0.5 #开方，得到欧氏距离 sortedDistIndicies = distances.argsort() #把向量中每个元素进行排序，结果是元素的索引形成的向量 #例子distance([1,4,3])，经过distance.argsort()之后的结果是([0,2,1] classCount={} #存放最终的分类结果及相应的结果投票数 #投票过程，就是统计前k个最近的样本所属类别包含的样本个数 for i in range(k): # index = sortedDistIndicies[i]是第i个最相近的元素索引，即样本下标 # voteIlabel = labels[index]是样本index对应的分类结果(&apos;A&apos; or &apos;B&apos;) voteIlabel = labels[sortedDistIndicies[i]] # classCount.get(voteIlabel, 0)返回voteIlabel的值，如果不存在，则返回0 # 然后将票数增1 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 # 把分类结果进行排序，然后返回得票数最多的分类结果 # key=operator.itemgetter(1)的意思是按照字典里的第一个排序 #例子a = [1, 2, 3]，b = operator.itemgetter(1)，b(a)返回为2 #b = operator.itemgetter(1, 0)，b(a)，定义函数b，获取对象的第1个域和第0个的值，返回 (2, 1) # reverse=True是降序排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] #返回类别最多的类别 file2matrix()将文本记录转换为NumPy将文本记录转换为NumPy的解析程序输入为矩阵，输出为训练样本矩阵和类标签向量 def file2matrix(filename): fr = open(filename) #打开文档 numberOfLines = len(fr.readlines()) #得到文件行数 #fr.readlines()读取行数,存在数组中,导入后每行中用\t隔开,两行之间用\n换行得到文件行数 returnMat = zeros((numberOfLines,3)) #创建返回NumPy矩阵，numberoflines行，3列的初始化零的矩阵 classLabelVector = []#定义一个空的数组 fr = open(filename) index = 0 for line in fr.readlines(): line = line.strip() #删除（）中的内容，这里表示删除空格 listFromLine = line.split(&apos;\t&apos;)#以\t分割 #print(listFromLine) returnMat[index,:] = listFromLine[0:3]#把每行前三个元素存入returnMat矩阵中，每行中存储三个 classLabelVector.append(int(listFromLine[-1]))#存储第四列元素即标签，在数组中append添加，-1表示最后一列 index += 1 return returnMat,classLabelVector autoNorm()归一化数值，避免某特征值过大，使得权重比例不均匀，对计算结果产生影响。autoNorm可以自动将数字特征值转化为0到1区间 def autoNorm(dataSet): minVals = dataSet.min(0)#一维数组，值为各项特征（列）中的最小值。参数0使得函数从列中选取最小值 #print(minVals) maxVals = dataSet.max(0) #print(maxVals) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) #创建与样本集一样大小的零矩阵 #print(normDataSet) m = dataSet.shape[0]#dataSet的行数 normDataSet = dataSet - tile(minVals, (m,1))#矩阵中所有的值减去最小值 #tile将原来的一个数组minVals，扩充成了m行1列的数组 normDataSet = normDataSet/tile(ranges, (m,1)) #矩阵中所有的值除以最大取值范围进行归一化 return normDataSet, ranges, minVals datingClassTest()测试算法，样本集中百分之九十的数据用来训练样本，百分之十的样本用来测试分类器kNN.classify0()。 def datingClassTest(): hoRatio = 0.10 #百分之十的数据用于测试分类器，更改该变量的值可更改参加测试分类器的数据量 datingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;) #导入数据 normMat, ranges, minVals = autoNorm(datingDataMat) #归一化数值 m = normMat.shape[0] #得到总行数 numTestVecs = int(m*hoRatio) #测试总数据数量，m*hoRatio是一个浮点型，需转化成整形 errorCount = 0.0 #初试错误率为0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) #分类器（需要测试的向量，训练样本集(90%)，标签集合，K） print(&quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, datingLabels[i])) if (classifierResult != datingLabels[i]): errorCount += 1.0 #计数，错误的个数 print(&quot;the total error rate is: %f&quot; % (errorCount/float(numTestVecs))) #错误率 print(errorCount) classifyPerson()约会数据,对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测， def classifyPerson(): resultList=[&apos;not at all&apos;,&apos;in samll doses&apos;,&apos;in large doses&apos;] #三种感兴趣程度 percentTats=float(input(&quot;percentage of time spent playing video games?&quot;)) ffMiles=floats=float(input(&quot;frequent flier miles earned per year?&quot;)) iceCream=float(input(&quot;liters of ice cream consuned per year?&quot;))#input键盘输入 datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;) # 导入数据 normMat,ranges,minvals=autoNorm(datingDataMat) # 归一化，ranges是归一化的分母 inArr=array([ffMiles,percentTats,iceCream]) # inArr是归一化之前的datingDataMat数组中的行 classifierResult=classify0((inArr-minvals)/ranges,normMat,datingLabels,3)#先归一化，然后调用分类函数 #print(classifierResult) print(&quot;you will probably like this person:%s&quot;%resultList[classifierResult-1]) img2vector()图片转向量手写体：3232的黑白图像图片转向量，将3232的二进制图像矩阵转换为1*1024的向量 def img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename) for i in range(32):#循环读出文件的前32行 lineStr = fr.readline() for j in range(32):#将每行的前32个字符存储在NumPy数组中 returnVect[0,32*i+j] = int(lineStr[j]) return returnVect#返回数组 handwritingClassTest()手写体测试 def handwritingClassTest(): hwLabels = [] trainingFileList = listdir(&apos;trainingDigits&apos;) #导入训练数据 #print(trainingFileList) m = len(trainingFileList) #训练数据的总数 #print(m) trainingMat = zeros((m,1024)) #m行1024列的零向量 for i in range(m): fileNameStr = trainingFileList[i] #文件名 fileStr = fileNameStr.split(&apos;.&apos;)[0] #取文件名.之前的名字 classNumStr = int(fileStr.split(&apos;_&apos;)[0]) #取文件名_之前的名字 hwLabels.append(classNumStr) trainingMat[i,:] = img2vector(&apos;trainingDigits/%s&apos; % fileNameStr) #将对应数据集下的文件一个个的转为向量 #print(trainingMat[i,:]) testFileList = listdir(&apos;testDigits&apos;) #测试数据 errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split(&apos;.&apos;)[0] classNumStr = int(fileStr.split(&apos;_&apos;)[0]) vectorUnderTest = img2vector(&apos;testDigits/%s&apos; % fileNameStr) classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) #利用训练的trainingMat测试 print(&quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, classNumStr)) if (classifierResult != classNumStr): errorCount += 1.0 print(&quot;\nthe total number of errors is: %d&quot; % errorCount) print(&quot;\nthe total error rate is: %f&quot; % (errorCount/float(mTest))) knn_main.pyimport kNN import matplotlib import matplotlib.pyplot as plt import numpy as np from imp import reload group,labels=kNN.createDataSet() #print(group) #print(labels) #print(kNN.classify0([0,0],group,labels,3)) 散点图fig=plt.figure() #建立画板 ax=fig.add_subplot(111) #添加一个子图，一行一列第一个子块，若括号内为349，则三行四列第9个子块 reload(kNN) datingDataMat,datingLabels=kNN.file2matrix(&apos;datingTestSet2.txt&apos;) #print(datingDataMat) #ax.scatter(datingDataMat[:,1],datingDataMat[:,2]) # scatter绘制散点图,使用第二列第三列数据 #ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*np.array(datingLabels),15.0*np.array(datingLabels)) ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*np.array(datingLabels),15.0*np.array(datingLabels)) #plt.show() 归一化数值normMat,ranges,minVals=kNN.autoNorm(datingDataMat) #print(normMat) #print(ranges) #print(minVals) 测试算法#kNN.datingClassTest() 约会预测#对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测， #输入10 10000 0.5 #kNN.classifyPerson() 手写体#trainingDigits包含大约2000个例子，每个数字约有200个样本 #testDigits包含大约900个测试数据 testVector=kNN.img2vector(&apos;trainingDigits/0_13.txt&apos;) #print(testVector[0,0:31]) #print(testVector[0,32:63]) #print(testVector[0,64:95]) kNN.handwritingClassTest()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伊始]]></title>
    <url>%2F2018%2F05%2F17%2F%E4%BC%8A%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈 简单随便说说建站方法Hexo就是以Hexo为主，剩下的自行百度吧，毕竟我要回寝室，没时间写了 Github Pages以github为载体实现的，也百度吧，啊啊啊，实验室就剩我自己了。 配置域名在博客的根目录下source文件中(例如：C:\hexo\source)新建一个名为CNAME的文件，注意没有任何后缀，用于github进行读取。在文件中添加自己的域名并保存，例如1amberwu.top 然后，重新生成静态文件并部署。CNAME文件也会被上传到github仓库当中，此时在浏览器中输入自己的域名，回车之后，你会第一次遇见自己的小天地~ Hexo的一些基本命令12hexo g #完整命令为hexo generate,用于生成静态文件hexo s #完整命令为hexo server,用于启动服务器，主要用来本地预览 在浏览器地址栏输入http://localhost:4000/, 按下回车键，熟悉的界面又出现了。12hexo d #完整命令为hexo deploy,用于将本地文件发布到github等git仓库上hexo n "my article" #完整命令为hexo new,用于新建一篇名为“my article”的文章 这样就会在博客目录下source_posts中生成相应的 my article.md文件( 例如 C:\blog\source_posts\my article.md ) Hexo修改及配置主题hexo初始化之后默认的主题是landscape , 然后你可以去这个地址 https://hexo.io/themes/ 里面找到你想要的主题。在github中搜索你要的主题名称，里面都会有该主题的如何使用的介绍，按着来就好了，反正就是改改改！我选的是next,看起来挺不错，至少是我喜欢的类型。更改主题需要修改配置文件更改主题需要修改配置文件，就是根目录下的_config.yml文件，找到 theme 字段，并将其值更改为next即可1theme: next 配置next主题next主题共分三种，在站点根目录/themes/next/_congig.yml 文件中修改，找到scheme关键字即可选择。1234# Schemes#scheme: Muse#scheme: Mistscheme: Pisces 当然，你完全可以进行很多的自定义设置甚至修改源码，定制自己的主题。小女子能力有限，更多的设置请参考官方文档http://theme-next.iissnan.com/getting-started.html添加背景图片将背景图片命名为background.jpg并放入主题根目录/source/images文件夹中打开博客根目录/themes/next/source/css/_custom/custom.styl文件加入如下代码：123456// Custom styles.body &#123; background-image: url(/images/background.jpg); background-attachment: fixed; background-repeat: no-repeat;&#125; 更多资料]]></content>
      <categories>
        <category>随便写写</category>
      </categories>
      <tags>
        <tag>个人随笔</tag>
      </tags>
  </entry>
</search>
