<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AmberWu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://moyingyao.github.io/"/>
  <updated>2018-05-25T07:52:04.485Z</updated>
  <id>http://moyingyao.github.io/</id>
  
  <author>
    <name>AmberWu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LaTeX入门</title>
    <link href="http://moyingyao.github.io/2018/05/25/LaTeX%E5%85%A5%E9%97%A8/"/>
    <id>http://moyingyao.github.io/2018/05/25/LaTeX入门/</id>
    <published>2018-05-25T07:04:26.000Z</published>
    <updated>2018-05-25T07:52:04.485Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><p>打开WinEdt，建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8。<br><a id="more"></a></p><pre><code>\documentclass{article}  \begin{document}  hello, world  \end{document} </code></pre><p>然后在WinEdt的工具栏中找到编译按钮（在垃圾桶和字母B中间），在下拉菜单中选择XeTeX，并点击编译。<br>如果顺利的话，就可以顺利生成出第一个pdf文件，点击工具栏中的放大镜按钮就可以快速打开生成的pdf文件。 </p><h1 id="标题、作者、章节和段落"><a href="#标题、作者、章节和段落" class="headerlink" title="标题、作者、章节和段落"></a>标题、作者、章节和段落</h1><p>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。</p><pre><code>\documentclass{article}  \author{My Name}  \title{The Title}  \begin{document}  \maketitle  hello, world % This is comment  \end{document} </code></pre><p>效果图如下：<br><img src="https://i.imgur.com/KPP0wzq.png" alt=""> </p><h1 id="加入目录"><a href="#加入目录" class="headerlink" title="加入目录"></a>加入目录</h1><p>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。</p><pre><code>\documentclass{article}  \begin{document}  \tableofcontents  \section{Hello China} China is in East Asia.  \subsection{Hello Beijing} Beijing is the capital of China.  \subsubsection{Hello Dongcheng District}  \paragraph{Hello Tian&apos;anmen Square}is in the center of Beijing  \subparagraph{Hello Chairman Mao} is in the center of Tian&apos;anmen Square  \end{document} </code></pre><p>效果图如下：<br><img src="https://i.imgur.com/5KAaLd6.png" alt=""> </p><h1 id="段落和换行"><a href="#段落和换行" class="headerlink" title="段落和换行"></a>段落和换行</h1><pre><code>\documentclass{article}  \begin{document}  Beijing is  the capital  of China.  New York is  the capital  of America.  Amsterdam is \\ the capital \\  of Netherlands.  \end{document} </code></pre><p>效果图如下：<br><img src="https://i.imgur.com/Rpk6Hh2.png" alt=""></p><h1 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h1><pre><code>\documentclass{article}  \usepackage{amsmath}  \usepackage{amssymb}  \begin{document}  The Newton&apos;s second law is F=ma.  The Newton&apos;s second law is $F=ma$.  The Newton&apos;s second law is  F=maThe Newton&apos;s second law is  F=maGreek Letters $\eta$ and $\mu$  Fraction $\frac{a}{b}$  Power $a^b$  Subscript $a_b$  Derivate $\frac{\partial y}{\partial t} $  Vector $\vec{n}$  Bold $\mathbf{n}$  To time differential $\dot{F}$  Matrix (lcr here means left, center or right for each column)  \[  \left[  \begin{array}{lcr}  a1 &amp; b22 &amp; c333 \\  d444 &amp; e555555 &amp; f6  \end{array}  \right]  \]  Equations(here \&amp; is the symbol for aligning different rows)  \begin{align}  a+b&amp;=c\\  d&amp;=e+f+g  \end{align}  \[  \left\{  \begin{aligned}  &amp;a+b=c\\  &amp;d=e+f+g  \end{aligned}  \right.  \]  \end{document} </code></pre><p>效果图如下：<br><img src="https://i.imgur.com/JCSYPxO.png" alt=""></p><h1 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h1><p>先搜索到一个将图片转成eps文件的软件，很容易找的，然后将图片保存为一个名字如figure1.eps。<br>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，放在和图片文件同一个文件夹里，编译并观察现象。</p><pre><code>\documentclass{article}  \usepackage{graphicx}  \begin{document}  \includegraphics[width=4.00in,height=3.00in]{figure1.eps}  \end{document}</code></pre><h1 id="简单表格"><a href="#简单表格" class="headerlink" title="简单表格"></a>简单表格</h1><pre><code>\documentclass{article}\begin{document}\begin{tabular}{|c|c|}a &amp; b \\c &amp; d\\\end{tabular}\begin{tabular}{|c|c|}\hlinea &amp; b \\\hlinec &amp; d\\\hline\end{tabular}\begin{center}\begin{tabular}{|c|c|}\hlinea &amp; b \\ \hlinec &amp; d\\\hline\end{tabular}\end{center}\end{document}</code></pre><p>效果图如下：<br><img src="https://i.imgur.com/JoYKa5s.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Hello-World&quot;&gt;&lt;a href=&quot;#Hello-World&quot; class=&quot;headerlink&quot; title=&quot;Hello World&quot;&gt;&lt;/a&gt;Hello World&lt;/h1&gt;&lt;p&gt;打开WinEdt，建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="文档编写" scheme="http://moyingyao.github.io/categories/%E6%96%87%E6%A1%A3%E7%BC%96%E5%86%99/"/>
    
    
      <category term="LaTeX" scheme="http://moyingyao.github.io/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战--决策树</title>
    <link href="http://moyingyao.github.io/2018/05/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://moyingyao.github.io/2018/05/25/机器学习实战-决策树/</id>
    <published>2018-05-25T02:32:28.000Z</published>
    <updated>2018-05-25T03:05:40.151Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型<br>专家系统中，经常使用决策树<br><a id="more"></a></p><h1 id="trees-py"><a href="#trees-py" class="headerlink" title="trees.py"></a>trees.py</h1><pre><code>from math import log    import operator</code></pre><h2 id="createDataSet"><a href="#createDataSet" class="headerlink" title="createDataSet()"></a>createDataSet()</h2><p>创建数据集</p><pre><code>def createDataSet():    # 数据集中两个特征&apos;no surfacing&apos;,&apos;flippers&apos;, 数据的两个类标签&apos;yes&apos;,&apos;no    #dataSet是个list    dataSet = [[1, 1, &apos;yes&apos;],                 [1, 1, &apos;yes&apos;],                 [1, 0, &apos;no&apos;],                 [0, 1, &apos;no&apos;],                 [0, 1, &apos;no&apos;]]    labels = [&apos;no surfacing&apos;,&apos;flippers&apos;]    return dataSet, labels</code></pre><h2 id="calcShannonEnt-dataSet"><a href="#calcShannonEnt-dataSet" class="headerlink" title="calcShannonEnt(dataSet)"></a>calcShannonEnt(dataSet)</h2><p>计算给定数据集的熵</p><pre><code>def calcShannonEnt(dataSet):    numEntries = len(dataSet)   #计算数据集中实例的总数    labelCounts = {}            #创建空字典    for featVec in dataSet:     #提取数据集每一行的特征向量        currentLabel = featVec[-1]  #获取特征向量最后一列的标签        # 检测字典的关键字key中是否存在该标签，如果不存在keys()关键字，将当前标签/0键值对存入字典中,并赋值为0        #print(labelCounts.keys())        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0        #print(labelCounts)        labelCounts[currentLabel] += 1  #否则将当前标签对应的键值加1        #print(&quot;%s=&quot;%currentLabel,labelCounts[currentLabel])    shannonEnt = 0.0    #初始化熵为0    for key in labelCounts:        prob = float(labelCounts[key])/numEntries   #计算各值出现的频率        shannonEnt -= prob * log(prob,2)    #以2为底求对数再乘以出现的频率，即信息期望值        #print(&quot;%s=&quot;%labelCounts[key],shannonEnt)    return shannonEnt</code></pre><h2 id="splitDataSet-dataSet-axis-value"><a href="#splitDataSet-dataSet-axis-value" class="headerlink" title="splitDataSet(dataSet, axis, value)"></a>splitDataSet(dataSet, axis, value)</h2><p>按照给定特征划分数据集<br>得到熵之后，还需划分数据集，以便判断当前是否正确地划分了数据集，三个输入参数分别为：带划分的数据集，划分数据集的特征，需要返回的特征得值，挑选出dataSet中axis位置值为value的剩余部分。</p><pre><code>def splitDataSet(dataSet, axis, value):    retDataSet = []    for featVec in dataSet:        if featVec[axis] == value:  #筛选出dataSet中axis位置值为value            #列表的索引中冒号的作用，a[1: ]表示该列表中的第1个元素到最后一个元素，而a[ : n]表示从第0歌元素到第n个元素(不包括n)            reducedFeatVec = featVec[:axis] #取出特定位置前面部分并赋值给reducedFeatVec            #print(featVec[axis+1:])            #print(reducedFeatVec)            reducedFeatVec.extend(featVec[axis+1:])     #取出特定位置后面部分并赋值给reducedFeatVec            retDataSet.append(reducedFeatVec)            #print(retDataSet)    return retDataSet</code></pre><h2 id="chooseBestFeatureToSplit-dataSet"><a href="#chooseBestFeatureToSplit-dataSet" class="headerlink" title="chooseBestFeatureToSplit(dataSet)"></a>chooseBestFeatureToSplit(dataSet)</h2><p>选择最好的数据集划分方式<br>选取特征，划分数据集，计算得出最好的划分数据集的特征</p><pre><code>def chooseBestFeatureToSplit(dataSet):    numFeatures = len(dataSet[0]) - 1      #计算特征数量，即每一列表元素具有的列数，再减去最后一列为标签，故需减去1    baseEntropy = calcShannonEnt(dataSet)       #计算信息熵，此处值为0.9709505944546686，此值将与划分之后的数据集计算的信息熵进行比较    bestInfoGain = 0.0;bestFeature = -1    for i in range(numFeatures):        featList = [example[i] for example in dataSet]      #创建标签列表        #print(featList)        uniqueVals = set(featList)       #确定某一特征下所有可能的取值,set集合类型中的每个值互不相同        #print(uniqueVals)        newEntropy = 0.0        for value in uniqueVals:        #计算每种划分方式的信息熵            subDataSet = splitDataSet(dataSet, i, value)        #抽取该特征的每个取值下其他特征的值组成新的子数据集            prob = len(subDataSet)/float(len(dataSet))      #计算该特征下的每一个取值对应的概率（或者说所占的比重）            newEntropy += prob * calcShannonEnt(subDataSet)     #计算该特征下每一个取值的子数据集的信息熵，并求和        infoGain = baseEntropy - newEntropy     #计算每个特征的信息增益        #print(&quot;第%d个特征是的取值是%s，对应的信息增益值是%f&quot;%((i+1),uniqueVals,infoGain))        if (infoGain &gt; bestInfoGain):            bestInfoGain = infoGain            bestFeature = i            #print(&quot;第%d个特征的信息增益最大，所以选择它作为划分的依据，其特征的取值为%s,对应的信息增益值是%f&quot;%((i+1),uniqueVals,infoGain))    return bestFeature</code></pre><h2 id="majorityCnt-classList"><a href="#majorityCnt-classList" class="headerlink" title="majorityCnt(classList)"></a>majorityCnt(classList)</h2><p>递归构建决策树，返回出现次数最多的分类名称</p><pre><code>def majorityCnt(classList):    classCount={}    for vote in classList:        if vote not in classCount.keys(): classCount[vote] = 0        classCount[vote] += 1    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)    return sortedClassCount[0][0]</code></pre><h2 id="createTree-dataSet-labels"><a href="#createTree-dataSet-labels" class="headerlink" title="createTree(dataSet,labels)"></a>createTree(dataSet,labels)</h2><p>创建树,参数为数据集和标签列表</p><pre><code>def createTree(dataSet,labels):    classList = [example[-1] for example in dataSet]       #提取dataset中的最后一列——种类标签    #print(classList)    if classList.count(classList[0]) == len(classList):    #计算classlist[0]出现的次数,如果相等，说明都是属于一类，不用继续往下划分        return classList[0]     #递归结束的第一个条件是所有的类标签完全相同，则直接返回该类标签    #print(dataSet[0])    if len(dataSet[0]) == 1: #看还剩下多少个属性，如果只有一个属性，但是类别标签有多个，就直接用majoritycnt()进行整理，选取类别最多的作为返回值        return majorityCnt(classList)   #递归结束的第二个条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组，则返回出现次数最多的类别    bestFeat = chooseBestFeatureToSplit(dataSet)    #选取信息增益最大的特征作为下一次分类的依据    bestFeatLabel = labels[bestFeat]     #选取特征对应的标签    #print(bestFeatLabel)    myTree = {bestFeatLabel:{}}  #创建tree字典，下一个特征位于第二个大括号内，循环递归    del(labels[bestFeat])   #删除使用过的特征    featValues = [example[bestFeat] for example in dataSet]     #特征值对应的该栏数据    #print(featValues)    uniqueVals = set(featValues)    #找到featvalues所包含的所有元素，去重复    for value in uniqueVals:        subLabels = labels[:]        #将使用过的标签删除更新后，赋值给新的列表，进行迭代        #print(subLabels)        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat,value),subLabels) #循环递归生成树    return myTree                            </code></pre><h2 id="classify-inputTree-featLabels-testVec"><a href="#classify-inputTree-featLabels-testVec" class="headerlink" title="classify(inputTree,featLabels,testVec):"></a>classify(inputTree,featLabels,testVec):</h2><p>测试算法，使用决策树执行分类</p><pre><code>def classify(inputTree,featLabels,testVec):    firstStr = list(inputTree.keys())[0]    #找到树的第一个分类特征，或者说根节点&apos;no surfacing&apos;    #print(firstStr)    secondDict = inputTree[firstStr]    #从树中得到该分类特征的分支，有0和1    #print(secondDict)    featIndex = featLabels.index(firstStr)  #根据分类特征的索引找到对应的标称型数据值，&apos;no surfacing&apos;对应的索引为0    #print(featIndex)    key = testVec[featIndex]    valueOfFeat = secondDict[key]    if isinstance(valueOfFeat, dict):         classLabel = classify(valueOfFeat, featLabels, testVec)    else: classLabel = valueOfFeat    return classLabel</code></pre><h2 id="storeTree-inputTree-filename"><a href="#storeTree-inputTree-filename" class="headerlink" title="storeTree(inputTree,filename)"></a>storeTree(inputTree,filename)</h2><p>决策树的存储，使用pickle序列化对象，可在磁盘中保存对象。</p><pre><code>def storeTree(inputTree,filename):    import pickle    fw = open(filename,&apos;wb&apos;)    #二进制写入&apos;wb&apos;    pickle.dump(inputTree,fw)   #pickle的dump函数将决策树写入文件中    fw.close()def grabTree(filename):    import pickle    fr = open(filename,&apos;rb&apos;)    #对应于二进制方式写入数据，&apos;rb&apos;采用二进制形式读出数据    return pickle.load(fr)</code></pre><h1 id="trees-main-py"><a href="#trees-main-py" class="headerlink" title="trees_main.py"></a>trees_main.py</h1><pre><code>import treesfrom imp import reloadimport treePlotter</code></pre><h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><pre><code>myDat,labels=trees.createDataSet()#print(myDat)#print(labels)#print(trees.calcShannonEnt(myDat))</code></pre><h2 id="熵增大的原因"><a href="#熵增大的原因" class="headerlink" title="熵增大的原因"></a>熵增大的原因</h2><p>熵越高，混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大</p><pre><code>#myDat[1][-1]=&apos;maybe&apos;#更改list中某一元素的值（除yes和no外的值），即为添加更多的分类，中括号中为对应元素行列的位置#print(myDat)#print(trees.calcShannonEnt(myDat))  #分类变多，熵增大</code></pre><h2 id="append-和extend-两类方法的区别"><a href="#append-和extend-两类方法的区别" class="headerlink" title="append()和extend()两类方法的区别"></a>append()和extend()两类方法的区别</h2><pre><code>a=[1,2,3]b=[4,5,6]a.append(b)#print(a)#[1, 2, 3, [4, 5, 6]]a.extend(b)#print(a)#[1, 2, 3, [4, 5, 6], 4, 5, 6]</code></pre><h2 id="按照给定特征划分数据集"><a href="#按照给定特征划分数据集" class="headerlink" title="按照给定特征划分数据集"></a>按照给定特征划分数据集</h2><pre><code>#print(myDat)#print(trees.splitDataSet(myDat,0,1))#print(trees.splitDataSet(myDat,0,0))</code></pre><h2 id="选择最好的数据集划分方式"><a href="#选择最好的数据集划分方式" class="headerlink" title="选择最好的数据集划分方式"></a>选择最好的数据集划分方式</h2><pre><code>#print(myDat)#print(trees.chooseBestFeatureToSplit(myDat))</code></pre><h2 id="创建树-参数为数据集和标签列表"><a href="#创建树-参数为数据集和标签列表" class="headerlink" title="创建树,参数为数据集和标签列表"></a>创建树,参数为数据集和标签列表</h2><pre><code>myTree=trees.createTree(myDat,labels)#print(myTree)myDat,labels=trees.createDataSet()myTree1=treePlotter.retrieveTree(0)    #print(myTree1)#print(trees.classify(myTree1,labels,[1,0]))#print(trees.classify(myTree,labels,[1,1]))</code></pre><h2 id="决策树的存储"><a href="#决策树的存储" class="headerlink" title="决策树的存储"></a>决策树的存储</h2><pre><code>trees.storeTree(myTree,&apos;classifierStorage.txt&apos;)#print(trees.grabTree(&apos;classifierStorage.txt&apos;))</code></pre><h2 id="使用决策树预测隐形眼镜类型"><a href="#使用决策树预测隐形眼镜类型" class="headerlink" title="使用决策树预测隐形眼镜类型"></a>使用决策树预测隐形眼镜类型</h2><pre><code>fr=open(&apos;lenses.txt&apos;)lenses = [inst.strip().split(&apos;\t&apos;) for inst in fr.readlines()]  #将文本数据的每一个数据行按照tab键分割，并依次存入lenseslensesLabels = [&apos;age&apos;, &apos;prescript&apos;, &apos;astigmatic&apos;, &apos;tearRate&apos;]   # 创建并存入特征标签列表lensesTree = trees.createTree(lenses, lensesLabels)   # 根据继续文件得到的数据集和特征标签列表创建决策树print(lensesTree)treePlotter.createPlot(lensesTree)</code></pre><h1 id="treePlotter-py"><a href="#treePlotter-py" class="headerlink" title="treePlotter.py"></a>treePlotter.py</h1><p>python中使用Matplotlib注解绘制树形图</p><pre><code>import matplotlib.pyplot as plt</code></pre><h2 id="定义文本框和箭头格式"><a href="#定义文本框和箭头格式" class="headerlink" title="定义文本框和箭头格式"></a>定义文本框和箭头格式</h2><pre><code>decisionNode = dict(boxstyle=&quot;sawtooth&quot;, fc=&quot;0.8&quot;)  # boxstyle为文本框的类型，sawtooth是锯齿形，fc是边框线粗细,pad指的是外边框锯齿形（圆形等）的大小leafNode = dict(boxstyle=&quot;round4&quot;, fc=&quot;0.8&quot;)    #定义决策树的叶子结点的描述属性，round4表示圆形arrow_args = dict(arrowstyle=&quot;&lt;-&quot;)  #定义箭头属性</code></pre><h2 id="plotNode-nodeTxt-centerPt-parentPt-nodeType"><a href="#plotNode-nodeTxt-centerPt-parentPt-nodeType" class="headerlink" title="plotNode(nodeTxt, centerPt, parentPt, nodeType)"></a>plotNode(nodeTxt, centerPt, parentPt, nodeType)</h2><p>绘制带箭头的注解<br>annotate是关于一个数据点的文本<br>nodeTxt为要显示的文本，centerPt为文本的中心点，箭头所在的点，parentPt为指向文本的点<br>annotate的作用是添加注释，nodetxt是注释的内容<br>nodetype指的是输入的节点（边框）的形状</p><pre><code>def plotNode(nodeTxt, centerPt, parentPt, nodeType):    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords=&apos;axes fraction&apos;,             xytext=centerPt, textcoords=&apos;axes fraction&apos;,             va=&quot;center&quot;, ha=&quot;center&quot;, bbox=nodeType, arrowprops=arrow_args )</code></pre><h2 id="def-createPlot"><a href="#def-createPlot" class="headerlink" title="def createPlot():"></a>def createPlot():</h2><p>第一版构造树函数，后面会改进，所以这里要注释上</p><pre><code>#fig = plt.figure(1, facecolor=&apos;white&apos;)#fig.clf()#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses#plotNode(&apos;a decision node&apos;, (0.5, 0.1), (0.1, 0.5), decisionNode)#plotNode(&apos;a leaf node&apos;, (0.8, 0.1), (0.3, 0.8), leafNode)#plt.show()</code></pre><h2 id="getNumLeafs-myTree"><a href="#getNumLeafs-myTree" class="headerlink" title="getNumLeafs(myTree)"></a>getNumLeafs(myTree)</h2><p>计算叶子节点的个数<br>构造注解树，需要知道叶节点的个数，以便可以正确确定x轴的长度；要知道树的层数，可以确定y轴的高度。</p><pre><code>def getNumLeafs(myTree):        numLeafs = 0    firstStr = list(myTree.keys())[0]  #获得myTree的第一个键值，即第一个特征，分割的标签    #print(firstStr)    secondDict = myTree[firstStr]   #根据键值得到对应的值，即根据第一个特征分类的结果    #print(secondDict)    for key in secondDict.keys():   #获取第二个小字典中的key        if type(secondDict[key]).__name__==&apos;dict&apos;:            #判断是否小字典中是否还包含新的字典（即新的分支）            numLeafs += getNumLeafs(secondDict[key])    #包含的话进行递归从而继续循环获得新的分支所包含的叶节点的数量        else:   numLeafs +=1    #不包含的话就停止迭代并把现在的小字典加一表示这边有一个分支    return numLeafsdef getTreeDepth(myTree):   #计算判断节点的个数    maxDepth = 0    firstStr = list(myTree.keys())[0]    secondDict = myTree[firstStr]    for key in secondDict.keys():        if type(secondDict[key]).__name__==&apos;dict&apos;:            thisDepth = 1 + getTreeDepth(secondDict[key])        else:   thisDepth = 1        if thisDepth &gt; maxDepth: maxDepth = thisDepth    return maxDepth</code></pre><h2 id="retrieveTree-i"><a href="#retrieveTree-i" class="headerlink" title="retrieveTree(i)"></a>retrieveTree(i)</h2><p>预先存储树信息</p><pre><code>def retrieveTree(i):    listOfTrees =[{&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}}},                  {&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: {&apos;head&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}, 1: &apos;no&apos;}}}}              ]    return listOfTrees[i]</code></pre><h2 id="plotMidText-cntrPt-parentPt-txtString"><a href="#plotMidText-cntrPt-parentPt-txtString" class="headerlink" title="plotMidText(cntrPt, parentPt, txtString)"></a>plotMidText(cntrPt, parentPt, txtString)</h2><p>作用是计算tree的中间位置，cntrPt起始位置,parentPt终止位置,txtString文本标签信息</p><pre><code>def plotMidText(cntrPt, parentPt, txtString):    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]  #cntrPt起点坐标，子节点坐标，parentPt结束坐标，父节点坐标    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]  #找到x和y的中间位置    createPlot.ax1.text(xMid, yMid, txtString, va=&quot;center&quot;, ha=&quot;center&quot;, rotation=30)def plotTree(myTree, parentPt, nodeTxt):    numLeafs = getNumLeafs(myTree)    depth = getTreeDepth(myTree)    firstStr = list(myTree.keys())[0]    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)   #计算子节点的坐标    plotMidText(cntrPt, parentPt, nodeTxt)      #绘制线上的文字    plotNode(firstStr, cntrPt, parentPt, decisionNode)      #绘制节点    secondDict = myTree[firstStr]    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD     #每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的    for key in secondDict.keys():        if type(secondDict[key]).__name__==&apos;dict&apos;:            plotTree(secondDict[key],cntrPt,str(key))        else:            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD</code></pre><h2 id="createPlot-inTree"><a href="#createPlot-inTree" class="headerlink" title="createPlot(inTree)"></a>createPlot(inTree)</h2><pre><code>def createPlot(inTree):    fig = plt.figure(1, facecolor=&apos;white&apos;)  #类似于Matlab的figure，定义一个画布，背景为白色    fig.clf()   # 把画布清空    axprops = dict(xticks=[], yticks=[])    #subplot定义了一个绘图    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks    #createPlot.ax1为全局变量，绘制图像的句柄，111表示figure中的图有1行1列，即1个，最后的1代表第一个图,frameon表示是否绘制坐标轴矩形    plotTree.totalW = float(getNumLeafs(inTree))    plotTree.totalD = float(getTreeDepth(inTree))    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;    plotTree(inTree, (0.5,1.0), &apos;&apos;)    plt.show()</code></pre><h1 id="treePlotter-main-py"><a href="#treePlotter-main-py" class="headerlink" title="treePlotter_main.py"></a>treePlotter_main.py</h1><pre><code>import  treePlotter#treePlotter.createPlot()#print(treePlotter.retrieveTree(1))myTree=treePlotter.retrieveTree(0)#print(treePlotter.getNumLeafs(myTree))#print(treePlotter.getTreeDepth(myTree))myTree[&apos;no surfacing&apos;][3]=&apos;maybe&apos;treePlotter.createPlot(myTree)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;p&gt;优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据&lt;br&gt;缺点：可能会产生过度匹配问题&lt;br&gt;使用数据类型：数值型和标称型&lt;br&gt;专家系统中，经常使用决策树&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://moyingyao.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习实战" scheme="http://moyingyao.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战--KNN</title>
    <link href="http://moyingyao.github.io/2018/05/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN/"/>
    <id>http://moyingyao.github.io/2018/05/23/机器学习实战-KNN/</id>
    <published>2018-05-23T12:24:18.000Z</published>
    <updated>2018-05-25T03:09:14.904Z</updated>
    
    <content type="html"><![CDATA[<p>这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0</p><h1 id="knn"><a href="#knn" class="headerlink" title="knn"></a>knn</h1><p>优       点：精度高，对异常值不敏感，无数据输入假定<br>缺       点：计算复杂度高，空间复杂度高，无法给出数据的内在含义<br>使用数据范围：数值型和标称型<br><a id="more"></a><br><img src="https://i.imgur.com/1diiQIB.png" alt=""></p><p>—————————————————————–下面进入正题—————————————————————–</p><h1 id="kNN-py"><a href="#kNN-py" class="headerlink" title="kNN.py"></a>kNN.py</h1><pre><code>from numpy import *  import operator  from os import listdir  </code></pre><h2 id="def-createDataSet"><a href="#def-createDataSet" class="headerlink" title="def createDataSet()"></a>def createDataSet()</h2><pre><code>def createDataSet():      group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])      labels = [&apos;A&apos;,&apos;A&apos;,&apos;B&apos;,&apos;B&apos;]      return group, labels </code></pre><h2 id="classify0"><a href="#classify0" class="headerlink" title="classify0()"></a>classify0()</h2><p>inX用于分类的输入向量,是一个向量<br>dataSet输入的训练样本集，是一个矩阵<br>labels标签向量<br>k用于选择最近邻居的数目<br>labels数目与dataSet的行数相同 </p><pre><code>def classify0(inX, dataSet, labels, k):    dataSetSize = dataSet.shape[0]  #返回的是dataSet的行数，行数就是样本的数量    diffMat = tile(inX, (dataSetSize,1)) - dataSet  #矩阵相减    #inX是个向量，而dataset是个矩阵，两者之间要进行相减的运算，需要把这个向量也补成一个和dataset有相同行数列数的矩阵，    #tile()的第二个参数，就是(datasetsize,1)，这个参数的意思就是把inX补成有datasetsize行数的矩阵。    #假如inX是（1，2），datasetsize =3，那么经过tile()转换后产生了一个这样的矩阵（[1,2],[1,2],[1,2]）    sqDiffMat = diffMat**2  #平方    sqDistances = sqDiffMat.sum(axis=1) #按行求和    # sqdiffMat是([1,2],[0,1],[3,4])，axis这个参数影响了对矩阵求和时候的顺序，axis=0是按照列求和，结果为([3.1.7])    # axis=1是按照行进行求和，结果是([4,7])。    distances = sqDistances**0.5    #开方，得到欧氏距离    sortedDistIndicies = distances.argsort()     #把向量中每个元素进行排序，结果是元素的索引形成的向量    #例子distance([1,4,3])，经过distance.argsort()之后的结果是([0,2,1]    classCount={}       #存放最终的分类结果及相应的结果投票数    #投票过程，就是统计前k个最近的样本所属类别包含的样本个数    for i in range(k):        # index = sortedDistIndicies[i]是第i个最相近的元素索引，即样本下标        # voteIlabel = labels[index]是样本index对应的分类结果(&apos;A&apos; or &apos;B&apos;)        voteIlabel = labels[sortedDistIndicies[i]]        # classCount.get(voteIlabel, 0)返回voteIlabel的值，如果不存在，则返回0        # 然后将票数增1        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1    # 把分类结果进行排序，然后返回得票数最多的分类结果    # key=operator.itemgetter(1)的意思是按照字典里的第一个排序    #例子a = [1, 2, 3]，b = operator.itemgetter(1)，b(a)返回为2    #b = operator.itemgetter(1, 0)，b(a)，定义函数b，获取对象的第1个域和第0个的值，返回 (2, 1)    # reverse=True是降序排序    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)    return sortedClassCount[0][0] #返回类别最多的类别</code></pre><h2 id="file2matrix"><a href="#file2matrix" class="headerlink" title="file2matrix()"></a>file2matrix()</h2><p>将文本记录转换为NumPy<br>将文本记录转换为NumPy的解析程序<br>输入为矩阵，输出为训练样本矩阵和类标签向量</p><pre><code>def file2matrix(filename):    fr = open(filename)     #打开文档    numberOfLines = len(fr.readlines())        #得到文件行数    #fr.readlines()读取行数,存在数组中,导入后每行中用\t隔开,两行之间用\n换行得到文件行数    returnMat = zeros((numberOfLines,3))        #创建返回NumPy矩阵，numberoflines行，3列的初始化零的矩阵    classLabelVector = []#定义一个空的数组    fr = open(filename)    index = 0    for line in fr.readlines():        line = line.strip() #删除（）中的内容，这里表示删除空格        listFromLine = line.split(&apos;\t&apos;)#以\t分割        #print(listFromLine)        returnMat[index,:] = listFromLine[0:3]#把每行前三个元素存入returnMat矩阵中，每行中存储三个        classLabelVector.append(int(listFromLine[-1]))#存储第四列元素即标签，在数组中append添加，-1表示最后一列        index += 1    return returnMat,classLabelVector</code></pre><h2 id="autoNorm"><a href="#autoNorm" class="headerlink" title="autoNorm()"></a>autoNorm()</h2><p>归一化数值，避免某特征值过大，使得权重比例不均匀，对计算结果产生影响。<br>autoNorm可以自动将数字特征值转化为0到1区间</p><pre><code>def autoNorm(dataSet):    minVals = dataSet.min(0)#一维数组，值为各项特征（列）中的最小值。参数0使得函数从列中选取最小值    #print(minVals)    maxVals = dataSet.max(0)    #print(maxVals)    ranges = maxVals - minVals    normDataSet = zeros(shape(dataSet)) #创建与样本集一样大小的零矩阵    #print(normDataSet)    m = dataSet.shape[0]#dataSet的行数    normDataSet = dataSet - tile(minVals, (m,1))#矩阵中所有的值减去最小值    #tile将原来的一个数组minVals，扩充成了m行1列的数组    normDataSet = normDataSet/tile(ranges, (m,1))   #矩阵中所有的值除以最大取值范围进行归一化    return normDataSet, ranges, minVals</code></pre><h2 id="datingClassTest"><a href="#datingClassTest" class="headerlink" title="datingClassTest()"></a>datingClassTest()</h2><p>测试算法，样本集中百分之九十的数据用来训练样本，百分之十的样本用来测试分类器kNN.classify0()。</p><pre><code>def datingClassTest():    hoRatio = 0.10      #百分之十的数据用于测试分类器，更改该变量的值可更改参加测试分类器的数据量    datingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)       #导入数据    normMat, ranges, minVals = autoNorm(datingDataMat)      #归一化数值    m = normMat.shape[0]    #得到总行数    numTestVecs = int(m*hoRatio)    #测试总数据数量，m*hoRatio是一个浮点型，需转化成整形    errorCount = 0.0    #初试错误率为0    for i in range(numTestVecs):        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)        #分类器（需要测试的向量，训练样本集(90%)，标签集合，K）        print(&quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, datingLabels[i]))        if (classifierResult != datingLabels[i]): errorCount += 1.0     #计数，错误的个数    print(&quot;the total error rate is: %f&quot; % (errorCount/float(numTestVecs)))      #错误率    print(errorCount)</code></pre><h2 id="classifyPerson"><a href="#classifyPerson" class="headerlink" title="classifyPerson()"></a>classifyPerson()</h2><p>约会数据,对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测，</p><pre><code>def classifyPerson():    resultList=[&apos;not at all&apos;,&apos;in samll doses&apos;,&apos;in large doses&apos;] #三种感兴趣程度    percentTats=float(input(&quot;percentage of time spent playing video games?&quot;))    ffMiles=floats=float(input(&quot;frequent flier miles earned per year?&quot;))    iceCream=float(input(&quot;liters of ice cream consuned per year?&quot;))#input键盘输入    datingDataMat,datingLabels=file2matrix(&apos;datingTestSet2.txt&apos;) # 导入数据    normMat,ranges,minvals=autoNorm(datingDataMat) # 归一化，ranges是归一化的分母    inArr=array([ffMiles,percentTats,iceCream]) # inArr是归一化之前的datingDataMat数组中的行    classifierResult=classify0((inArr-minvals)/ranges,normMat,datingLabels,3)#先归一化，然后调用分类函数    #print(classifierResult)    print(&quot;you will probably like this person:%s&quot;%resultList[classifierResult-1])</code></pre><h2 id="img2vector"><a href="#img2vector" class="headerlink" title="img2vector()"></a>img2vector()</h2><p>图片转向量<br>手写体：32<em>32的黑白图像<br>图片转向量，将32</em>32的二进制图像矩阵转换为1*1024的向量</p><pre><code>def img2vector(filename):    returnVect = zeros((1,1024))    fr = open(filename)    for i in range(32):#循环读出文件的前32行        lineStr = fr.readline()        for j in range(32):#将每行的前32个字符存储在NumPy数组中            returnVect[0,32*i+j] = int(lineStr[j])    return returnVect#返回数组</code></pre><h2 id="handwritingClassTest"><a href="#handwritingClassTest" class="headerlink" title="handwritingClassTest()"></a>handwritingClassTest()</h2><p>手写体测试</p><pre><code>def handwritingClassTest():    hwLabels = []    trainingFileList = listdir(&apos;trainingDigits&apos;)           #导入训练数据    #print(trainingFileList)    m = len(trainingFileList)   #训练数据的总数    #print(m)    trainingMat = zeros((m,1024))   #m行1024列的零向量    for i in range(m):        fileNameStr = trainingFileList[i]   #文件名        fileStr = fileNameStr.split(&apos;.&apos;)[0]     #取文件名.之前的名字        classNumStr = int(fileStr.split(&apos;_&apos;)[0])    #取文件名_之前的名字        hwLabels.append(classNumStr)        trainingMat[i,:] = img2vector(&apos;trainingDigits/%s&apos; % fileNameStr)    #将对应数据集下的文件一个个的转为向量        #print(trainingMat[i,:])    testFileList = listdir(&apos;testDigits&apos;)        #测试数据    errorCount = 0.0    mTest = len(testFileList)    for i in range(mTest):        fileNameStr = testFileList[i]        fileStr = fileNameStr.split(&apos;.&apos;)[0]        classNumStr = int(fileStr.split(&apos;_&apos;)[0])        vectorUnderTest = img2vector(&apos;testDigits/%s&apos; % fileNameStr)        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) #利用训练的trainingMat测试        print(&quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, classNumStr))        if (classifierResult != classNumStr): errorCount += 1.0    print(&quot;\nthe total number of errors is: %d&quot; % errorCount)    print(&quot;\nthe total error rate is: %f&quot; % (errorCount/float(mTest)))</code></pre><h1 id="knn-main-py"><a href="#knn-main-py" class="headerlink" title="knn_main.py"></a>knn_main.py</h1><pre><code>import kNN  import matplotlib  import matplotlib.pyplot as plt  import numpy as np  from imp import reload  group,labels=kNN.createDataSet()  #print(group)  #print(labels)  #print(kNN.classify0([0,0],group,labels,3))  </code></pre><h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><pre><code>fig=plt.figure()                #建立画板  ax=fig.add_subplot(111)         #添加一个子图，一行一列第一个子块，若括号内为349，则三行四列第9个子块  reload(kNN)  datingDataMat,datingLabels=kNN.file2matrix(&apos;datingTestSet2.txt&apos;)  #print(datingDataMat)  #ax.scatter(datingDataMat[:,1],datingDataMat[:,2])        # scatter绘制散点图,使用第二列第三列数据  #ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*np.array(datingLabels),15.0*np.array(datingLabels))  ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*np.array(datingLabels),15.0*np.array(datingLabels))  #plt.show()  </code></pre><h2 id="归一化数值"><a href="#归一化数值" class="headerlink" title="归一化数值"></a>归一化数值</h2><pre><code>normMat,ranges,minVals=kNN.autoNorm(datingDataMat)  #print(normMat)  #print(ranges)  #print(minVals)  </code></pre><h2 id="测试算法"><a href="#测试算法" class="headerlink" title="测试算法"></a>测试算法</h2><pre><code>#kNN.datingClassTest()  </code></pre><h2 id="约会预测"><a href="#约会预测" class="headerlink" title="约会预测"></a>约会预测</h2><pre><code>#对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测，  #输入10   10000   0.5  #kNN.classifyPerson()  </code></pre><h2 id="手写体"><a href="#手写体" class="headerlink" title="手写体"></a>手写体</h2><pre><code>#trainingDigits包含大约2000个例子，每个数字约有200个样本  #testDigits包含大约900个测试数据  testVector=kNN.img2vector(&apos;trainingDigits/0_13.txt&apos;)  #print(testVector[0,0:31])  #print(testVector[0,32:63])  #print(testVector[0,64:95])  kNN.handwritingClassTest()  </code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0&lt;/p&gt;
&lt;h1 id=&quot;knn&quot;&gt;&lt;a href=&quot;#knn&quot; class=&quot;headerlink&quot; title=&quot;knn&quot;&gt;&lt;/a&gt;knn&lt;/h1&gt;&lt;p&gt;优       点：精度高，对异常值不敏感，无数据输入假定&lt;br&gt;缺       点：计算复杂度高，空间复杂度高，无法给出数据的内在含义&lt;br&gt;使用数据范围：数值型和标称型&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://moyingyao.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习实战" scheme="http://moyingyao.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>伊始</title>
    <link href="http://moyingyao.github.io/2018/05/17/%E4%BC%8A%E5%A7%8B/"/>
    <id>http://moyingyao.github.io/2018/05/17/伊始/</id>
    <published>2018-05-17T14:27:24.000Z</published>
    <updated>2018-05-18T14:27:18.211Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈<br><a id="more"></a></p><h2 id="简单随便说说建站方法"><a href="#简单随便说说建站方法" class="headerlink" title="简单随便说说建站方法"></a>简单随便说说建站方法</h2><h3 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h3><p>就是以Hexo为主，剩下的自行百度吧，毕竟我要回寝室，没时间写了</p><h3 id="Github-Pages"><a href="#Github-Pages" class="headerlink" title="Github Pages"></a>Github Pages</h3><p>以github为载体实现的，也百度吧，啊啊啊，实验室就剩我自己了。</p><h3 id="更多资料"><a href="#更多资料" class="headerlink" title="更多资料"></a>更多资料</h3><p>参考资料请点击：:</p><p><a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">资料一</a><br><a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">资料二</a><br><a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">资料三</a><br><a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">资料四</a><br><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#%E9%99%84%E4%B8%8A%E6%88%91%E7%9A%84-custom-styl" target="_blank" rel="noopener">这个是会让博客变好看的哦</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="随便写写" scheme="http://moyingyao.github.io/categories/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/"/>
    
    
      <category term="个人随笔" scheme="http://moyingyao.github.io/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
