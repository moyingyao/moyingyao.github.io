<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AmberWu</title>
  
  <subtitle>越努力，越幸运</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://moyingyao.github.io/"/>
  <updated>2019-06-20T07:20:44.618Z</updated>
  <id>http://moyingyao.github.io/</id>
  
  <author>
    <name>AmberWu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于深度学习框架的水声信号的扩充和分类识别</title>
    <link href="http://moyingyao.github.io/2019/05/28/20190528%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E6%B0%B4%E5%A3%B0%E4%BF%A1%E5%8F%B7%E7%9A%84%E6%89%A9%E5%85%85%E5%92%8C%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB/"/>
    <id>http://moyingyao.github.io/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/</id>
    <published>2019-05-28T02:15:01.000Z</published>
    <updated>2019-06-20T07:20:44.618Z</updated>
    
    <content type="html"><![CDATA[<p>现如今，基于声音信号的海上目标识别是进行海量探测和目标识别的可靠方法，也是水声信号处理领域的重要研究内容。<a id="more"></a></p><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>原始数据包含15类的水下音频数据，每类里面包含长度不等的一段音频数据。</p><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>训练一个模型，对这15类目标进行识别分类。</p><h3 id="思考方法"><a href="#思考方法" class="headerlink" title="思考方法"></a>思考方法</h3><ol><li>最开始尝试直接用语音识别的模型（<code>LSTM</code>）对其分类，后来发现效果很差，考虑到可能是水下环境复杂，原始音频数据包含大量噪音。</li><li>于是开始考虑其他方法。最后通过查阅资料发现，在水声领域，通常可以将音频信号数据转换为谱图进行处理，于是尝试将原始音频数据先转换为四种主流的谱图，再用<code>CNN</code>对其进行分类识别。</li><li><code>CNN</code>的方法相比于<code>LSTM</code>，在准确率上有所提升，并且通过对比发现了比较适用于深度学习训练的图谱。</li><li>不过准确率不够理想，又考虑到原始数据由于安全保密或者是收集困难等原因，数据集数量不多，而大量的训练数据集是保证深度学习方法性能的关键。遂最终又引入了<code>GAN</code>网络。先对转换后的频谱做了扩充，然后再用<code>CNN</code>网络对其识别分类，准确率相较于仅用<code>CNN</code>，又有了不错的提升。</li></ol><h2 id="项目实现过程"><a href="#项目实现过程" class="headerlink" title="项目实现过程"></a>项目实现过程</h2><p><strong>整个实现流程如下图所示</strong></p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/1.png" alt=""></p><ol><li>（a）是 .wav格式的原始样本</li><li>（b）是通过频谱进行的数据预处理和基于CNN的频谱选择，从而确保样本可以被神经网络识别。 </li><li>（c）由GAN进行数据集扩展，额外的CNN是为了确保生成频谱的质量</li><li>（d）对网络进行评估并选择最佳网络。</li></ol><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>在水声领域，常见的频谱图有很多，例如<code>Lofar</code>、<code>Audio</code>、<code>Demon</code>、<code>Histogram</code>等，我们尝试将原始音频转换为了各种谱图存储，每种谱图都包含15类。部分转换后的频谱如下图所示：</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/2.png" alt=""></p><p>为了找到最好的频谱图，我们直接选择了<code>LeNet</code>网络，针对每种转换好的频谱，为其划分好训练集和测试集，各训练了一个分类模型。通过比较在测试集上的准确率，我们发现<code>Lofar</code>频谱的识别率最高，于是选定它为原始音频转换后的频谱。</p><p><strong>Lofar频谱的转换</strong></p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/3.png" alt=""></p><p>将原始音频转换为Lofar频谱，主要是通过<code>STFT</code>(快速傅里叶变换)实现的。</p><ol><li>对于每一类的音频，对其做<code>STFT</code>后，先找到具有特征的频率段，</li><li>然后将数据根据能量差值转换为对应的灰度值，</li><li>每隔1s选取一段音频，重复步骤1和2，最终每类音频得到<strong>1000</strong>张灰度的<code>Lofar</code>谱图</li></ol><h3 id="CNN网络"><a href="#CNN网络" class="headerlink" title="CNN网络"></a>CNN网络</h3><p>得到每类的<code>Lofar</code>频谱后，我们设计了一个CNN网络来训练分类模型。网络结构图和参数设置如下所示：<br><strong>网络模型</strong><br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/4.png" alt=""></p><p><strong>参数设置</strong><br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/5.png" alt=""></p><p>最终测试集在上述分类模型中取得了<strong>75.7%</strong>的识别率。</p><h3 id="GAN网络"><a href="#GAN网络" class="headerlink" title="GAN网络"></a>GAN网络</h3><p>在引入<code>GAN</code>网络前，我们也尝试了原始的数据增广的方法，例如<strong>旋转、反转、亮度增强、添加噪音等</strong>，如下图所示，但是这些方法生成的数据都有很大的局限性，即图片缺乏多样性，训练出来的分类模型准确率基本没有提升。</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/7.png" alt=""></p><h4 id="原始GAN"><a href="#原始GAN" class="headerlink" title="原始GAN"></a>原始GAN</h4><p>为了进一步提升识别准确率，我们引入了<code>GAN</code>生成式对抗网络。生成式对抗网络主要由2部分组成：</p><ol><li><strong>判别模型D</strong>：一个二分类器，估计一个样本来自训练数据（真实）的概率。若样本来自真实数据，输出大概率值，否则输出小概率值。</li><li><strong>生成模型G</strong>：捕捉生成数据的分布。用服从某一分布（<strong>均匀分布、高斯分布</strong>）的噪音向量z去生成一个类似真实训练数据的分布。</li></ol><p>G要尽量最小化D的输出值，D要最大化输出值，两者相互对抗。所以<strong>原始GAN模型函数</strong>可以表示如下：</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/16.png" alt=""></p><p><strong>GAN模型示意图</strong></p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/6.png" alt=""></p><p>如果1用于表示实际样本，则0是假样本。 对于来自实际样本的数据，D应尽可能地输出概率值1。 对于来自生成器的假样本，D应该尽量输出0的概率值。它们相互竞争并最终达到一定的<strong>稳定状态</strong>：即G的分布尽可能接近实际样本的分布。</p><h4 id="CGAN"><a href="#CGAN" class="headerlink" title="CGAN"></a>CGAN</h4><p>在本项目中，我们的数据共有15类，如果使用原始的GAN网络，我们需要针对每一个类别的Lofar谱图训练一个生成模型，这样太过麻烦。最终我们选择了<code>CGAN</code>网络。</p><p><strong>CGAN</strong>网络将原始GAN网络从无监督变为有监督。<font color="#FF0000">生成器和判别器都增加了额外信息y为条件</font>，y可以是任意信息，例如<strong>类别信息（标签）</strong>或者是其他模态的数据。相比于原始GAN，<strong>CGAN的模型函数</strong>可以表示如下（<strong>只是多了标签数据y</strong>）：<br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/18.png" alt=""></p><p><strong>CGAN模型示意图</strong></p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/8.png" alt=""></p><p>生成式模型G中，先验输入噪音P(z)和条件信息y联合组成了联合隐层表征。</p><h4 id="CGAN网络设计"><a href="#CGAN网络设计" class="headerlink" title="CGAN网络设计"></a>CGAN网络设计</h4><p><strong>网络模型</strong><br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/9.png" alt=""></p><p><strong>参数设置</strong><br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/10.png" alt=""><br><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/11.png" alt=""></p><p>我们输入15种类型的LOFAR频谱，每种频谱包含1000个样本到设计的CGAN网络中，并且一些超参设置如下：<code>batch_size=64，learning_rate=0.0001，epoches=40</code>。</p><p>训练后，我们的方法可以利用G模型不仅可以生成某种特定的光谱样本，而且还能生成各种光谱样本。 在下图中展示了一些原始和生成的样本，其中<code>（a）</code>和<code>（b）</code>对应于单个类，而<code>（c）</code>和<code>（d）</code>对应于各种类。</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/12.png" alt=""></p><p> 相比之下，我们发现由我们的网络生成的数据包含共同的特征线，并且在某些情况下，所有这些特征线都清晰地显示并且具有比原始训练样本更突出的特征线。</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/13.png" alt=""></p><p> 除此之外，我们也对原始样本和该图中生成的样本之间进行了更深入和更详细的比较。最后，我们可以发现生成的样本具有丰富的多样性，其中包含与<code>（a）</code>中的原始样本相似的样本，以及包括具有一定噪声的样本，并且所有上述样本都在<code>（b）</code>中示出。<br> <code>（b）</code>中的顶部样本与原始样本类似，底部包含一些噪声，但都具有明显的光谱特征。 使用生成的样本作为训练数据可以极大地改善训练样本的多样性，从而在一定程度上避免过度拟合问题。</p><h3 id="图像质量验证"><a href="#图像质量验证" class="headerlink" title="图像质量验证"></a>图像质量验证</h3><p>上面只是通过观察得到的结果，为了进一步验证我们的网络生成的样本是高质量的，我们使用G模型为每个类生成3000个样本，然后将它们与原始样本混合并进入<code>LeNet</code>分类网络进行训练。主要进行了以下实验：</p><ol><li>我们的方法使用80％的原始样本进行训练，并验证剩余的20％。</li><li>同时，我们还应用上述模型对15种类型的生成样本进行分类和识别。</li><li>此外，我们将原始样本和生成样本混合，其中80％用于训练，其余20％用于验证。</li></ol><p><strong>实验结果如下所示</strong>：</p><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/14.png" alt=""></p><p>上述实验表明混合数据所获得的性能比仅用原始数据集训练的性能高出15％以上。这证明了我们的CGAN网络可以生成高质量的lofar谱，从而解决了水声信号领域数据样本不足的问题。生成的谱图可以用于提高水声信号分类和识别任务的准确性和稳定性。</p><h2 id="CNN网络选取"><a href="#CNN网络选取" class="headerlink" title="CNN网络选取"></a>CNN网络选取</h2><p>确定了生成的样本具有较高的质量后，我们用<code>G</code>模型为每类Lofar谱图生成了3000个样本，除了将其输入到我们设计的CNN网络之外，我们也和其他的一些CNN网络（LeNet、AlexNet、VGG16）做了对比，最终我们的设计的网络模型识别率较高。</p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>如果希望再进一步提升识别率，可以从以下2方面做出改进：</p><ol><li><strong>其他的GAN模型</strong>。CGAN只是使用了真实图片的类别信息，可以考虑其他的衍生GAN模型，例如<strong>InfoGAN    WGAN    WGAN-GP     BEGAN……</strong>，也许能生成质量更高的样本</li><li><strong>去噪</strong>。水下环境复杂，原始的音频数据包含较多的噪音。可以考虑先对原始数据进行去噪，得到高质量数据后，再转换为谱图用CNN分类</li></ol><p><img src="/2019/05/28/20190528基于深度学习框架的水声信号的扩充和分类识别/15.png" alt=""></p><p>当然，也可以把上述2点结合，先去噪，再尝试其他的GAN模型，也许能得到更好的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现如今，基于声音信号的海上目标识别是进行海量探测和目标识别的可靠方法，也是水声信号处理领域的重要研究内容。&lt;/p&gt;
    
    </summary>
    
      <category term="项目实战" scheme="http://moyingyao.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="GAN" scheme="http://moyingyao.github.io/tags/GAN/"/>
    
      <category term="项目" scheme="http://moyingyao.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>七月在线-深度学习</title>
    <link href="http://moyingyao.github.io/2019/03/25/20190325%E4%B8%83%E6%9C%88%E5%9C%A8%E7%BA%BF-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://moyingyao.github.io/2019/03/25/20190325七月在线-深度学习/</id>
    <published>2019-03-25T02:39:54.000Z</published>
    <updated>2019-03-25T15:32:05.679Z</updated>
    
    <content type="html"><![CDATA[<p>​      </p><a id="more"></a><h3 id="线性与非线性"><a href="#线性与非线性" class="headerlink" title="线性与非线性"></a>线性与非线性</h3><p>线性：随机梯度下降，卷积函数<br>非线性：修正线性单元（Relu）</p><h3 id="Dropout-Boosting-Bagging-Stacking-Mapping"><a href="#Dropout-Boosting-Bagging-Stacking-Mapping" class="headerlink" title="Dropout,Boosting,Bagging,Stacking,Mapping"></a>Dropout,Boosting,Bagging,Stacking,Mapping</h3><h4 id="Bagging和Dropout"><a href="#Bagging和Dropout" class="headerlink" title="Bagging和Dropout"></a>Bagging和Dropout</h4><p>Bagging能实现跟神经网络中Dropout类似的效果。<br>Dropout是将许多单独训练的子网络集成起来，某些权值是共享的。<br>Bagging是将许多单独训练的学习机集成起来；<br>Dropout和Bagging这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。</p><h4 id="Bagging和Boosting的区别："><a href="#Bagging和Boosting的区别：" class="headerlink" title="Bagging和Boosting的区别："></a>Bagging和Boosting的区别：</h4><p>Boosting并不是单独训练的，而是按照有一定的顺序训练的，具有相互依赖关系。</p><p>1）样本选择上：<br>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。<br>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。<br>2）样例权重：<br>Bagging：使用均匀取样，每个样例的权重相等。<br>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。<br>3）预测函数：<br>Bagging：所有预测函数的权重相等。<br>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。<br>4）并行计算：<br>Bagging：各个预测函数可以并行生成<br>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果</p><p>Stacking是通过两层学习机完成的学习。</p><h3 id="处理过拟合的方法"><a href="#处理过拟合的方法" class="headerlink" title="处理过拟合的方法"></a>处理过拟合的方法</h3><p>Dropout；</p><h3 id="调整超参数来最小化代价函数（cost-functon）的技术："><a href="#调整超参数来最小化代价函数（cost-functon）的技术：" class="headerlink" title="调整超参数来最小化代价函数（cost functon）的技术："></a>调整超参数来最小化代价函数（cost functon）的技术：</h3><p>网格搜索，随机搜索，贝叶斯（bayesian）优化，居于梯度的优化。</p><h3 id="批规范化（Batch-Normalization）的好处："><a href="#批规范化（Batch-Normalization）的好处：" class="headerlink" title="批规范化（Batch Normalization）的好处："></a>批规范化（Batch Normalization）的好处：</h3><p>增加反向传播速度，避免梯度消失；<br>加速网络收敛；<br>减轻参数初始化的影响。<br>批规范化（Batch Normalization）<strong>不能处理过拟合</strong>，因为同一个数据在不同批中被归一化后的值会有差别，相当于做了数据增强（data augmentation）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​      &lt;/p&gt;
    
    </summary>
    
      <category term="七月在线" scheme="http://moyingyao.github.io/categories/%E4%B8%83%E6%9C%88%E5%9C%A8%E7%BA%BF/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="七月在线" scheme="http://moyingyao.github.io/tags/%E4%B8%83%E6%9C%88%E5%9C%A8%E7%BA%BF/"/>
    
      <category term="笔试题" scheme="http://moyingyao.github.io/tags/%E7%AC%94%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>FaceNet在FPGA等硬件平台上的实现</title>
    <link href="http://moyingyao.github.io/2019/02/17/20190217FaceNet%E5%9C%A8FPGA%E7%AD%89%E7%A1%AC%E4%BB%B6%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://moyingyao.github.io/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/</id>
    <published>2019-02-17T00:56:28.000Z</published>
    <updated>2019-06-20T07:20:57.038Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍将训练好的网络模型，移植到FPGA等硬件平台上所必须的准备工作。<a id="more"></a><br>不涉及具体的用C语言重新编写卷积操作、RAM存储等设计，本人做的只是辅助工作=-=。</p><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p><strong>论文地址</strong>：<a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></p><p>将在服务器上训练好的FaceNet模型移植到FPGA等硬件平台上，实现人脸的检测推断过程。要想实现上述操作，必须先进行下面2个操作：</p><ol><li><strong>模型参数的提取</strong>：解析 FaceNet 的网络结构， Restore 训练好的模型，提取各网络层参数。</li><li><strong>参数的量化压缩</strong>：模型参数数量巨大(浮点)，为了节省空间及方便计算，将参数量化为 8 位的<br>定点数。</li></ol><h3 id="参数的提取"><a href="#参数的提取" class="headerlink" title="参数的提取"></a>参数的提取</h3><p>在提取参数前，我们先通过可视化工具<code>Tensorboard</code>解析了一下FaceNet的网络结构，它主要包含5个大模块：</p><h4 id="block35"><a href="#block35" class="headerlink" title="block35"></a>block35</h4><ol><li><strong>Branch_0</strong>：32个1x1卷积</li><li><strong>Branch_1</strong>：32个1x1卷积、32个3x3卷积</li><li><strong>Branch_2</strong>：32个1x1卷积、32个3x3卷积、32个3x 3卷积</li><li><strong>Mixed</strong>：将Branch_0、Branch_1和Branch_2连接起来</li><li><strong>Conv</strong>：32个1x1卷积</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/1.png" alt=""></p><h4 id="block17"><a href="#block17" class="headerlink" title="block17"></a>block17</h4><ol><li><strong>Branch_0</strong>：128个1x1卷积</li><li><strong>Branch_1</strong>：128个1x1卷积、128个1x7卷积、128个7 x1卷积</li><li><strong>Mixed</strong>：将Branch_0和Branch_1连接起来</li><li><strong>Conv</strong>：128个1x1卷积</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/2019-4-25-FaceNet在FPGA等硬件平台上的实现/2.png" alt=""></p><h4 id="block8"><a href="#block8" class="headerlink" title="block8"></a>block8</h4><ol><li><strong>Branch_0</strong>：192个1x1卷积</li><li><strong>Branch_1</strong>：192个1x1卷积、192个1x3卷积、192个3x1卷积</li><li><strong>Mixed</strong>：将Branch_0和Branch_1连接起来</li><li><strong>Conv</strong>：192个1x1卷积</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/3.png" alt=""></p><h4 id="reduction-a"><a href="#reduction-a" class="headerlink" title="reduction_a"></a>reduction_a</h4><ol><li><strong>Branch_0</strong>：192个3x3（stride=2）卷积</li><li><strong>Branch_1</strong>：192个1x1卷积、256个3x3卷积、384个3x3（stride=2）卷积</li><li><strong>Branch_2</strong>：3x3，步长为2的最大池化</li><li><strong>Mixed</strong>：将Branch_0、Branch_1和Branch_2连接起来</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/4.png" alt=""></p><h4 id="reduction-b"><a href="#reduction-b" class="headerlink" title="reduction_b"></a>reduction_b</h4><ol><li><strong>Branch_0</strong>：256个1x1卷积、384个3x3（stride=2）卷积</li><li><strong>Branch_1</strong>：256个1x1卷积、256个3x3（stride=2）卷积</li><li><strong>Branch_2</strong>：256个1x1卷积、256个3x3卷积、256个x3（stride=2）卷积</li><li><strong>Branch_3</strong>：3x 3，步长为2的最大池化</li><li><strong>Mixed</strong>：将Branch_0、Branch_1、Branch_2和Branch_3连接起来</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/5.png" alt=""></p><p><strong>总的网络结构如下所示</strong>：</p><ol><li><strong>Conv2d_1a</strong>：32个3x3，stride=2的卷积</li><li><strong>Conv2d_2a</strong>：32个3x3的卷积</li><li><strong>Conv2d_2b</strong>：64个3x3的卷积</li><li><strong>MaxPool_3a</strong>：3x3，stride=2的最大池化</li><li><strong>Conv2d_3b</strong>：80个1x1的卷积</li><li><strong>Conv2d_4a</strong>：192个3x3的卷积</li><li><strong>Conv2d_4b</strong>：256个3x3，stride=2的卷积</li><li><strong>repeat</strong>：5个<strong>block35</strong>模块</li><li><strong>Mixed_6a</strong>：1个<strong>reduction_a</strong>模块</li><li><strong>repeat1</strong>：10个<strong>block17</strong>模块</li><li><strong>Mixed_7a</strong>：1个<strong>reduction_b</strong>模块</li><li><strong>repeat2</strong>：5个<strong>block8</strong>模块</li><li><strong>block8</strong>：1个<strong>block8</strong>模块</li><li><strong>Logits</strong>：平均池化、flatten、Dropout</li></ol><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/6.png" alt=""></p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>代码中会用到<code>float_to_bin()</code>这一个量化函数，下面会有所介绍</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> pywrap_tensorflow</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> float_bin</span><br><span class="line"><span class="keyword">import</span> xiaoshu_bin</span><br><span class="line">Max = <span class="number">35.004695892333984</span>          <span class="comment">#参数的最大值</span></span><br><span class="line">Min = <span class="number">-11.588409423828125</span>         <span class="comment">#参数最小值</span></span><br><span class="line">Mean = <span class="number">-0.0007627065894155365</span>     <span class="comment">#参数均值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入模型</span></span><br><span class="line">checkpoint_path = os.path.join(<span class="string">'facenet_lmq/20170512-110547'</span>, <span class="string">"model-20170512-110547.ckpt-250000"</span>)</span><br><span class="line"><span class="comment">#读取模型参数</span></span><br><span class="line">reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)</span><br><span class="line"><span class="comment">#获取参数中所有的key、value值</span></span><br><span class="line">var_to_shape_map = reader.get_variable_to_shape_map()</span><br><span class="line"></span><br><span class="line"><span class="comment">#循环存储每一个key（tensor名字）对应的value</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> var_to_shape_map:</span><br><span class="line">    par_name=str(key)</span><br><span class="line">    par_final_name =par_name.replace(<span class="string">'/'</span>,<span class="string">'_'</span>) <span class="comment">#特殊字符替换</span></span><br><span class="line">    file_path = <span class="string">'D:/PycharmProjects/faceface/bb/'</span>+par_final_name+<span class="string">'.txt'</span> <span class="comment">#创建存储路径</span></span><br><span class="line">    par_shape=reader.get_tensor(key).shape         <span class="comment"># Tensor维度</span></span><br><span class="line">    par_value=reader.get_tensor(key).flatten()     <span class="comment">#value拉平，方便下面的量化操作</span></span><br><span class="line">    <span class="comment"># print(type(par_value))</span></span><br><span class="line">    <span class="comment"># print(par_value.shape)</span></span><br><span class="line">    list =[]</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(par_value)):  <span class="comment">#对每一个tensor的value量化</span></span><br><span class="line">        par_value[index] =(par_value[index]-Mean)/(Max-Min) <span class="comment">#归一化</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">'moving_variance'</span> <span class="keyword">in</span> par_name):       <span class="comment"># 特殊的tensor需要进行一些处理（BN）</span></span><br><span class="line">            <span class="comment"># for index in range(len(par_value)):</span></span><br><span class="line">            par_value[index] = <span class="number">1</span>/(math.sqrt(par_value[index]))</span><br><span class="line">        list.append(float_bin.float_to_bin(par_value[index]))  <span class="comment">#调用量化函数float_to_bin（）</span></span><br><span class="line">    np.savetxt(file_path,np.array(list),fmt=<span class="string">'%s'</span>,header=str(par_shape))  <span class="comment">#存储量化后的参数</span></span><br><span class="line">    print(<span class="string">'done'</span>)</span><br><span class="line">    <span class="comment">#print(type(par_value))</span></span><br></pre></td></tr></table></figure><h3 id="参数的量化压缩"><a href="#参数的量化压缩" class="headerlink" title="参数的量化压缩"></a>参数的量化压缩</h3><p>训练得到的模型参数都是浮点型的，为了节省在硬件上的存储空间并加速计算，我们将参数量化到了8位的定点数。主要包含2个函数：<code>float_to_bin()</code>和<code>xiaoshu_bin()</code>。</p><ol><li><p><strong>float_to_bin()</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> xiaoshu_bin</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">float_to_bin</span><span class="params">(innum,n)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> innum_abs,res_nint_array</span><br><span class="line">    list = []</span><br><span class="line">    min = <span class="number">2</span>**(-n)              <span class="comment">#小数位取n位后，8位定点数能表示的最小值</span></span><br><span class="line">    max = <span class="number">2</span>**(<span class="number">7</span>-n)-min         <span class="comment">#小数位取n位后，8位定点数能表示的最大值</span></span><br><span class="line">    innum_abs = abs(innum)     <span class="comment">#不管正负，都按正数处理</span></span><br><span class="line">    <span class="keyword">if</span> (innum_abs&lt;min):        <span class="comment">#如果表示的数小于最小，按最小处理</span></span><br><span class="line">        innum_abs = min</span><br><span class="line">    <span class="keyword">if</span> (innum_abs&gt;max):        <span class="comment">#如果表示的数大于最大，按最大处理</span></span><br><span class="line">        innum_abs =max</span><br><span class="line">    nint = math.floor(innum_abs)     <span class="comment">#取整，分割小数部分和整数部分</span></span><br><span class="line">    nf = innum_abs-nint              <span class="comment">#小数部分</span></span><br><span class="line">    res_nint = bin(int(nint)).replace(<span class="string">'0b'</span>,<span class="string">''</span>)   <span class="comment">#整数部分直接调用bin函数处理</span></span><br><span class="line">    nint_num = len(res_nint)                     <span class="comment">#整数部分的二进制表示占的位数长度</span></span><br><span class="line">    res_nint_array =np.zeros(nint_num)           <span class="comment">#创建矩阵</span></span><br><span class="line">    <span class="comment">#print(nint_num)</span></span><br><span class="line">    res_nf = xiaoshu_bin.xiaoshu(nf,n)           <span class="comment">#小数部分调用xiaoshu_bin()函数</span></span><br><span class="line">    <span class="keyword">if</span> (innum&gt;=<span class="number">0</span>):                               <span class="comment">#原数为正数，二进制第一位为0</span></span><br><span class="line">        c =<span class="number">0</span></span><br><span class="line">        num_add =<span class="number">8</span>-n-nint_num                    <span class="comment">#除去小数位和整数位占的二进制位数后，还剩几位</span></span><br><span class="line">        num_add =np.zeros(num_add)               <span class="comment">#补0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> res_nint:</span><br><span class="line">            res_nint_array[c] =int(value)        <span class="comment">#整数部分二进制</span></span><br><span class="line">            c= c+<span class="number">1</span></span><br><span class="line">        <span class="comment">#@final =[num_add,res_nint_array,n,res_nf]</span></span><br><span class="line">    <span class="keyword">else</span>:                                            <span class="comment">#原数为负数，二进制第一位为1</span></span><br><span class="line">        d =<span class="number">0</span></span><br><span class="line">        num_add = <span class="number">8</span>-n-nint_num</span><br><span class="line">        num_add = np.zeros(num_add)</span><br><span class="line">        num_add[<span class="number">0</span>] =<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> res_nint:</span><br><span class="line">            res_nint_array[d] =int(value)</span><br><span class="line">            d= d+<span class="number">1</span></span><br><span class="line">        <span class="comment">#final = [num_add,res_nint_array,n,res_nf]</span></span><br><span class="line">    final_bin =np.hstack((num_add,res_nint_array,res_nf))  <span class="comment">#最终表示</span></span><br><span class="line">    <span class="keyword">for</span> bin_value <span class="keyword">in</span> final_bin:</span><br><span class="line">        list.append(str(int(bin_value)))                 <span class="comment">#字符串输出</span></span><br><span class="line">    final_bin_value =<span class="string">''</span>.join(list)</span><br><span class="line">    <span class="keyword">return</span> final_bin_value</span><br><span class="line"><span class="comment">#print(float_to_bin(-4.5，3))</span></span><br></pre></td></tr></table></figure></li><li><p><strong>xiaoshu_bin()</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xiaoshu</span><span class="params">(innum, n)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> N</span><br><span class="line">    N =n                       <span class="comment">#小数部分占的位数</span></span><br><span class="line">    count =<span class="number">0</span></span><br><span class="line">    temp = innum</span><br><span class="line">    reco =np.zeros(N)        <span class="comment">#创建全0矩阵</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (innum&gt;<span class="number">1</span>) <span class="keyword">or</span> (N==<span class="number">0</span>):   <span class="comment">#不是小数</span></span><br><span class="line">        print(<span class="string">'Error!'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">while</span>(N):                <span class="comment">#未超过小数部分的位数</span></span><br><span class="line">        count =count+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (count&gt;N):</span><br><span class="line">            N = <span class="number">0</span></span><br><span class="line">            <span class="keyword">return</span> reco</span><br><span class="line">        temp =temp*<span class="number">2</span>              <span class="comment">#小数部分不断的乘2</span></span><br><span class="line">        <span class="keyword">if</span> (temp&gt;<span class="number">1</span>):</span><br><span class="line">            reco[count<span class="number">-1</span>] =<span class="number">1</span></span><br><span class="line">            temp = temp<span class="number">-1</span></span><br><span class="line">        <span class="keyword">elif</span> (temp==<span class="number">1</span>):</span><br><span class="line">            reco[count<span class="number">-1</span>] =<span class="number">1</span></span><br><span class="line">            N =<span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reco[count<span class="number">-1</span>] =<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (reco)</span><br><span class="line"><span class="comment">#print(xiaoshu(0.0525,4))</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="量化结果展示"><a href="#量化结果展示" class="headerlink" title="量化结果展示"></a>量化结果展示</h3><p>以<code>InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm_beta</code>这一tensor为例：</p><p><strong>量化前后对比</strong>：</p><p><img src="/2019/02/17/20190217FaceNet在FPGA等硬件平台上的实现/7.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍将训练好的网络模型，移植到FPGA等硬件平台上所必须的准备工作。&lt;/p&gt;
    
    </summary>
    
      <category term="项目实战" scheme="http://moyingyao.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="硬件平台实现" scheme="http://moyingyao.github.io/tags/%E7%A1%AC%E4%BB%B6%E5%B9%B3%E5%8F%B0%E5%AE%9E%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>基于深度神经网络的图像分类与去噪</title>
    <link href="http://moyingyao.github.io/2018/10/04/20181004%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8E%BB%E5%99%AA/"/>
    <id>http://moyingyao.github.io/2018/10/04/20181004基于深度神经网络的图像分类与去噪/</id>
    <published>2018-10-04T08:23:24.000Z</published>
    <updated>2019-06-20T07:21:13.479Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博文主要介绍噪音类型的分类和去噪。<a id="more"></a></p><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>现有的去噪方法取决于噪声类型的信息，通常由专家分类。换句话说，那些方法没有应用计算方法来对图像噪声类型进行预分类。此外，这些方法假设图像的噪声类型是像高斯噪声那样的单类噪声类型，这限制了实际应用中去噪方法的选择和能力。<br>与现有方法不同，我们采用一种新框架，<strong>不仅可以对单一类型噪声进行分类和去噪，而且可以根据实际需要对混合类型的噪声进行分类和去噪</strong>。</p><p><strong>我们的方法和现有方法对比</strong><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/1.png" alt=""></p><p><strong>数据集</strong></p><ol><li><code>Mnist</code>：Subset（10000张）</li><li><code>cifar10</code>：Subset（10000张）</li><li><code>BSD500</code>：Fullset（500张）</li></ol><p><strong>方法</strong><br>设计一个包含2个网络的框架：<strong>噪音分类网络</strong> <strong>去噪网络</strong>。</p><ol><li>噪音分类网络：对图像所包含的噪音类型进行识别分类</li><li>去噪网络：根据噪音分类网络判别的噪音类型选择相应的去噪模型去除噪音，进而得到干净图像</li></ol><p><strong>总体框架图如下所示</strong><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/2.png" alt=""></p><h2 id="项目实现过程"><a href="#项目实现过程" class="headerlink" title="项目实现过程"></a>项目实现过程</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>针对原始<strong>BSD500</strong>数据集，对其进行灰度转换，得到它的灰度图。<br>对于<code>Mnist（gray）</code>、<code>Cifar10（RGB）</code>、<code>BSD500（Gray&amp;RGB）</code>这4个数据集，在每个数据集上添加<strong>Gaussian、Salt、Speckle、Poisson</strong>噪音（可以组合），最终得到</p><ol><li>单类噪音数据集4类：Ga，Sa，Sp ，Po</li><li>两类噪音数据集6类：Ga&amp;Sa，Ga&amp;Sp，Ga&amp;Po，Sa&amp;Sp，Sa&amp;Po，Sp&amp;Po</li><li>多类噪音数据集5类（3和4）：Ga&amp;Sa&amp;Sp，Ga&amp;Sa&amp;Po，Ga&amp;Sp&amp;Po，Sa&amp;Sp&amp;Po， Ga&amp;Sa&amp;Sp&amp;Po</li><li>混合噪音数据集共15类：综合上述1，2，3</li></ol><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/3.png" alt=""></p><h3 id="噪音分类网络"><a href="#噪音分类网络" class="headerlink" title="噪音分类网络"></a>噪音分类网络</h3><p><strong>网络模型</strong><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/4.png" alt=""></p><p><strong>详细参数</strong><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/5.png" alt=""></p><p>利用Mnist和Cifar10的噪音数据集，对比几个分类网络，筛选出噪音识别准确率最高的网络（<strong>Our</strong>）。</p><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/7.png" alt=""></p><p><strong>为了和去噪网络的数据集匹配，将BSD的混合噪音也输入了我们设计的CNN网络，训练了一个分类模型。</strong>结果如下：</p><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/8.png" alt=""></p><h3 id="去噪网络设计"><a href="#去噪网络设计" class="headerlink" title="去噪网络设计"></a>去噪网络设计</h3><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/6.png" alt=""></p><p>去噪网络主要由三种类型的层组成，如上图所示，<strong>由三种不同的颜色表示</strong>。 </p><ol><li>在第一层，我们使用96个尺寸的3×3×c滤波器进行卷积并生成96个特征图，我们在输入到下一层时使用整流线性单元的变量LReLU 来实现非线性。 这里，c表示训练图像的通道数，c表示灰度1，c表示彩色图3。</li><li>从第2层到第（d-1）层，使用96个尺寸为3×3×96的滤波器 并且还使用了LReLU，并在卷积层和LReLU层之间进行了批量归一化操作，从而可以提高训练速度并加快收敛过程。</li><li>对于最后一层，它使用大小为3×3×96的c滤波器重建输出。 它还删除了所有下采样层，因为这会大大降低我们模型的去噪效果。</li></ol><p>把BSD500的8种单类噪音（<strong>Gray和RGB各4种</strong>）数据输入设计的去噪网络，得到了8个单类噪音去噪模型。</p><p><strong><font color="#FF0000">测试</font></strong>：</p><ol><li>如果图片被分类网络识别出仅包含单类噪音，就选择相应的单类去噪模型去除噪音</li><li>如果图片被分类网络识别出包含多种噪音，根据所包含的噪音类型，依次选择多个单类去噪模型去除噪音</li></ol><h3 id="去噪结果分析"><a href="#去噪结果分析" class="headerlink" title="去噪结果分析"></a>去噪结果分析</h3><p><strong>去噪好坏的评价标准</strong></p><ol><li><strong>PSNR</strong>（Peak Signal to Noise Ratio）峰值信噪比<br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/9.png" alt=""></li></ol><p>其中，<code>MSE</code>表示当前图像X和参考图像Y的均方误差（Mean Square Error），<code>H、W</code>分别为图像的高度和宽度；<code>n</code>为每像素的比特数，一般取8，即像素灰阶数为256.。<code>PSNR</code>的单位是<code>dB</code>，<strong>数值越大表示失真越小越好</strong>。PSNR是最普遍和使用最为广泛的一种图像客观评价指标，然而它是基于对应像素点间的误差，即基于误差敏感的图像质量评价。<strong>由于并未考虑到人眼的视觉特性</strong>（人眼对空间频率较低的对比差异敏感度较高，人眼对亮度对比差异的敏感度较色度高，人眼对一个区域的感知结果会受到其周围邻近区域的影响等），<strong>因而经常出现评价结果与人的主观感觉不一致的情况</strong>。为此我们还引入了下面一个评价指标</p><ol start="2"><li><strong>SSIM</strong>（structural similarity）结构相似性。它分别从<strong>亮度、对比度、结构</strong>三方面度量图像相似性。<br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/10.png" alt=""></li></ol><p>其中<img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/111.png" alt="">分别表示图像X和Y的均值，<img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/222.png" alt="">分别表示图像X和Y的方差，<img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/333.png" alt="">表示图像X和Y的协方差，即</p><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/11.png" alt=""></p><p>C1,C2,C3为常数，为了避免分母为0的情况，通常取<img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/444.png" alt="">一般地K1=0.01,K2=0.03, L=255​， 则</p><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/12.png" alt=""></p><p><strong>SSIM</strong>取值范围[0,1]，值越大，表示图像失真越小越好。</p><h4 id="单类噪音"><a href="#单类噪音" class="headerlink" title="单类噪音"></a>单类噪音</h4><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/13.png" alt=""><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/14.png" alt=""></p><p><strong>与其它方法的对比</strong><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/15.png" alt=""><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/16.png" alt=""></p><h4 id="两类噪音"><a href="#两类噪音" class="headerlink" title="两类噪音"></a>两类噪音</h4><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/17.png" alt=""><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/18.png" alt=""></p><h4 id="多类噪音"><a href="#多类噪音" class="headerlink" title="多类噪音"></a>多类噪音</h4><p><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/19.png" alt=""><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/20.png" alt=""><br><img src="/2018/10/04/20181004基于深度神经网络的图像分类与去噪/21.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>在单类噪音上取得了不错的效果，彩色图要比灰度图的去噪效果好，而且比已有的方法要好。</li><li>多类噪音去噪效果不够好，灰度图要比彩色图好。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇博文主要介绍噪音类型的分类和去噪。&lt;/p&gt;
    
    </summary>
    
      <category term="项目实战" scheme="http://moyingyao.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="denoise" scheme="http://moyingyao.github.io/tags/denoise/"/>
    
  </entry>
  
  <entry>
    <title>利用深度学习对医学CT图像(LIDC-IDRI)中的肺结节进行良恶性判断</title>
    <link href="http://moyingyao.github.io/2018/08/15/20180815%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%8C%BB%E5%AD%A6-CT-%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%82%BA%E7%BB%93%E8%8A%82%E8%BF%9B%E8%A1%8C%E8%89%AF%E6%81%B6%E6%80%A7%E5%88%A4%E6%96%AD/"/>
    <id>http://moyingyao.github.io/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/</id>
    <published>2018-08-15T02:22:47.000Z</published>
    <updated>2019-06-19T08:59:28.745Z</updated>
    
    <content type="html"><![CDATA[<p>肺癌是最常见的癌症，目前，CT可用于帮助医生在早期阶段检测肺癌。 在许多情况下，识别肺癌的诊断取决于医生的经验，这可能会忽略一些患者并导致一些问题。 在许多医学影像诊断领域，深度学习已被证明是一种流行且有效的方法。<a id="more"></a> 本文主要基于<a href="http://imaging.cancer.gov/reportsandpublications/reportsandpresentations/firstdataset" target="_blank" rel="noopener">LIDC-IDRI</a>这一公开数据集，对其进行了肺结节的提取，并利用CNN对其分类训练，从而辅助医生作出判断。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据集采用为<strong> LIDC-IDRI</strong> （The Lung Image Database Consortium），该数据集由胸部医学<strong>图像文件(.dcm)</strong>(如CT、X光片)和对应的诊断结果病变<strong>标注(.xml)</strong>组成。数据是由美国国家癌症研究所(National Cancer Institute)发起收集的，目的是为了研究高危人群早期癌症检测。<br>该数据集中，共收录了1012个研究实例。对于每个实例中的图像，都由4位经验丰富的胸部放射科医师进行两阶段的诊断标注。在第一阶段，每位医师分别独立诊断并标注病患位置，其中会标注三中类别：</p><ol><li>&gt;=3mm的结节</li><li>&lt;3mm的结节</li><li>&gt;=​3mm的非结节</li></ol><p>在随后的第二阶段中，各位医师都分别独立的复审其他三位医师的标注，并给出自己最终的诊断结果。这样的两阶段标注可以在避免forced consensus的前提下，尽可能完整的标注所有结果。</p><h4 id="图像信息（-dcm）"><a href="#图像信息（-dcm）" class="headerlink" title="图像信息（.dcm）"></a>图像信息（.dcm）</h4><p>图像文件为<strong>Dicom格式</strong>，是医疗图像的标准格式，其中除了图像像素外，还有一些辅助的元数据如图像类型、图像时间等信息。<br>一张CT图像有 512x512 个像素点，在dicom文件中每个像素由2字节表示，所以每张图片约512KB大小。<br>目前测试一共1012个病例数据，对于每个实例，可以看为一个<strong>三维矩阵D(slicer <em> rows </em> cols)</strong>, slicer表示切片的个数(对应每个病例的.dcm文件数)，rows和cols分别表示图片的行数和列数(默认为512)。<br><strong>eg:</strong> 对于病例LIDC-IDRI-0001，即为133x512x512的矩阵，一共133张切片，每张大小512x512。</p><p><strong>查看dcm文件：</strong></p><ol><li>通过pip或者Anaconda安装<strong>pydicom</strong>模块，该模块是python专门用来处理dicom格式文件的库。</li></ol><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/1.png" alt=""></p><ol start="2"><li>通过软件<strong>MicroDicom viewer</strong></li></ol><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/2.png" alt=""></p><p>通过上面2种方式，我们可以看出dicom文件中包含了一些图像信息（SOP Instance UID、Study Instance UID，Series Instance UID······）<br><strong>SOP Instance UID：</strong>用于唯一区分每一张dcm切片<br><strong>Study Instance UID:</strong> 每个病例对应的检查实例号<br><strong>Series Instance UID: </strong>不同检查对应的序列实例号</p><h4 id="注释信息（-xml）"><a href="#注释信息（-xml）" class="headerlink" title="注释信息（.xml）"></a>注释信息（.xml）</h4><p>Xml文件中包含放射科医生对病人CT图像中疑似肺结节的标注信息，主要分为三类：</p><ol><li>结节(3mm-33mm):<font color="#FF0000">包含结节的特征信息（characteristics）、结节的完整轮廓(roi)</font></li><li>结节（&lt;3mm）：只显示结节的近似三维重心，若不透明则不标记</li><li>非结节（&gt;3mm）：只显示其近似的三维重心，指出非结节连接区域</li></ol><p><strong>Xml文件大体结构图如下：</strong></p><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/3.png" alt=""></p><p>其中对于3mm—33mm结节的characteristics，包含了如下信息：<br>1）    Subtlety：检测难度（1-5级，1最难，5最明显）<br>2）    internalStructure：内部结构（4种，软组织、液体、脂肪、空气）<br>3）    calcification：钙化（6种情况）<br>4）    sphericity：球形度（5种程度，但只明确3种）<br>5）    margin：边缘（5种程度）<br>6）    lobulation：分叶征（5种情况，但只明确2种）<br>7）    spiculation：毛刺征（5种情况，但只明确2种）<br>8）    texture：纹理（5种情况，但只明确3种）</p><font color="#FF0000">9）    maliynancy：恶性程度（1-5，1最低，5最高）</font><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>本部分主要做的工作是分割肺实质，提取肺结节。</p><h4 id="图像存储格式转换"><a href="#图像存储格式转换" class="headerlink" title="图像存储格式转换"></a>图像存储格式转换</h4><p>原始数据集的图像信息是以dcm格式存储的，但通常我们用作训练数据输入网络的图像大多是jpg或者png格式，所以为了方便以后的训练，我们首先要将原始图像转为jpg格式或者png格式存储，在这里我们是转为jpg格式存储的。<br>此处测试共包含1012个病例，每个病例包含约100—300个dcm文件，我们使用MicroDicom viewer软件对其进行批量转换。<br><strong>以LIDC-IDRI-0001 为例：</strong></p><blockquote><p><strong>原始数据</strong><br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/4.png" alt=""><br><strong>转换后</strong><br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/5.png" alt=""></p></blockquote><p>将原始数据转换为jpg格式的图片后，下面我们会利用matlab编写函数分割肺实质，提取肺结节，主要包含下面6个函数：</p><ol><li><code>find_files()</code>: 递归的遍历文件目录</li><li><code>fengefeishizhi()</code>: 分割肺实质</li><li><code>readxml()</code>: 读取标注信息(.xml)文件</li><li><code>readdicom()</code> 存储标注信息中的肺结节信息，方便后面提取</li><li><code>jianqieimage()</code>: 剪切肺结节</li><li><code>jianqie()</code>: 根据肺结节的轮廓信息将其剪切出来，存为图片</li></ol><h4 id="分割肺实质"><a href="#分割肺实质" class="headerlink" title="分割肺实质"></a>分割肺实质</h4><p>将图像转换为jpg格式存储后，我们还要对数据进一步处理。由于我们最终是以肺结节图像作为训练数据输入网络，那么<strong>CT图像中除肺部以外的信息是无用的</strong>，所以我们要将肺实质分割出来。主要用到1个函数： <code>fengefeishizhi()</code>。<br><strong>代码如下：</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line">tic</span><br><span class="line"><span class="keyword">for</span> q =<span class="number">2</span>:<span class="number">3</span></span><br><span class="line">    str1 = num2str(q);</span><br><span class="line">    <span class="comment">%jpg数据格式的存储路径</span></span><br><span class="line">    str2 = <span class="string">'D:\MATLAB\work\0001-0120\LIDC-IDRI-000'</span>;</span><br><span class="line">    str3 =<span class="string">'\*.jpg'</span>;</span><br><span class="line">    str4 =<span class="string">'\';</span></span><br><span class="line"><span class="string">    %分割好肺实质后的图片存储路径</span></span><br><span class="line"><span class="string">    str5='</span>D:\MATLAB\work\<span class="number">0001</span><span class="number">-0120</span>_fenge\<span class="number">000</span><span class="string">';</span></span><br><span class="line"><span class="string">    str_imagedir = strcat(str2,str1,str3);</span></span><br><span class="line"><span class="string">    str_dirname = strcat(str2,str1,str4);</span></span><br><span class="line"><span class="string">    str_write= strcat(str5,str1,str4);</span></span><br><span class="line"><span class="string">    %disp(str_imagedir)</span></span><br><span class="line"><span class="string">    %disp(str_dirname)</span></span><br><span class="line"><span class="string">    %disp(str_write)7</span></span><br><span class="line"><span class="string">    imagelist = dir(str_imagedir);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    for i = 1:length(imagelist)</span></span><br><span class="line"><span class="string">        name = imagelist(i).name;</span></span><br><span class="line"><span class="string">        dirname = [str_dirname,name];</span></span><br><span class="line"><span class="string">%B=imread(dirname);%读取原图像</span></span><br><span class="line"><span class="string">% B=rgb2gray(A);%将原图像转换为灰度图像</span></span><br><span class="line"><span class="string">    A=imread(dirname);</span></span><br><span class="line"><span class="string">    B=rgb2gray(A);</span></span><br><span class="line"><span class="string">%subplot(2,2,1),imshow(B,[]),title('</span>DICOM图像导入后显示<span class="string">');</span></span><br><span class="line"><span class="string">% figure,imshow(B),title('</span>图像导入后显示<span class="string">');</span></span><br><span class="line"><span class="string">%====================================================</span></span><br><span class="line"><span class="string">    min(min(B));</span></span><br><span class="line"><span class="string">    max(max(B));</span></span><br><span class="line"><span class="string">    t=graythresh(B);%计算阈值t</span></span><br><span class="line"><span class="string">    C=im2bw(B,t);%根据阈值二值化图像</span></span><br><span class="line"><span class="string">% figure(),imshow(C,[]),title('</span>显示二值化图像<span class="string">');</span></span><br><span class="line"><span class="string">% C=bwareaopen(C,6000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为10000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    D=imfill(C,4,'</span>holes');<span class="comment">%对二值化后的图像填充肺实质</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% figure(),imshow(D,[]),title('显示填充肺实质图像');</span></span><br><span class="line">    E=D-C;<span class="comment">%得到肺实质的图像E</span></span><br><span class="line"><span class="comment">% figure(),imshow(E,[]),title('显示肺实质的图像');</span></span><br><span class="line">    F=imfill(E,<span class="number">8</span>,<span class="string">'holes'</span>);<span class="comment">%填充肺实质空洞</span></span><br><span class="line"><span class="comment">%  FMask=bwareaopen(F,1000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600</span></span><br><span class="line">    FMask=bwareaopen(F,<span class="number">6000</span>);<span class="comment">%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600</span></span><br><span class="line"><span class="comment">%  figure(),imshow(FMask,[]),title('显示掩摸');%得到掩膜</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------分开左右肺----------------------------------------</span></span><br><span class="line">    r_ball=<span class="number">90</span>;<span class="comment">%可变的，取值为10/15,越小越细致</span></span><br><span class="line">    se_ball=strel(<span class="string">'ball'</span>,r_ball,<span class="number">10</span>);<span class="comment">%椭圆体半径10，高度10</span></span><br><span class="line">    r_disk=<span class="built_in">ceil</span>(r_ball/<span class="number">6</span>);<span class="comment">%圆整r_ball/6得到大于或等于它的最接近整数。ceil取整</span></span><br><span class="line">        <span class="keyword">if</span> r_disk==<span class="number">0</span>;</span><br><span class="line">            r_disk=<span class="number">1</span>;<span class="comment">%最小为1</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    se_erode=strel(<span class="string">'disk'</span>,r_disk,<span class="number">0</span>); <span class="comment">%圆形半径</span></span><br><span class="line">    mask=imopen(FMask,<span class="number">1</span>);<span class="comment">%开操作</span></span><br><span class="line"><span class="comment">% figure(),imshow(mask,[]);</span></span><br><span class="line"></span><br><span class="line">    L=bwlabel(FMask);   <span class="comment">%数学形态重建，基于膨胀运算，用掩摸对二值图像标记，将图像分成多个区域</span></span><br><span class="line"><span class="comment">%stat = regionprops(FMask);%,计算图像区域特征，区域连通，object为二值图像，</span></span><br><span class="line">    [row,col]=<span class="built_in">size</span>(B);</span><br><span class="line"><span class="comment">%im2bw，Convert image to binary image, based on threshold</span></span><br><span class="line"><span class="comment">%im2bw默认threshold0.5，得到512*512空矩阵</span></span><br><span class="line">    mask_leftlung=im2bw(<span class="built_in">zeros</span>(row,col));<span class="comment">%左肺掩膜</span></span><br><span class="line">    mask_rightlung=im2bw(<span class="built_in">zeros</span>(row,col));<span class="comment">%右肺掩膜</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:row</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:col</span><br><span class="line">                <span class="keyword">if</span> L(<span class="built_in">i</span>,<span class="built_in">j</span>)==<span class="number">1</span> <span class="comment">%如果是左肺</span></span><br><span class="line">                    mask_leftlung(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="number">1</span>;<span class="comment">% 分开左右肺，肺是白色的</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                <span class="keyword">if</span> L(<span class="built_in">i</span>,<span class="built_in">j</span>)==<span class="number">2</span></span><br><span class="line">                    mask_rightlung(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% figure(),imshow(mask_leftlung,[]);title('左肺掩摸显示')</span></span><br><span class="line"><span class="comment">% figure(),imshow(mask_rightlung,[]);title('右肺掩摸显示')</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%----------------------对左肺修补-------------------------------------------</span></span><br><span class="line">    object1=<span class="number">1</span>-mask_leftlung; <span class="comment">%左肺反向</span></span><br><span class="line"><span class="comment">% figure();imshow(object1,[]);title('左肺反向后显示')</span></span><br><span class="line">    object2=imopen(object1,se_ball);<span class="comment">%开操作，椭圆体半径30，高度10</span></span><br><span class="line"><span class="comment">% figure();imshow(object2,[]);title('反向左肺模糊重影图显示')   %得到反向左肺模糊重影图</span></span><br><span class="line">    leftmask1=<span class="number">1</span>-object2;<span class="comment">%左肺模糊重影图  </span></span><br><span class="line"><span class="comment">% figure();imshow(leftmask1,[]);title('左肺模糊重影图显示')</span></span><br><span class="line"></span><br><span class="line">    leftmask2=im2bw(leftmask1,<span class="number">0.5</span>);<span class="comment">%根据阈值0.5将图像生成二值图像</span></span><br><span class="line"><span class="comment">%figure();imshow(leftmask2,[]);title('左肺清晰二值图像显示')</span></span><br><span class="line"><span class="comment">%%得到左肺清晰的二值图像，支气管消去了，结节的毛刺也消除，结节变小；对左肺进行了修补</span></span><br><span class="line">    leftmask3=imfill(leftmask2,<span class="string">'hole'</span>);  <span class="comment">%填充左肺实质空洞</span></span><br><span class="line"><span class="comment">% figure();imshow(leftmask3,[]),title('填充左肺实质后显示'); %只是填充了左肺实质，得到不平滑的左肺图像  </span></span><br><span class="line">    leftmask4=imerode(leftmask3,se_erode);<span class="comment">%腐蚀左肺操作，肺结节大了点，平滑作用</span></span><br><span class="line"><span class="comment">% figure();imshow(leftmask4,[]),title('leftlungmask');%得到平滑效果图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%---------------------补回空洞----------------------------------------------</span></span><br><span class="line">    ConvHull=bwconvhull(leftmask4,<span class="string">'object'</span>);<span class="comment">%对左肺掩摸求凸壳</span></span><br><span class="line"><span class="comment">%figure();imshow(ConvHull,[]),title('凸壳图像');</span></span><br><span class="line">    DIF_ConvHull=ConvHull-leftmask4;<span class="comment">%将补的缺口部分取出来</span></span><br><span class="line"><span class="comment">%figure();imshow(DIF_ConvHull,[]),title('与左肺原图差值图像');</span></span><br><span class="line">    BW1 = bwconncomp(DIF_ConvHull);<span class="comment">%利用连通域分析左肺凸壳</span></span><br><span class="line">    stats = regionprops(BW1, <span class="string">'Area'</span>,<span class="string">'Eccentricity'</span>);<span class="comment">%获得每个连通域得面积、离心率</span></span><br><span class="line">    idx = <span class="built_in">find</span>([stats.Area] &gt; <span class="number">80</span> &amp; [stats.Eccentricity] &lt; <span class="number">0.8</span>); </span><br><span class="line"><span class="comment">% % % % BW2 = ismember(labelmatrix(BW1), idx);%取出符合要求的区域</span></span><br><span class="line"><span class="comment">% % % % figure();imshow(BW2,[]),title('左肺所需要补的部分显示');</span></span><br><span class="line"><span class="comment">% % % % leftmask5=BW2+leftmask4;%将符合要求的区域“补”到左肺掩摸中</span></span><br><span class="line"><span class="comment">%figure();imshow(leftmask5,[]),title('显示最终的左肺掩摸');</span></span><br><span class="line"><span class="comment">%---------------------对右肺修补--------------------------------------------</span></span><br><span class="line">    object1=<span class="number">1</span>-mask_rightlung; <span class="comment">%反转右肺轮廓</span></span><br><span class="line">    <span class="comment">%figure();imshow(object1,[]);title('右肺反向后显示')</span></span><br><span class="line">    object2=imopen(object1,se_ball);<span class="comment">%开操作</span></span><br><span class="line"> <span class="comment">%figure();imshow(object2,[]);title('反向右肺模糊重影图显示')   %得到反向右肺模糊重影图</span></span><br><span class="line">    rightmask1=<span class="number">1</span>-object2;<span class="comment">%得到右肺模糊掩膜，反转回来，实质为白色</span></span><br><span class="line"> <span class="comment">%figure();imshow(rightmask1,[]);title('右肺模糊重影图显示')</span></span><br><span class="line"></span><br><span class="line">    rightmask2=im2bw(rightmask1,<span class="number">0.5</span>);<span class="comment">%右肺转换为二值图像</span></span><br><span class="line"> <span class="comment">%figure();imshow(rightmask2,[]);title('右肺清晰二值图像显示')</span></span><br><span class="line">    rightmask3=imfill(rightmask2,<span class="string">'hole'</span>);<span class="comment">%填充右肺实质空洞</span></span><br><span class="line"> <span class="comment">%figure();imshow(rightmask3,[]),title('填充右肺实质后显示');</span></span><br><span class="line">    rightmask4=imerode(rightmask3,se_erode);<span class="comment">%腐蚀操作，平滑作用</span></span><br><span class="line"><span class="comment">%  figure();imshow(rightmask4,[]),title('rightlungmask');</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">% % %  lungmask=im2bw(leftmask5+rightmask4);%将左右肺合并，得到全肺掩膜</span></span><br><span class="line">    lungmask=im2bw(leftmask4+rightmask4);<span class="comment">%将左右肺合并，得到全肺掩膜</span></span><br><span class="line">    lung=immultiply(lungmask,B);<span class="comment">%相与,得到的是灰度值从0到max-min+1的灰度图像</span></span><br><span class="line"> <span class="comment">%dicomwrite(lung,'E:\1_毕业设计\images_CT\S60\I00');%dicomwrite()函数将lung（从源图像提取出来的肺实质）图像保存为dicom文件格式，方便下次使用</span></span><br><span class="line"> <span class="comment">%subplot(2,2,2),imshow(lung,[]),title('提取的肺实质');</span></span><br><span class="line"> <span class="comment">%figure;imshow(lung,[]),title('提取的肺实质');</span></span><br><span class="line"><span class="comment">%name = + name;</span></span><br><span class="line">    feishizhi = [str_write,name];</span><br><span class="line">    imwrite(lung,feishizhi);</span><br><span class="line"><span class="comment">%break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><strong>以LIDC-IDRI-0001 中的部分切片为例，其中左侧为原始CT图像，右侧为分割肺实质后的图像，效果图如下:</strong></p><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/6.png" alt=""></p><h4 id="读取标注信息并存储"><a href="#读取标注信息并存储" class="headerlink" title="读取标注信息并存储"></a>读取标注信息并存储</h4><p>我们从医生的标注信息文件（.xml）读取肺结节的位置信息和良恶性程度，然后存储到对应的xls表中。主要用到2个函数： <code>readxml()</code>和<code>readdicom()</code><br><strong>代码如下：</strong><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[num_mal,sop_text,max_min_xy]</span>=<span class="title">zl_readxml</span><span class="params">(xml_path)</span></span></span><br><span class="line"><span class="comment">% % function [sop_text,max_min_xy]=zl_readxml(xml_path)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">% clear all</span></span><br><span class="line"><span class="comment">% clc</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%xml_path = 'H:\肺结节\数据\LIDC-IDRI\900-300\LIDC-IDRI\LIDC-IDRI-0060\1.3.6.1.4.1.14519.5.2.1.6279.6001.203745372924354240670222118382\1.3.6.1.4.1.14519.5.2.1.6279.6001.463214953282361219537913355115\191.xml';</span></span><br><span class="line"><span class="comment">%% 跳转到内层标签unblindedReadNodule</span></span><br><span class="line">docNode = xmlread(xml_path);     <span class="comment">%读取XML文件返回一个文件模型节点*  </span></span><br><span class="line">document = docNode.getDocumentElement();</span><br><span class="line">readingSession = document.getElementsByTagName(<span class="string">'readingSession'</span>);  <span class="comment">%返回与给定的元素所有子节点的Nodelist对象*</span></span><br><span class="line"><span class="comment">%% 最后返回的三个值</span></span><br><span class="line"><span class="comment">%% 最后返回的三个值</span></span><br><span class="line">num_mal = []; <span class="comment">%每个结节的恶性度和属于该类别的图片的数量</span></span><br><span class="line">sop_text = &#123; &#125;; <span class="comment">%每个图片的标号</span></span><br><span class="line">max_min_xy = []; <span class="comment">%每个图像中肺结节的x和y的最小值和最大值</span></span><br><span class="line">sop_num = <span class="number">0</span>;         <span class="comment">%总结节个数？*</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="keyword">for</span> r = <span class="number">0</span>:readingSession.getLength()<span class="number">-1</span></span><br><span class="line">        unblinded_nodule = readingSession.item(r).getElementsByTagName(<span class="string">'unblindedReadNodule'</span>);     <span class="comment">%unblindedReadNodule一个节点标记，&lt;unblindedReadNodule&gt;节点数据包括在&lt;/unblindedReadNodule&gt;*</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> u = <span class="number">0</span> : unblinded_nodule.getLength()<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">            roi = unblinded_nodule.item(u).getElementsByTagName(<span class="string">'roi'</span>);   <span class="comment">%item() 方法可返回节点列表中处于指定索引号的节点。*&lt;roi&gt;结节轮廓&lt;/roi&gt;*</span></span><br><span class="line">            mal = unblinded_nodule.item(u).getElementsByTagName(<span class="string">'malignancy'</span>);    <span class="comment">%&lt;malignancy&gt;结节恶性度&lt;/malignancy&gt;*</span></span><br><span class="line">            <span class="comment">%如果xml文件中没有malignancy或者roi标签直接跳过</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isempty</span>(roi.item(<span class="number">0</span>))       </span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isempty</span>(mal.item(<span class="number">0</span>))       </span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">            Num_roi = roi.getLength();   <span class="comment">%该类别的图片的数量</span></span><br><span class="line">            mal_int = str2num(char(mal.item(<span class="number">0</span>).getTextContent()));</span><br><span class="line">            num_mal = [num_mal();mal_int,Num_roi];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">0</span> : Num_roi<span class="number">-1</span>  <span class="comment">%遍历*</span></span><br><span class="line">                sop_id = roi.item(<span class="built_in">i</span>).getElementsByTagName(<span class="string">'imageSOP_UID'</span>);    <span class="comment">%图片编号*  </span></span><br><span class="line">                sop_text&#123;sop_num + i + <span class="number">1</span>&#125; = char(sop_id.item(<span class="number">0</span>).getTextContent());   <span class="comment">%数组*</span></span><br><span class="line">                edgeMap = roi.item(<span class="built_in">i</span>).getElementsByTagName(<span class="string">'edgeMap'</span>);   <span class="comment">%边界* </span></span><br><span class="line">                 xy = [];</span><br><span class="line">                <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">0</span> :edgeMap.getLength()<span class="number">-1</span>            <span class="comment">%获得坐标*</span></span><br><span class="line">                    xCoord = edgeMap.item(<span class="built_in">j</span>).getElementsByTagName(<span class="string">'xCoord'</span>);</span><br><span class="line">                    xCoord_int = str2num(char(xCoord.item(<span class="number">0</span>).getTextContent()));</span><br><span class="line"></span><br><span class="line">                    yCoord = edgeMap.item(<span class="built_in">j</span>).getElementsByTagName(<span class="string">'yCoord'</span>);</span><br><span class="line">                    yCoord_int = str2num(char(yCoord.item(<span class="number">0</span>).getTextContent()));</span><br><span class="line">                    xy=[xy();xCoord_int,yCoord_int];</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                <span class="comment">%找到结节轮廓*</span></span><br><span class="line">                <span class="keyword">if</span> edgeMap.getLength()==<span class="number">1</span></span><br><span class="line">                   max_min_xy = [max_min_xy();xy,xy];</span><br><span class="line">                   <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                [maxr,max_index] = max(xy);</span><br><span class="line">                [minr,min_index] = min(xy);</span><br><span class="line">                max_min_xy = [max_min_xy();minr,maxr];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">            sop_num = sop_num + Num_roi;   <span class="comment">%总个数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isempty</span>(num_mal)                      </span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">         num_mal = [num_mal();<span class="number">0</span>,<span class="number">0</span>];    <span class="comment">%扩展维数*</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p><p><strong>上述是辅助函数，提取函数如下所示：</strong><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">clear;</span><br><span class="line">clc;     </span><br><span class="line"><span class="comment">%% Each treatment 100 or 200       %处理数据导入到表格中</span></span><br><span class="line"><span class="comment">%LIDC_path   = 'E:\zhaolei\深度学习\肺结节\400-499\LIDC-IDRI\';    原路径</span></span><br><span class="line">LIDC_path   = <span class="string">'D:\MATLAB\tiqu\LIDC-IDRI'</span>;</span><br><span class="line"><span class="comment">%IDRI_path = 'H:\肺结节\数据\LIDC-IDRI\';     原路径</span></span><br><span class="line"><span class="comment">%XLS_path = 'H:\肺结节\数据\excel\excel_all';     原路径</span></span><br><span class="line">IDRI_path = <span class="string">'D:\MATLAB\tiqu\LIDC-IDRI'</span>;</span><br><span class="line">XLS_path = <span class="string">'D:\MATLAB\tiqu\xls'</span>;</span><br><span class="line">IDRI_child_path = dir(IDRI_path);                               <span class="comment">%打开文件目录并返回文件结构体*</span></span><br><span class="line">num_IDRI_child = <span class="built_in">size</span>(IDRI_child_path);                             <span class="comment">%返回列和行数的数组*  </span></span><br><span class="line"><span class="comment">%for n = 8 :num_IDRI_child 原版            %为啥从8开始？？？</span></span><br><span class="line"><span class="keyword">for</span> n = <span class="number">3</span> :num_IDRI_child    <span class="comment">%非原版</span></span><br><span class="line">   <span class="comment">% child_idri_path = [IDRI_path,IDRI_child_path(n).name];原版（可能有错）</span></span><br><span class="line">    child_idri_path = [IDRI_path,<span class="string">'\',IDRI_child_path(n).name];   %非原版</span></span><br><span class="line"><span class="string">    child_idri_path_temp = dir(child_idri_path);                %打开文件*</span></span><br><span class="line"><span class="string">    LIDC_path = [child_idri_path,'</span>\<span class="string">',child_idri_path_temp(3).name];     %文件目录*</span></span><br><span class="line"><span class="string">    LIDC_child_path = dir(LIDC_path);        %打开</span></span><br><span class="line"><span class="string">    num_child = size(LIDC_child_path);      %返回文件的列和行数的数组*</span></span><br><span class="line"><span class="string">    for i = 3 : num_child(1)       %从3开始（前两个是. ..） </span></span><br><span class="line"><span class="string">        %% find dicom file list</span></span><br><span class="line"><span class="string">        child_path = [LIDC_path,'</span>\<span class="string">',LIDC_child_path(i).name];        %一步步打开文件夹</span></span><br><span class="line"><span class="string">        child_path_temp = dir(child_path);</span></span><br><span class="line"><span class="string">        child_path1 = [child_path,'</span>\<span class="string">',child_path_temp(3).name];</span></span><br><span class="line"><span class="string">        child_path_temp = dir(child_path1);</span></span><br><span class="line"><span class="string">        %xml_path = [child_path1,'</span>\<span class="string">',child_path_temp(3).name];</span></span><br><span class="line"><span class="string">        xml_path = [child_path,'</span>\<span class="string">'];</span></span><br><span class="line"><span class="string">        %获取单个文件夹中的dicom和xml文件</span></span><br><span class="line"><span class="string">        dcm_files = find_files(xml_path, '</span>.dcm<span class="string">');    % 获得文件列表    </span></span><br><span class="line"><span class="string">        xml_files = find_files(xml_path, '</span>.xml<span class="string">');</span></span><br><span class="line"><span class="string">        xml_path = char(xml_files);      </span></span><br><span class="line"><span class="string">        [num_mal,sop_text,max_min_xy]=zl_readxml(xml_path);  %函数调用</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">       %  num_mal = []; %每个结节的恶性度和属于该类别的图片的数量</span></span><br><span class="line"><span class="string">       %  sop_text = &#123; &#125;; %每个图片的标号</span></span><br><span class="line"><span class="string">       % max_min_xy = []; %每个图像中肺结节的x和y的最小值和最大值</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        sop_num = size(sop_text);   %  获得行列数，行：？ 列：图片数*</span></span><br><span class="line"><span class="string">        mal_num = size(num_mal);        %行： 图片数？*</span></span><br><span class="line"><span class="string">        dcm_number = [ ];   %图片编号*    </span></span><br><span class="line"><span class="string">        %??</span></span><br><span class="line"><span class="string">        if sop_num(2)&gt;mal_num(1)           %要根据他们两个的差值来决定补多少个0</span></span><br><span class="line"><span class="string">            for m = 1 : sop_num(2)-mal_num(1)    </span></span><br><span class="line"><span class="string">                num_mal = [num_mal();0,0];    %添加扩展维度*</span></span><br><span class="line"><span class="string">            end</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        if sop_num(2)&lt; mal_num(1)</span></span><br><span class="line"><span class="string">            for m = 1 :  mal_num(1) - sop_num(2)     %  只有数据维度一样才能被写入到文件中！所以少的要补上四个0</span></span><br><span class="line"><span class="string">                dcm_number= [dcm_number;0];            %添加扩展维度</span></span><br><span class="line"><span class="string">                max_min_xy = [max_min_xy;0,0,0,0];       %添加扩展维度   </span></span><br><span class="line"><span class="string">            end</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">        %??</span></span><br><span class="line"><span class="string">        %% Get the number and file name of the image In a single folder</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">         for md = 1 : sop_num(2)      %???            </span></span><br><span class="line"><span class="string">            dcm_number= [dcm_number;0];    </span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">           for j = 1:numel(dcm_files)    %遍历文件</span></span><br><span class="line"><span class="string">                    dicomInformation = dicominfo(dcm_files&#123;j&#125;);          %存储图片信息</span></span><br><span class="line"><span class="string">                    instance = dicomInformation.SOPInstanceUID;   </span></span><br><span class="line"><span class="string">                    imagenum = dicomInformation.InstanceNumber; </span></span><br><span class="line"><span class="string">                    % Make sure that the StudyInstanceUID matches that found in</span></span><br><span class="line"><span class="string">                    % the XML annotations</span></span><br><span class="line"><span class="string">                    for s = 1 : sop_num(2)    %对比</span></span><br><span class="line"><span class="string">                        if strcmpi(instance,sop_text(1,s))</span></span><br><span class="line"><span class="string">                           dcm_number(s) = imagenum;     %编号？？?*</span></span><br><span class="line"><span class="string">                        end</span></span><br><span class="line"><span class="string">                    end</span></span><br><span class="line"><span class="string">                    </span></span><br><span class="line"><span class="string">           end</span></span><br><span class="line"><span class="string">           </span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">           total = [num_mal,dcm_number,max_min_xy];</span></span><br><span class="line"><span class="string">           if isempty(total)</span></span><br><span class="line"><span class="string">               continue;</span></span><br><span class="line"><span class="string">           end</span></span><br><span class="line"><span class="string">           child_path = [XLS_path,'</span>\<span class="string">',LIDC_child_path(i).name]</span></span><br><span class="line"><span class="string">           xlswrite(child_path,total);     %导入到表格中 2017/4/10</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">end</span></span><br></pre></td></tr></table></figure></p><h4 id="提取肺结节"><a href="#提取肺结节" class="headerlink" title="提取肺结节"></a>提取肺结节</h4><p>读取到肺结节的位置信息和良恶性程度后，我们要根据该信息提取肺结节。主要用到2个函数： <code>jianqieimage()</code>和<code>jianqie()</code><br><strong>代码如下：</strong><br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">zl_jianqie</span><span class="params">(img_path,dir,times,size_center,col4xy)</span></span></span><br><span class="line">   <span class="comment">%dir 为患病可能程度，col4xy为剪切区域</span></span><br><span class="line">    train_path = <span class="string">'I:\肺结节\数据\result2\train23jpg\';</span></span><br><span class="line"><span class="string">    result_name = [img_path(24:37),'</span>_',char(num2str(dir)),<span class="string">'_'</span>,char(num2str(times)),img_path(<span class="number">38</span>:<span class="number">46</span>)];</span><br><span class="line">    train_path = [train_path,char(num2str(dir)),<span class="string">'\',result_name];  %剪切路径*</span></span><br><span class="line"><span class="string">    img=imread(img_path);  %读取图片文件*</span></span><br><span class="line"><span class="string">    img1=imcrop(img,col4xy);   %返回图像的一个裁剪区域*  I2=imcrop(I,[a b c d]);%利用裁剪函数裁剪图像，其中，</span></span><br><span class="line"><span class="string">    %（a,b）表示裁剪后左上角像素在原图像中的位置；c表示裁剪后图像的宽，d表示裁剪后图像的高</span></span><br><span class="line"><span class="string"> %% 分割肺结节实质</span></span><br><span class="line"><span class="string">    img1_size = size(img1);</span></span><br><span class="line"><span class="string">    min(min(img1));    %   找到最小值，最大值</span></span><br><span class="line"><span class="string">    max(max(img1));</span></span><br><span class="line"><span class="string">    t=graythresh(img1);    %使用最大类间方差法找到图片的一个合适的阈值threshold</span></span><br><span class="line"><span class="string">    C=im2bw(img1,t);   %转换为二值图像*</span></span><br><span class="line"><span class="string">    D=imfill(C,4,'</span>holes<span class="string">');%对二值化后的图像填充肺实质</span></span><br><span class="line"><span class="string">    if dir &gt;=4    %大概率为肺癌*</span></span><br><span class="line"><span class="string">        FMask=bwareaopen(D,10);  % 除二值图像中面积小于10的对象  </span></span><br><span class="line"><span class="string">        D = FMask;</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">    total = 0;  </span></span><br><span class="line"><span class="string">    for i = 1:img1_size(1)   %行数    </span></span><br><span class="line"><span class="string">        for j = 1:img1_size(2)   %列数</span></span><br><span class="line"><span class="string">            if D(i,j) == 0   %二值图像当值为0时  （黑色）</span></span><br><span class="line"><span class="string">                img1(i,j) = 0;</span></span><br><span class="line"><span class="string">            end</span></span><br><span class="line"><span class="string">            if D(i,j) == 1   %二值图像当值为1时  （白色）</span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">                if ~(i &gt; size_center(1) &amp;&amp; j &gt; size_center(1)&amp;&amp; j &lt; size_center(1) + size_center(3)&amp;&amp; i &lt; size_center(1) + size_center(3))    %不在范围内*？</span></span><br><span class="line"><span class="string">                     img1(i,j) = 0;   %取为黑色*</span></span><br><span class="line"><span class="string">                end</span></span><br><span class="line"><span class="string">            end</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">  %% 保存图片</span></span><br><span class="line"><span class="string">  for m = 1:img1_size(1)</span></span><br><span class="line"><span class="string">        for n = 1:img1_size(2)</span></span><br><span class="line"><span class="string">            if img1(m,n) == 0    %黑色元素点个数*</span></span><br><span class="line"><span class="string">                total = total + 1;</span></span><br><span class="line"><span class="string">            end</span></span><br><span class="line"><span class="string">        end</span></span><br><span class="line"><span class="string">  end</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    if total ~= img1_size(1)*img1_size(2)   %如果不全是黑*</span></span><br><span class="line"><span class="string">            imwrite(img1,train_path);    %存入图片*</span></span><br><span class="line"><span class="string">    end</span></span><br><span class="line"><span class="string">end</span></span><br></pre></td></tr></table></figure></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">clear;</span><br><span class="line">clc;</span><br><span class="line"><span class="comment">%肺实质的图片</span></span><br><span class="line">image_path = <span class="string">'I:\肺结节\数据\result2\jpg2\';</span></span><br><span class="line"><span class="string">%肺结节的位置信息和良恶性程度</span></span><br><span class="line"><span class="string">xls_path = '</span>I:\肺结节\数据\result2\result22.xls';</span><br><span class="line">[txt,xls_text] = xlsread(xls_path);</span><br><span class="line">xls_num = <span class="built_in">size</span>(xls_text);</span><br><span class="line">xls_num(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> m = <span class="number">1</span>:xls_num(<span class="number">1</span>)</span><br><span class="line">     img_name = xls_text(m,<span class="number">1</span>);</span><br><span class="line">     str = img_name&#123;<span class="number">1</span>&#125;;</span><br><span class="line">     img_name = [str,<span class="string">'.jpg'</span>];</span><br><span class="line">     jpg_child_path = [image_path,img_name]</span><br><span class="line">      <span class="keyword">if</span> exist(jpg_child_path,<span class="string">'file'</span>)</span><br><span class="line">          col4x = txt(m,<span class="number">4</span>) - txt(m,<span class="number">2</span>);</span><br><span class="line">          col4y = txt(m,<span class="number">5</span>) - txt(m,<span class="number">3</span>);</span><br><span class="line">          dir = txt(m,<span class="number">6</span>);</span><br><span class="line">          times = txt(m,<span class="number">7</span>);</span><br><span class="line">          size_center = [ ];</span><br><span class="line">          <span class="keyword">if</span> col4x &lt; <span class="number">32</span> &amp;&amp; col4y &lt; <span class="number">32</span></span><br><span class="line">              ma = <span class="number">0.5</span> * (<span class="number">32</span> - max(col4x,col4y));      </span><br><span class="line">              col4xy = [txt(m,<span class="number">2</span>)-ma,txt(m,<span class="number">3</span>)-ma,<span class="number">32</span>,<span class="number">32</span>];  </span><br><span class="line">              size_center =[ma,ma,max(col4x,col4y)];</span><br><span class="line">              zl_jianqie(jpg_child_path,dir,times,size_center,col4xy);</span><br><span class="line">              <span class="keyword">continue</span>;</span><br><span class="line">          <span class="keyword">end</span></span><br><span class="line">          size_center =[<span class="number">0</span>,<span class="number">0</span>,max(col4x,col4y)];</span><br><span class="line">          col4xy = [txt(m,<span class="number">2</span>),txt(m,<span class="number">3</span>),max(col4x,col4y),max(col4x,col4y)];</span><br><span class="line">          zl_jianqie(jpg_child_path,dir,times,size_center,col4xy);</span><br><span class="line">      <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">%     break;</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><strong>通过上面的函数，我们可以将肺结节提取出来，并按照良恶性程度分类存储</strong>。<br><strong>部分示例如下：</strong></p><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/7.png" alt=""></p><p>其中1-5表示肺结节的良恶性程度，5表示恶性可能性最大，1表示恶性可能性最小。<br><strong>注意：</strong>3表示不确定是否为肺结节，良恶性程度也不确定。</p><h3 id="肺结节数据集"><a href="#肺结节数据集" class="headerlink" title="肺结节数据集"></a>肺结节数据集</h3><p>通过文中的方法，我们最后根据肺结节的<strong>良恶性程度（1-5）</strong>得到了5类肺结节。数目如下：</p><ul><li>良恶性程度为1：1254</li><li>良恶性程度为2：1532</li><li>良恶性程度为3：1721</li><li>良恶性程度为4：1226</li><li>良恶性程度为5：1646</li></ul><p>之前介绍过，3表示不确定，所以我们训练时舍弃该数据集。我们只需要判断良恶性，因而我们将良恶性程度为1和2归为<strong>良性</strong><code>（Malignant）</code>，良恶性程度为4和5归为另一类<strong>恶性</strong><code>（benign）</code>。主要采取的网络有<code>（CNN、DNN和SAE）</code>。</p><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p>本文网络结构见论文： Using Deep Learning for Classification of Lung Nodules on Computed Tomography Images</p><ol><li><p>CNN的网络结构和参数<br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/11.png" alt=""><br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/22.png" alt=""></p></li><li><p>DNN的网络结构和参数<br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/33.png" alt=""><br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/44.png" alt=""></p></li><li><p>SAE的网络结构和参数<br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/55.png" alt=""><br><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/66.png" alt=""></p></li></ol><p><strong>最后在验证集上的测试结果如下：</strong></p><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/77.png" alt=""></p><p>通过比较可以发现，CNN的分类效果最好。</p><h3 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h3><p>尽管CNN模型已经取得了不错的分类效果，但还需要提升。考虑到分类数据集还是太少，我们引入了生成式对抗网络（GAN）来扩充我们的肺结节数据集。关于GAN的介绍可以看我之前的一篇博客<a href="https://amberwu.top/2018/07/05/20180705%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN/" target="_blank" rel="noopener">深度学习的常见模型 （GAN）</a>,此处不详细介绍。</p><p>我们通过CGAN网络训练了生成模型，扩充了我们的数据集，生成示例如下：</p><p><img src="/2018/08/15/20180815利用深度学习对医学-CT-图像中的肺结节进行良恶性判断/88.png" alt=""></p><p>然后将生成的数据与原始数据混合，重新训练CNN分类模型，在准确率上果然取得了一定的提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;肺癌是最常见的癌症，目前，CT可用于帮助医生在早期阶段检测肺癌。 在许多情况下，识别肺癌的诊断取决于医生的经验，这可能会忽略一些患者并导致一些问题。 在许多医学影像诊断领域，深度学习已被证明是一种流行且有效的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="项目实战" scheme="http://moyingyao.github.io/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="项目" scheme="http://moyingyao.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow入门学习2</title>
    <link href="http://moyingyao.github.io/2018/08/07/20180807Tensorflow%E5%85%A5%E9%97%A82/"/>
    <id>http://moyingyao.github.io/2018/08/07/20180807Tensorflow入门2/</id>
    <published>2018-08-07T14:16:09.000Z</published>
    <updated>2019-04-12T07:00:31.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://liufan.vip/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-2-23-Tensorflow%E5%85%A5%E9%97%A8/" target="_blank" rel="noopener"><strong>这篇博文</strong></a>主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。</p><p>然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。</p><a id="more"></a><hr><h2 id="深度学习模型"><a href="#深度学习模型" class="headerlink" title="深度学习模型"></a>深度学习模型</h2><p>TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。</p><h3 id="基本设置"><a href="#基本设置" class="headerlink" title="基本设置"></a>基本设置</h3><p>在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。</p><h3 id="加载MNIST数据集"><a href="#加载MNIST数据集" class="headerlink" title="加载MNIST数据集"></a>加载MNIST数据集</h3><p>TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>这里<code>mnist</code>是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。</p><h3 id="开启TensorFlow会话"><a href="#开启TensorFlow会话" class="headerlink" title="开启TensorFlow会话"></a>开启TensorFlow会话</h3><p>TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。</p><p>这里我们使用了一个更为方便的类，<code>InteractiveSession</code>，这能让你在构建代码时更加灵活。<code>InteractiveSession</code>允许你做一些交互操作，通过创建一个计算流图(<a href="https://www.tensorflow.org/versions/r0.7/get_started/basic_usage.html#the-computation-graph" target="_blank" rel="noopener">computation graph</a>)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用<code>InteractiveSession</code>，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。</p><p>下面是如何创建一个<code>InteractiveSession</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h3 id="计算流图-Computation-Graph"><a href="#计算流图-Computation-Graph" class="headerlink" title="计算流图(Computation Graph)"></a>计算流图(Computation Graph)</h3><p>为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了<code>Theano</code>和<code>Torch</code>上。</p><p>所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见<a href="https://www.tensorflow.org/versions/r0.7/get_started/basic_usage.html#the-computation-graph" target="_blank" rel="noopener">这里</a>。</p><h3 id="Softmax-Regression模型"><a href="#Softmax-Regression模型" class="headerlink" title="Softmax Regression模型"></a>Softmax Regression模型</h3><p>见<a href="http://liufan.vip/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-2-23-Tensorflow%E5%85%A5%E9%97%A8/" target="_blank" rel="noopener"><strong>这篇博文</strong></a>。</p><h3 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h3><p>Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。</p><h4 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h4><p>为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的<code>噪声</code>以破坏参数整体的对称性，同时避免梯度为0.由于我们使用<code>ReLU</code>激活函数（<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="noopener"><strong>详细介绍</strong></a>)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure><h4 id="卷积-Convolution-和池化-Pooling"><a href="#卷积-Convolution-和池化-Pooling" class="headerlink" title="卷积(Convolution)和池化(Pooling)"></a>卷积(Convolution)和池化(Pooling)</h4><p>TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure><p>其中，<code>padding=&#39;SAME&#39;</code>表示通过填充0，使得输入和输出的形状一致。</p><h4 id="第一层：卷积层"><a href="#第一层：卷积层" class="headerlink" title="第一层：卷积层"></a>第一层：卷积层</h4><p>第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br></pre></td></tr></table></figure><p>为了使得图片与计算层匹配，我们首先<code>reshape</code>输入图像<code>x</code>为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（<strong>-1就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几</strong>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>然后，使用<code>weight tensor</code>对<code>x_image</code>进行卷积计算，加上<code>bias</code>，再应用到一个<code>ReLU</code>激活函数，最终采用最大池化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br></pre></td></tr></table></figure><h4 id="第二层：卷积层"><a href="#第二层：卷积层" class="headerlink" title="第二层：卷积层"></a>第二层：卷积层</h4><p>为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure><h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个<code>ReLu</code>函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure><p>整个CNN的网络结构如下图：</p><p><img src="/2018/08/07/20180807Tensorflow入门2/2019-2-24-Tensorflow入门2/1.png" alt=""></p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>为了减少过拟合程度，在输出层之前应用<code>dropout</code>技术（即丢弃某些神经元的输出结果）。我们创建一个<code>placeholder</code>来表示一个神经元的输出在<code>dropout</code>时不被丢弃的概率。<code>Dropout</code>能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的<code>tf.nn.dropout</code>操作能够利用<code>mask</code>技术处理各种规模的神经元输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure><h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>最终，我们用一个<code>softmax</code>层，得到类别上的概率分布。（与之前的<code>Softmax Regression</code>模型相同）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br></pre></td></tr></table></figure><h4 id="模型训练和测试"><a href="#模型训练和测试" class="headerlink" title="模型训练和测试"></a>模型训练和测试</h4><p>为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前<code>Softmax Regression</code>模型中的训练、测试过程类似。区别在于：</p><ol><li>用更复杂的<code>ADAM</code>最优化方法代替了之前的梯度下降；</li><li>增了额外的参数<code>keep_prob</code>在<code>feed_dict</code>中，以控制<code>dropout</code>的几率；</li><li>在训练过程中，增加了log输出功能（每100次迭代输出一次）。</li></ol><p>下面是程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure><p>最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的<code>Softmax Regression</code>模型。</p><h4 id="完整代码及运行结果"><a href="#完整代码及运行结果" class="headerlink" title="完整代码及运行结果"></a>完整代码及运行结果</h4><p>利用CNN模型实现手写体识别的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">__author__ = <span class="string">'chapter'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_varible</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">"Download Done!"</span>)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># paras</span></span><br><span class="line">W_conv1 = weight_varible([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv layer-1</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv layer-2</span></span><br><span class="line">W_conv2 = weight_varible([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># full connection</span></span><br><span class="line">W_fc1 = weight_varible([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dropout</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output layer: softmax</span></span><br><span class="line">W_fc2 = weight_varible([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># model training</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.arg_max(y_conv, <span class="number">1</span>), tf.arg_max(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuacy = accuracy.eval(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuacy))</span><br><span class="line">    train_step.run(feed_dict = &#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># accuacy on test</span></span><br><span class="line">print(<span class="string">"test accuracy %g"</span>%(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;)))</span><br></pre></td></tr></table></figure><p>运行结果如下图：</p><p><img src="/2018/08/07/20180807Tensorflow入门2/2019-2-24-Tensorflow入门2/2.png" alt=""></p><hr><p><strong>参考资料</strong></p><p><a href="https://blog.csdn.net/junjun_zhao/article/details/79594791" target="_blank" rel="noopener">Tensorflow 实战 Google 深度学习框架</a><br><a href="https://blog.csdn.net/cqrtxwd/article/details/79028264" target="_blank" rel="noopener">TensorFlow——Mnist手写数字识别实战教程</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://liufan.vip/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-2-23-Tensorflow%E5%85%A5%E9%97%A8/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;这篇博文&lt;/strong&gt;&lt;/a&gt;主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。&lt;/p&gt;
&lt;p&gt;然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://moyingyao.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="Tensorflow" scheme="http://moyingyao.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow入门学习</title>
    <link href="http://moyingyao.github.io/2018/08/04/20180804Tensorflow%E5%85%A5%E9%97%A8/"/>
    <id>http://moyingyao.github.io/2018/08/04/20180804Tensorflow入门/</id>
    <published>2018-08-04T09:49:44.000Z</published>
    <updated>2019-04-12T08:58:33.915Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TensorFlow-简介"><a href="#TensorFlow-简介" class="headerlink" title="TensorFlow 简介"></a>TensorFlow 简介</h2><p>TensorFlow是Google在2015年11月份开源的人工智能框架（<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener"><strong>Github项目地址</strong></a>），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音、图像、水平等多个领域。<br><a id="more"></a><br><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">官网</a>上对TensorFlow的介绍是，一个使用数据流图(Data Flow Graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点之间的边，代表<strong>多维数据</strong>(Tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。</p><h3 id="什么是数据流图-Data-Flow-Graph"><a href="#什么是数据流图-Data-Flow-Graph" class="headerlink" title="什么是数据流图(Data Flow Graph)"></a>什么是数据流图(Data Flow Graph)</h3><p>数据流图是描述<strong>有向图</strong>中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。这些tensors的flow也就是TensorFlow的命名来源。<br>节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。</p><h3 id="TensorFlow的特性"><a href="#TensorFlow的特性" class="headerlink" title="TensorFlow的特性"></a>TensorFlow的特性</h3><h4 id="灵活性"><a href="#灵活性" class="headerlink" title="灵活性"></a>灵活性</h4><p>TensorFlow不是一个严格的神经网络工具包，如果你可以使用数据流图来描述你的计算过程，那么你可以使用TensorFlow做任何事情。你还可以十分方便地根据需要来构建数据流图，使用简单的Python语言来实现高层次的功能。</p><h4 id="可移植性"><a href="#可移植性" class="headerlink" title="可移植性"></a>可移植性</h4><p>TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。</p><h4 id="提高开发效率"><a href="#提高开发效率" class="headerlink" title="提高开发效率"></a>提高开发效率</h4><p>TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。</p><h4 id="支持语言选项"><a href="#支持语言选项" class="headerlink" title="支持语言选项"></a>支持语言选项</h4><p>目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口）</p><h4 id="充分利用硬件资源，最大化计算性能"><a href="#充分利用硬件资源，最大化计算性能" class="headerlink" title="充分利用硬件资源，最大化计算性能"></a>充分利用硬件资源，最大化计算性能</h4><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>你需要理解在TensorFlow中，是如何：</p><ul><li>将计算流程表示成图；</li><li>通过<strong>Sessions</strong>来执行图计算；</li><li>将数据表示为<strong>tensors</strong>；</li><li>使用<strong>Variables</strong>来保持状态信息；</li><li>分别使用<strong>feeds</strong>和<strong>fetches</strong>来填充数据和抓取任意的操作结果；</li></ul><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p>TensorFlow是一种将计算表示为图的编程系统。图中的节点称为<strong>ops</strong>(Operation的简称)。一个<strong>ops</strong>使用0个或以上的<strong>Tensors</strong>，通过执行某些运算，产生0个或以上的Tensors。<strong>一个Tensor是一个多维数组</strong>，例如，你可以将一批图像表示为一个四维的数组<strong>[batch, height, width, channels]</strong>，数组中的数值类型均为浮点数。<br>TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPU和GPU)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor.</p><h3 id="图计算"><a href="#图计算" class="headerlink" title="图计算"></a>图计算</h3><p>TensorFlow程序中图的创建类似于一个<strong>施工阶段</strong>，而在<strong>执行阶段</strong>则利用一个session来执行图中的节点。很常见的情况是，在<strong>施工阶段</strong>创建一个图来表示和训练神经网络，而在<strong>执行阶段</strong>在图中重复执行一系列的训练操作。</p><h4 id="创建图"><a href="#创建图" class="headerlink" title="创建图"></a>创建图</h4><p>在TensorFlow中，<strong>Constant</strong>是一种没有输入的<strong>ops</strong>，但是你可以将它作为其他ops的输入。Python库中的<strong>ops构造器</strong>将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解<a href="https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph" target="_blank" rel="noopener">Graph Class文档</a>。</p><p>见下面的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Constant op that produces a 1x2 matrix.  The op is</span></span><br><span class="line"><span class="comment"># added as a node to the default graph.</span></span><br><span class="line"><span class="comment"># The value returned by the constructor represents the output</span></span><br><span class="line"><span class="comment"># of the Constant op.</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another Constant that produces a 2x1 matrix.</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.</span></span><br><span class="line"><span class="comment"># The returned value, 'product', represents the result of the matrix</span></span><br><span class="line"><span class="comment"># multiplication.</span></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure><p>默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。</p><h4 id="在Session中执行图计算"><a href="#在Session中执行图计算" class="headerlink" title="在Session中执行图计算"></a>在Session中执行图计算</h4><p>见下面的示例代码，更多可了解<a href="https://www.tensorflow.org/versions/r0.7/api_docs/python/client.html#session-management" target="_blank" rel="noopener">Session Class</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Launch the default graph.</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># To run the matmul op we call the session 'run()' method, passing 'product'</span></span><br><span class="line"><span class="comment"># which represents the output of the matmul op.  This indicates to the call</span></span><br><span class="line"><span class="comment"># that we want to get the output of the matmul op back.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># All inputs needed by the op are run automatically by the session.  They</span></span><br><span class="line"><span class="comment"># typically are run in parallel.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The call 'run(product)' thus causes the execution of threes ops in the</span></span><br><span class="line"><span class="comment"># graph: the two constants and matmul.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The output of the op is returned in 'result' as a numpy `ndarray` object.</span></span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment"># ==&gt; [[ 12.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Close the Session when we're done.</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><p>Sessions最后需要关闭，以释放相关的资源；也可以使用with模块，session在with模块中自动会关闭：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  result = sess.run([product])</span><br><span class="line">  print(result)</span><br></pre></td></tr></table></figure><p>TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果想在第二块多余的GPU上执行(GPU默认标记为0)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</span><br><span class="line">    matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line">    matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line">    product = tf.matmul(matrix1, matrix2)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>device中的各个字符串含义如下：</p><ul><li><strong>“/cpu:0”</strong>: 你机器的CPU；</li><li><strong>“/gpu:0”</strong>: 你机器的第一个GPU；</li><li><strong>“/gpu:1”</strong>: 你机器的第二个GPU；</li></ul><p>关于Tensorflow中GPU的使用见<a href="https://www.tensorflow.org/versions/r0.7/how_tos/using_gpu/index.html" target="_blank" rel="noopener">这里</a>。</p><h3 id="交互环境下的使用"><a href="#交互环境下的使用" class="headerlink" title="交互环境下的使用"></a>交互环境下的使用</h3><p>以上的python示例中，使用了<strong>Session</strong>和<strong>Session.run()</strong>来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用<strong>InteractiveSession</strong>类，以及<strong>Tensor.eval()</strong>、<strong>Operation.run()</strong>等方法。例如，在交互的Python环境下执行以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enter an interactive TensorFlow Session.</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">a = tf.constant([<span class="number">3.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize 'x' using the run() method of its initializer op.</span></span><br><span class="line">x.initializer.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add an op to subtract 'a' from 'x'.  Run it and print the result</span></span><br><span class="line">sub = tf.sub(x, a)</span><br><span class="line">print(sub.eval())</span><br><span class="line"><span class="comment"># ==&gt; [-2. -1.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Close the Session when we're done.</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><p>Tensorflow中使用<strong>tensor</strong>数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个<strong>tensor</strong>具有固定的类型、级别和大小，更加深入理解这些概念可参考<a href="https://www.tensorflow.org/versions/r0.7/resources/dims_types.html" target="_blank" rel="noopener">Rank, Shape, and Type</a>。</p><h3 id="变量-Variables"><a href="#变量-Variables" class="headerlink" title="变量(Variables)"></a>变量(Variables)</h3><p>变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Variable, that will be initialized to the scalar value 0.</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an Op to add one to `state`.</span></span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variables must be initialized by running an `init` Op after having</span></span><br><span class="line"><span class="comment"># launched the graph.  We first have to add the `init` Op to the graph.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph and run the ops.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Run the 'init' op</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># Print the initial value of 'state'</span></span><br><span class="line">  print(sess.run(state))</span><br><span class="line">  <span class="comment"># Run the op that updates 'state' and print 'state'.</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">    print(sess.run(state))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>赋值函数assign()和add()函数类似，直到session的r<strong>un()</strong>之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。</p><h3 id="抓取-Fetches"><a href="#抓取-Fetches" class="headerlink" title="抓取(Fetches)"></a>抓取(Fetches)</h3><p>为了抓取ops的输出，需要先执行session的run()函数。然后，通过print函数打印状态信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">input2 = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">input3 = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">intermed = tf.add(input2, input3)</span><br><span class="line">mul = tf.mul(input1, intermed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  result = sess.run([mul, intermed])</span><br><span class="line">  print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span></span><br></pre></td></tr></table></figure><p>所有tensors的输出都是一次性连贯执行的。</p><h3 id="填充-Feeds"><a href="#填充-Feeds" class="headerlink" title="填充(Feeds)"></a>填充(Feeds)</h3><p>TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">output = tf.mul(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure><p>如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考<a href="https://www.tensorflow.org/versions/r0.7/tutorials/mnist/tf/index.html" target="_blank" rel="noopener">MNIST fully-connected feed tutorial (source code)</a>。</p><h2 id="示例：曲线拟合"><a href="#示例：曲线拟合" class="headerlink" title="示例：曲线拟合"></a>示例：曲线拟合</h2><p>下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简化调用库名</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>).astype(<span class="string">"float32"</span>)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值）</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">y = W * x_data + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化均方误差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化TensorFlow参数</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行数据流图（注意在这一步才开始执行计算过程）</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察多次迭代计算时，w和b的拟合值</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(step, sess.run(W), sess.run(b))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最好的情况是w和b分别接近甚至等于0.1和0.3</span></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># 0 [ 0.59550095] [ 0.06236772]</span></span><br><span class="line"><span class="comment"># 20 [ 0.20834503] [ 0.24582449]</span></span><br><span class="line"><span class="comment"># 40 [ 0.12388583] [ 0.28805643]</span></span><br><span class="line"><span class="comment"># 60 [ 0.10526589] [ 0.29736692]</span></span><br><span class="line"><span class="comment"># 80 [ 0.10116094] [ 0.29941952]</span></span><br><span class="line"><span class="comment"># 100 [ 0.10025596] [ 0.29987204]</span></span><br><span class="line"><span class="comment"># 120 [ 0.10005643] [ 0.29997179]</span></span><br><span class="line"><span class="comment"># 140 [ 0.10001245] [ 0.29999378]</span></span><br><span class="line"><span class="comment"># 160 [ 0.10000274] [ 0.29999864]</span></span><br><span class="line"><span class="comment"># 180 [ 0.10000062] [ 0.29999971]</span></span><br><span class="line"><span class="comment"># 200 [ 0.10000014] [ 0.29999995]</span></span><br></pre></td></tr></table></figure><h2 id="MNIST手写体识别任务"><a href="#MNIST手写体识别任务" class="headerlink" title="MNIST手写体识别任务"></a>MNIST手写体识别任务</h2><p>下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。</p><h3 id="MNIST数据集介绍"><a href="#MNIST数据集介绍" class="headerlink" title="MNIST数据集介绍"></a>MNIST数据集介绍</h3><p>MNIST是一个简单的图片数据集（<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">数据集下载地址</a>），包含了大量的数字手写体图片。下面是一些示例图片：</p><p><img src="/2018/08/04/20180804Tensorflow入门/1.png" alt="MNIST"></p><p>MNIST数据集是含标注信息的，以上图片分别代表5，0，4，1。</p><p>由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。</p><p>MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用<strong>xs</strong>和<strong>ys</strong>分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。</p><p>一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下：</p><p><img src="/2018/08/04/20180804Tensorflow入门/2.png" alt=""></p><p>但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。</p><p>所以MNIST的训练数据集可以是一个形状为55000 * 784位的<strong>tensor</strong>，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（tensor中的像素值在0到1之间）。如下图：</p><p><img src="/2018/08/04/20180804Tensorflow入门/3.png" alt=""></p><p>MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示<code>3</code>。所以，mnist.train.labels是一个55000 * 10的二维数组。如下：</p><p><img src="/2018/08/04/20180804Tensorflow入门/4.png" alt=""></p><p>以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。</p><h3 id="Softmax-Regression模型"><a href="#Softmax-Regression模型" class="headerlink" title="Softmax Regression模型"></a>Softmax Regression模型</h3><p>数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9.</p><p>Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。</p><p>Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）：</p><p><strong>Step 1</strong>: add up the evidence of our input being in certain classes;<br><strong>Step 2</strong>: convert that evidence into probabilities.</p><p>翻译为：</p><p>第1步：将我们输入的证据（数据的意思？）加在某些类别中;<br>第2步：将证据（数据的意思？）转换为概率。</p><p>为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）：</p><p><img src="/2018/08/04/20180804Tensorflow入门/5.png" alt=""></p><p>最后，我们在一个图片类别的evidence中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为</p><p><img src="/2018/08/04/20180804Tensorflow入门/111.jpg" alt=""></p><p>其中，Wi  和 bi 分别为类别 i 的权值和偏置， j 是输入图片 x 的像素索引。然后，我们将得到的evidence值通过一个softmax函数转化为概率值 y:</p><p><img src="/2018/08/04/20180804Tensorflow入门/222.jpg" alt=""></p><p>这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下：</p><p><img src="/2018/08/04/20180804Tensorflow入门/333.jpg" alt=""></p><p>具体计算方式如下</p><p><img src="/2018/08/04/20180804Tensorflow入门/444.jpg" alt=""></p><p>这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程：</p><p><img src="/2018/08/04/20180804Tensorflow入门/6.png" alt=""></p><p>如果我们将这个过程公式化，将得到</p><p><img src="/2018/08/04/20180804Tensorflow入门/7.png" alt=""></p><p>实际的计算中，我们通常采用矢量计算的方式，如下</p><p><img src="/2018/08/04/20180804Tensorflow入门/8.png" alt=""></p><p>也可以简化成</p><p><img src="/2018/08/04/20180804Tensorflow入门/555.jpg" alt=""></p><h3 id="Softmax-Regression的程序实现"><a href="#Softmax-Regression的程序实现" class="headerlink" title="Softmax Regression的程序实现"></a>Softmax Regression的程序实现</h3><p>为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。</p><p>为了使用TensorFlow，我们需要引用该库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><p>我们利用一些符号变量来描述交互计算的过程，创建如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br></pre></td></tr></table></figure><p>这里的 x不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。</p><p>我们使用<strong>Variable</strong>(变量)来表示模型中的权值和偏置，这些参数是可变的。如下，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><p>这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。</p><p><strong>实现softmax regression模型仅需要一行代码</strong>，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br></pre></td></tr></table></figure><p>其中，<strong>matmul</strong>函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。</p><h3 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h3><p>在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下：</p><p><img src="/2018/08/04/20180804Tensorflow入门/666.jpg" alt=""></p><p>这里y是所预测的概率分布，而 y’ 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数。</p><p>为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>然后得到交叉熵，即</p><p><img src="/2018/08/04/20180804Tensorflow入门/777.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure><p>注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。</p><p>接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用<a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">后向传播算法</a>来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多<a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#optimizers" target="_blank" rel="noopener">其他的优化算法</a>，仅需要一行代码即可实现调用。</p><p>TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.initialize_all_variables()</span><br></pre></td></tr></table></figure><p>我们可以在一个Session里面运行模型，并且进行初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure><p>接下来，进行模型的训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">  sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure><p>每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。</p><p>以上过程称为<strong>随机梯度下降</strong>，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。</p><h3 id="模型的评价"><a href="#模型的评价" class="headerlink" title="模型的评价"></a>模型的评价</h3><p>怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。</p><p>首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br></pre></td></tr></table></figure><p>最后，我们来获取模型在测试集上的准确率，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><p>Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。</p><h3 id="完整代码及运行结果"><a href="#完整代码及运行结果" class="headerlink" title="完整代码及运行结果"></a>完整代码及运行结果</h3><p>利用Softmax模型实现手写体识别的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">__author__ = <span class="string">'chapter'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">"Download Done!"</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># paras</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss func</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># init</span></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">correct_prediction = tf.equal(tf.arg_max(y, <span class="number">1</span>), tf.arg_max(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Accuarcy on Test-dataset: "</span>, sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><p>运行结果如下图：</p><p><img src="/2018/08/04/20180804Tensorflow入门/888.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TensorFlow-简介&quot;&gt;&lt;a href=&quot;#TensorFlow-简介&quot; class=&quot;headerlink&quot; title=&quot;TensorFlow 简介&quot;&gt;&lt;/a&gt;TensorFlow 简介&lt;/h2&gt;&lt;p&gt;TensorFlow是Google在2015年11月份开源的人工智能框架（&lt;a href=&quot;https://github.com/tensorflow/tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Github项目地址&lt;/strong&gt;&lt;/a&gt;），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音、图像、水平等多个领域。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://moyingyao.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="Tensorflow" scheme="http://moyingyao.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>集群上Docker的简单使用</title>
    <link href="http://moyingyao.github.io/2018/07/27/20180727Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3/"/>
    <id>http://moyingyao.github.io/2018/07/27/20180727Docker集群使用文档/</id>
    <published>2018-07-27T10:27:24.000Z</published>
    <updated>2019-03-15T04:46:58.872Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。<br>由于深度学习众多框架如果同时使用会互相影响，在集群上使用docker可以很好地解决这种情况。<br><a id="more"></a></p></blockquote><h3 id="用户管理-管理员权限"><a href="#用户管理-管理员权限" class="headerlink" title="用户管理 (管理员权限)"></a>用户管理<font color="#FF0000"> (管理员权限)</font></h3><h4 id="添加docker用户组"><a href="#添加docker用户组" class="headerlink" title="添加docker用户组:"></a>添加docker用户组:</h4><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd -g <span class="number">344</span> docker</span><br></pre></td></tr></table></figure><h4 id="添加用户到用户组："><a href="#添加用户到用户组：" class="headerlink" title="添加用户到用户组："></a>添加用户到用户组：</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -<span class="selector-tag">a</span> -G 用户组 用户</span><br></pre></td></tr></table></figure><blockquote><p>即给用户docker的使用权限，放开权限后，需要重启docker服务。</p></blockquote><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">service docker restart</span></span><br></pre></td></tr></table></figure><h4 id="从用户组中删除用户"><a href="#从用户组中删除用户" class="headerlink" title="从用户组中删除用户"></a>从用户组中删除用户</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpasswd -d 用户 用户组</span><br></pre></td></tr></table></figure><h3 id="镜像的基本操作"><a href="#镜像的基本操作" class="headerlink" title="镜像的基本操作"></a>镜像的基本操作</h3><h4 id="列出本地镜像"><a href="#列出本地镜像" class="headerlink" title="列出本地镜像"></a>列出本地镜像</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/1.png" alt="示例图片"></p><blockquote><p>各个选项说明:</p><ul><li>REPOSITORY：表示镜像的仓库源（不唯一）</li><li>TAG：镜像的标签(不唯一，可以自己设定)</li><li>IMAGE ID：镜像ID（唯一）</li><li>CREATED：镜像创建时间</li><li>SIZE：镜像大小<br>同一个镜像ID可以有多个仓库源和标签，如图中红框所示。</li></ul></blockquote><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><blockquote><p>我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：<a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a><br>我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search httpd</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/2.png" alt="示例图片"></p><blockquote><ul><li>NAME:镜像仓库源的名称</li><li>DESCRIPTION:镜像的描述</li><li>OFFICIAL:是否docker官方发布</li></ul></blockquote><h4 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h4><blockquote><p>当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。<br>此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull 镜像名(:标签)</span><br></pre></td></tr></table></figure><h4 id="设置镜像标签"><a href="#设置镜像标签" class="headerlink" title="设置镜像标签"></a>设置镜像标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag 原始镜像名 新镜像名:标签</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/3.png" alt="示例图片"></p><blockquote><p>发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。</p></blockquote><h4 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h4><blockquote><p>当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。</p></blockquote><p><strong>1.若一个镜像有多个标签，我们只想删除已经没用的标签</strong></p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi 仓库源<span class="comment">(liufan)</span>: 镜像标签<span class="comment">(lf)</span></span><br></pre></td></tr></table></figure><blockquote><p>删除前后</p></blockquote><p><img src="/2018/07/27/20180727Docker集群使用文档/4.png" alt="示例图片"><br><img src="/2018/07/27/20180727Docker集群使用文档/5.png" alt="示例图片"></p><blockquote><p>我们发现liufan:lf已经被删除</p></blockquote><p><strong>2.彻底删除镜像</strong></p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi –f 镜像ID（以<span class="number">8</span><span class="keyword">c</span><span class="number">811</span>b<span class="number">4</span>aec<span class="number">35</span>为例）（不建议-f强制删除）</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/6.png" alt="示例图片"></p><blockquote><p>我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签）</p></blockquote><p><strong>3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。</strong><font color="#FF0000">（容器会在下面章节有所讲解）</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name liufan ubuntu/numpy /bin/bash</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/7.png" alt="示例图片"></p><blockquote><p>我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除：</p></blockquote><p><img src="/2018/07/27/20180727Docker集群使用文档/8.png" alt="示例图片"></p><blockquote><p>若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中）</p></blockquote><h4 id="导入导出镜像"><a href="#导入导出镜像" class="headerlink" title="导入导出镜像"></a>导入导出镜像</h4><h5 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar)</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/9.png" alt="示例图片"></p><blockquote><p>已经在对应目录生成压缩文件<br>先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi busybox &amp;&amp; docker images</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/10.png" alt="示例图片"></p><h5 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load &lt; (镜像存储位置)/home/lf/aa.tar</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/11.png" alt="示例图片"></p><blockquote><p>我们发现busybox镜像已经成功导入。</p></blockquote><blockquote><p><strong>注意</strong>：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2种方式：<br>1）    通过Dockerfile文件制作镜像（较难）<br>2）    基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单）</p></blockquote><h3 id="容器基本操作"><a href="#容器基本操作" class="headerlink" title="容器基本操作"></a>容器基本操作</h3><blockquote><p>容器是镜像的一个运行实例，它是基于镜像创建的。</p></blockquote><h4 id="新建容器"><a href="#新建容器" class="headerlink" title="新建容器"></a>新建容器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker create -it --name lf tensorflow</span><br><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/12.png" alt="示例图片"></p><blockquote><p>可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。<br>使用docker create 命令新建的容器一开始是处于停止状态的，需要用如下命令来启动并进入它。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker start lf</span><br><span class="line">docker attach lf</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/13.png" alt="示例图片"></p><h4 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h4><blockquote><p>启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it  --name liufan ubuntu/numpy /bin/bash</span><br></pre></td></tr></table></figure><blockquote><p>上述命令等价于先执行docker create,再执行docker start和docker attach命令。</p></blockquote><p><img src="/2018/07/27/20180727Docker集群使用文档/14.png" alt="示例图片"></p><blockquote><p>上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。<br>命令中：</p><ul><li>-i：表示让容器的标准输入保持打开，</li><li>-t：让docker分配一个伪终端并绑定到容器的标准输入上，</li><li>/bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出）</li></ul></blockquote><blockquote><p>我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。</p></blockquote><h4 id="查看终止删除容器"><a href="#查看终止删除容器" class="headerlink" title="查看终止删除容器"></a>查看终止删除容器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps // 查看所有正在运行容器</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/15.png" alt="示例图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a // 查看所有容器</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/16.png" alt="示例图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a -q // 查看所有容器ID</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/17.png" alt="示例图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop containerId // 停止容器运行，containerId 是容器的ID或者名字，一个或多个</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/18.png" alt="示例图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rm containerId // 删除容器，containerId 是容器的ID或者名字，一个或多个</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/19.png" alt="示例图片"></p><blockquote><p>可以看到lf、wh这两个容器已经被删除</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop $(docker ps -a -q) //  stop停止所有容器</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/20.png" alt="示例图片"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker  rm $(docker ps -a -q) //   删除所有容器</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/21.png" alt="示例图片"></p><blockquote><p><strong>注意：</strong>删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。</p></blockquote><h4 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h4><p><strong>1.attach命令</strong><br><img src="/2018/07/27/20180727Docker集群使用文档/22.png" alt="示例图片"></p><blockquote><p>使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。</p></blockquote><p><strong>2.exec命令</strong></p><blockquote><p>docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash</p></blockquote><p><img src="/2018/07/27/20180727Docker集群使用文档/23.png" alt="示例图片"></p><h4 id="导入和导出容器"><a href="#导入和导出容器" class="headerlink" title="导入和导出容器"></a>导入和导出容器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name liufan ubuntu/numpy /bin/bash</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/24.png" alt="示例图片"></p><blockquote><p>我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它<strong>导出</strong>：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar)</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/25.png" alt="示例图片"><br>我们将liufan这个容器导出本地并压缩命名为aa.tar文件。</p><blockquote><p><strong>导入：</strong><br>先将liufan容器删除在尝试导入</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -a</span><br><span class="line">docker <span class="keyword">import</span> /home/lf/aa.tar test/ubuntu:lf</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/26.png" alt="示例图片"><br><img src="/2018/07/27/20180727Docker集群使用文档/27.png" alt="示例图片"></p><blockquote><p>我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。<br>前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个镜像到本地镜像库。</p></blockquote><p><strong>两者的区别是：</strong></p><blockquote><p>docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。<br>docker load：将保存完整记录，体积较大。</p></blockquote><h3 id="代码实例（以Tensorflow为例）"><a href="#代码实例（以Tensorflow为例）" class="headerlink" title="代码实例（以Tensorflow为例）"></a>代码实例（以Tensorflow为例）</h3><blockquote><p>上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。<br>集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。</p></blockquote><h4 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name liufan bluesliuf/tensorflow /bin/bash</span><br></pre></td></tr></table></figure><p><img src="/2018/07/27/20180727Docker集群使用文档/28.png" alt="示例图片"></p><blockquote><p>我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。</p></blockquote><font color="#FF0000">问题：</font>我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。<br>Docker提供了一种方法：<font color="#FF0000">挂载</font>。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。<br><br><strong>挂载本地目录到容器</strong><br><strong>注意</strong>：不可先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。<br>我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。<br><img src="/2018/07/27/20180727Docker集群使用文档/29.png" alt="示例图片"><br>创建容器有2种方式<br>1.<strong>未调用GPU</strong>，在终端输入命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v /home/lf/lf/catdogs:/var/catdogs --name liufan bluesliuf/tensorflow /bin/bash</span><br></pre></td></tr></table></figure><br><br>2.<strong>调用GPU</strong>，在终端输入命令：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v /home/lf/lf/catdogs:/var/catdogs -v /usr/local/docker-inspur/nvidia-volumes/volume:/usr/local/nvidia:ro --volume-driver=nvidia-docker --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools --device=/dev/nvidia0 --device=/dev/nvidia1 --device=/dev/nvidia2 --device=/dev/nvidia3 --device=/dev/nvidia4 --device=/dev/nvidia5 --name liufan bluesliuf/tensorflow /bin/bash</span><br></pre></td></tr></table></figure><br><br><strong>-v:</strong>挂载的命令参数<br><strong>冒号前：</strong>本地目录的绝对路径<br><strong>冒号后：</strong>容器挂载本地目录的绝对路径<br><strong>调用GPU比不调用GPU多出的部分：</strong>表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU，可以不加此部分<br><strong>name: </strong>创建的容器名<br><strong>bluesliuf/tensorflow:</strong>基于的镜像<br><br><font size="4">不调用GPU（本机）：</font><br><img src="/2018/07/27/20180727Docker集群使用文档/30.png" alt="示例图片"><br><font size="4">调用GPU（集群）：</font><br><img src="/2018/07/27/20180727Docker集群使用文档/31.png" alt="示例图片"><br>可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。<br><font size="4">运行代码（本机）：</font><br><font color="#FF0000">注:</font>本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。<br><img src="/2018/07/27/20180727Docker集群使用文档/32.png" alt="示例图片"><br>再去容器内部看，本地的修改已经同步到容器内了。<br><img src="/2018/07/27/20180727Docker集群使用文档/33.png" alt="示例图片"><br>在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。<br>在终端输入命令：<font color="#FF0000">python 代码文件名（此处我是training.py）</font><p><font size="4">不调用GPU（本机）：</font><br><img src="/2018/07/27/20180727Docker集群使用文档/34.png" alt="示例图片"><br><img src="/2018/07/27/20180727Docker集群使用文档/35.png" alt="示例图片"></p><p>可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）<br>此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按<font color="#FF0000">【ctrl+p】【ctrl+q】</font>，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如：<br><img src="/2018/07/27/20180727Docker集群使用文档/36.png" alt="示例图片"></p><p><font size="4">调用GPU：</font><br><img src="/2018/07/27/20180727Docker集群使用文档/37.png" alt="示例图片"><br>在后台运行和上面一样，也是利用<font color="#FF0000">【ctrl+p】【ctrl+q】</font>。</p><h4 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h4><p>Docker针对挂载目录还提供了一种高级的用法。叫数据卷。</p><p><font color="#FF0000">数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。</font>感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下：<br>1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash</span><br></pre></td></tr></table></figure></p><p>2.再创建一个新的容器，来使用这个数据卷。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --volumes-<span class="keyword">from</span> wuhao --name lf bluesliuf/tensorflow /bin/bash</span><br></pre></td></tr></table></figure></p><p><font color="#FF0000">–volumes-from</font>用来指定要从哪个数据卷来挂载数据。<br><img src="/2018/07/27/20180727Docker集群使用文档/38.png" alt="示例图片"><br>我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。</p><p><font color="#FF0000"><strong>通过数据卷挂载目录更具有优势。</strong></font><br>1）    我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。<br>2）    当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可<font color="#FF0000">（类似一个全局变量）</font>，而无须一个个修改其他容器的本地挂载目录。</p><p>挂载成功后。运行代码步骤与上面一样。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。&lt;br&gt;由于深度学习众多框架如果同时使用会互相影响，在集群上使用docker可以很好地解决这种情况。&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Docker" scheme="http://moyingyao.github.io/categories/Docker/"/>
    
    
      <category term="集群" scheme="http://moyingyao.github.io/tags/%E9%9B%86%E7%BE%A4/"/>
    
      <category term="Docker" scheme="http://moyingyao.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>深度学习的常见模型-GAN</title>
    <link href="http://moyingyao.github.io/2018/07/05/20180705%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN/"/>
    <id>http://moyingyao.github.io/2018/07/05/20180705深度学习的常见模型-GAN/</id>
    <published>2018-07-05T14:27:24.000Z</published>
    <updated>2019-03-14T11:39:02.051Z</updated>
    
    <content type="html"><![CDATA[<h3 id="GAN的来源"><a href="#GAN的来源" class="headerlink" title="GAN的来源"></a>GAN的来源</h3><blockquote><p>2014年Goodfellow提出Generative Adversarial Nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。</p></blockquote><a id="more"></a><blockquote><p>GANs简单的想法就是用两个模型，<font color="#FF0000"> 一个生成模型，一个判别模型。</font>判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，<font color="#FF0000">这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假</font>，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。</p></blockquote><h3 id="GAN的基本组成"><a href="#GAN的基本组成" class="headerlink" title="GAN的基本组成"></a>GAN的基本组成</h3><blockquote><p>GAN 模型中的两位博弈方分别由<font color="#FF0000">生成式模型</font>（Generative Model）和<font color="#FF0000">判别式模型</font>（Discriminative Model）充当。</p></blockquote><blockquote><blockquote><p>生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好；</p></blockquote></blockquote><blockquote><blockquote><p>判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。</p></blockquote></blockquote><blockquote><blockquote><p>可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。</p></blockquote></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/1.png" alt="示例图片"></p><blockquote><p>上图是GAN网络的流程图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能让判别器输出值为1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，使得判别器输出假数据的值为1是生成器的目标，而对于这些假数据，判别器要尽可能输出0。<br>GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示：</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/111.png" alt="示例图片"></p><blockquote><p>其本质上是两个优化问题，把拆解就如同下面两个公式，上面是优化D的，下面是优化G的。</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/222.png" alt="示例图片"></p><blockquote><p>当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。</p></blockquote><h3 id="GAN的训练过程"><a href="#GAN的训练过程" class="headerlink" title="GAN的训练过程"></a>GAN的训练过程</h3><blockquote><p>下图为GAN的训练过程。<br>生成式对抗网络主要由生成器G和判别器D组成，训练过程如下所述：</p><ul><li>输入噪声（隐藏变量）Z</li><li>通过生成器G得到x_fake=G(z)</li><li>从真实数据集中获取一部分真实数据x_real</li><li>将两者混合x=x_fake+x_real</li><li>将数据喂入判别部分D，给定标签x_fake=0,x_real=1,这一过程就是简单的二分类</li><li>按照分类结果，回传loss</li></ul></blockquote><blockquote><p>在整个过程中，D要尽可能的使D(G(z))=0,D(x_real)=1（火眼金睛，不错杀也不漏杀）。而G则要使得D(G(z))=1(即让生成的图片以假乱真)</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/2.png" alt="示例图片"></p><p>GAN的算法流程和动态求解过程如下图所示：</p><blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/3.png" alt="示例图片"><br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/4.png" alt="示例图片"><br>一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。</p></blockquote><h3 id="GAN的网络结构"><a href="#GAN的网络结构" class="headerlink" title="GAN的网络结构"></a>GAN的网络结构</h3><h4 id="判别器-卷积"><a href="#判别器-卷积" class="headerlink" title="判别器(卷积)"></a><font color="#FF0000">判别器(卷积)</font></h4><blockquote><p>卷积层大家应该都很熟悉了,为了方便说明，定义如下：</p><ul><li>二维的离散卷积（N=2）</li><li>方形的特征输入（i<sub>1</sub>=i<sub>2</sub>=i）</li><li>方形的卷积核尺寸（k<sub>1</sub>=k<sub>2</sub>=k ）</li><li>每个维度相同的步长（s<sub>1</sub>=s<sub>2</sub>=s）</li><li>每个维度相同的padding (p<sub>1</sub>=p<sub>2</sub>=p)</li></ul></blockquote><blockquote><p>下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o<sub>1</sub>=o<sub>2</sub>=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o<sub>1</sub>=o<sub>2</sub>=o=3)。</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/5.png" alt="示例图片"></p><blockquote><p>从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为：</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/333.png" alt="示例图片"></p><h4 id="生成器-反卷积"><a href="#生成器-反卷积" class="headerlink" title="生成器(反卷积)"></a><font color="#FF0000">生成器(反卷积)</font></h4><blockquote><p>在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。对于上述卷积运算，我们把上图所示的3x3卷积核展开成一个如下图所示的[4.16]的稀疏矩阵C，其中非0元素W<sub>i,j</sub>表示卷积核的第i行和第j列。</p></blockquote><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/6.png" alt="示例图片"></p><blockquote><p>我们再把4x4的输入特征展开成[16,1]的矩阵X，那么Y=CX则是一个[4,1]的输出特征矩阵，把它重新排列成2x2的输出特征就得到最终的结果，从上述分析可以看出卷积层的计算其实是可以转化成矩阵相乘的。值得注意的是，在一些深度学习网络的开源框架中，并不是通过这种转换方法来计算卷积的，因为这个转换会存在很多无用的0乘操作，caffe中具体实现卷积计算的方法可以参考impleming convolution as a matrix multiplication。</p></blockquote><blockquote><p>通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么<font color="#FF0000">我们很容易得到卷积层的反向传播就是和C的转置相乘。</font></p></blockquote><p><strong>反卷积和卷积的关系如下</strong></p><blockquote><p>反卷积又称transposed（转置） convolution，我们可以看出其实卷积层的前向传播过程就是反卷积层的反向传播过程，卷积层的反向传播过程就是反卷积层的前向传播过程。因为卷积层的前向反向计算分别为乘C和C<sup>T</sup>,而反卷积层的前向反向计算分别为乘C<sup>T</sup>和(C<sup>T</sup>)<sup>T</sup>,所以他们的前向传播和反向传播刚好交换过来。<br>同样为了说明，定义反卷积操作参数如下：</p><ul><li>二维的离散卷积（N=2）</li><li>方形的特征输入（i<sub>1</sub><sup>‘</sup>=i<sub>2</sub><sup>‘</sup>=i<sup>‘</sup>）</li><li>方形的卷积核尺寸（k<sub>1</sub><sup>‘</sup>=k<sub>2</sub><sup>‘</sup>=k<sup>‘</sup>）</li><li>每个维度相同的步长（s<sub>1</sub><sup>‘</sup>=s<sub>2</sub><sup>‘</sup>=s<sup>‘</sup>）</li><li>每个维度相同的padding (p<sub>1</sub><sup>‘</sup>=p<sub>2</sub><sup>‘</sup>=p<sup>‘</sup>)<br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/7.png" alt="示例图片"><br>上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为： <strong>o′=i′-k′+2p′+1=i′+(k-1)-2p</strong></li></ul></blockquote><p><strong>GAN的优点</strong></p><blockquote><ul><li>GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 </li><li>相比其他所有模型, GAN可以产生更加清晰，真实的样本</li><li>GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</li></ul></blockquote><p><strong>GAN的缺点</strong></p><blockquote><ul><li>训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的</li><li>GAN不适合处理离散形式的数据，比如文本</li><li>GAN存在训练不稳定、梯度消失、模式崩溃的问题</li></ul></blockquote><h3 id="实例DCGAN网络"><a href="#实例DCGAN网络" class="headerlink" title="实例DCGAN网络"></a>实例DCGAN网络</h3><p><strong>网络结构 (判别器)</strong><br> <img src="/2018/07/05/20180705深度学习的常见模型-GAN/8.png" alt="示例图片"> </p><p><strong>网络结构 (生成器)</strong><br> <img src="/2018/07/05/20180705深度学习的常见模型-GAN/9.png" alt="示例图片"> </p><p><strong>二次元动漫人脸（共50个epoch）</strong><br>数据集：51223张动漫人脸，图左为原始数据集，图右为训练过程<br> <img src="/2018/07/05/20180705深度学习的常见模型-GAN/10.png" alt="示例图片"> </p><p>训练过程生成效果图如下：<br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/11.png" alt="示例图片"><br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/12.png" alt="示例图片"></p><p><strong>真实人脸（共100个epoch）</strong><br>数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下：<br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/13.png" alt="示例图片"> </p><p>训练过程生成效果图如下</p><p><img src="/2018/07/05/20180705深度学习的常见模型-GAN/14.png" alt="示例图片"><br><img src="/2018/07/05/20180705深度学习的常见模型-GAN/15.png" alt="示例图片"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;GAN的来源&quot;&gt;&lt;a href=&quot;#GAN的来源&quot; class=&quot;headerlink&quot; title=&quot;GAN的来源&quot;&gt;&lt;/a&gt;GAN的来源&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;2014年Goodfellow提出Generative Adversarial Nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://moyingyao.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="GAN" scheme="http://moyingyao.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>深度学习的发展</title>
    <link href="http://moyingyao.github.io/2018/06/28/20180628%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95/"/>
    <id>http://moyingyao.github.io/2018/06/28/20180628深度学习发展/</id>
    <published>2018-06-28T14:27:24.000Z</published>
    <updated>2019-03-14T10:25:04.598Z</updated>
    
    <content type="html"><![CDATA[<h2 id="深度学习的发展历程"><a href="#深度学习的发展历程" class="headerlink" title="深度学习的发展历程"></a>深度学习的发展历程</h2><ul><li>人工智能（爷爷）</li><li>机器学习（爸爸）</li><li>深度学习（儿子）</li></ul><a id="more"></a><h3 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h3><blockquote><p>远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。<br>一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。</p></blockquote><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><blockquote><p>机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。<br>机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 </p></blockquote><p><img src="/2018/06/28/20180628深度学习发展/machine.jpg" alt=""></p><blockquote><p>上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。</p></blockquote><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><blockquote><p>深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。<br>其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素：</p><ul><li>大数据，用于训练数据的增加；</li><li>计算机的算力大大增加，更快的CPU、通用GPU 的出现</li></ul></blockquote><p><img src="/2018/06/28/20180628深度学习发展/deep.jpg" alt=""></p><blockquote><p>上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。<br>输入层输入图片，即像素矩阵；<br>对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；<br>最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。</p></blockquote><h2 id="深度学习常见的编程框架"><a href="#深度学习常见的编程框架" class="headerlink" title="深度学习常见的编程框架"></a>深度学习常见的编程框架</h2><p><img src="/2018/06/28/20180628深度学习发展/d1.png" alt=""></p><blockquote><p>  观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。<br>  另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。</p></blockquote><h2 id="深度学习的应用"><a href="#深度学习的应用" class="headerlink" title="深度学习的应用"></a>深度学习的应用</h2><h3 id="无人驾驶"><a href="#无人驾驶" class="headerlink" title="无人驾驶"></a>无人驾驶</h3><blockquote><p>  深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。<br>  在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。</p></blockquote><p><img src="/2018/06/28/20180628深度学习发展/d2.jpg" alt=""></p><blockquote><p>从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程就是用深度学习完成。</p></blockquote><h3 id="AlphaGo阿尔法狗"><a href="#AlphaGo阿尔法狗" class="headerlink" title="AlphaGo阿尔法狗"></a>AlphaGo阿尔法狗</h3><blockquote><p>阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。<br>早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。</p></blockquote><h4 id="AlphaGo主要包括三个组成部分："><a href="#AlphaGo主要包括三个组成部分：" class="headerlink" title="AlphaGo主要包括三个组成部分："></a>AlphaGo主要包括三个组成部分：</h4><blockquote><ul><li>蒙特卡洛搜索树（MonteCarlo tree search，MCTS）</li><li>估值网络（Value network）</li><li>策略网络（Policy notebook）</li></ul></blockquote><blockquote><p>AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。<br>AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。<br>AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。</p></blockquote><h3 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h3><blockquote><p>人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。<br>人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.将3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。</p></blockquote><p><img src="/2018/06/28/20180628深度学习发展/d2.png" alt=""><br><img src="/2018/06/28/20180628深度学习发展/d3.png" alt=""></p><h2 id="学习深度学习所需的基础知识"><a href="#学习深度学习所需的基础知识" class="headerlink" title="学习深度学习所需的基础知识"></a>学习深度学习所需的基础知识</h2><blockquote><ul><li>高数（链式求导，偏导，微积分）</li><li>线代（各种矩阵变换、线性方程）</li><li>概率论（各种统计分布函数，贝叶斯，傅里叶变换）</li><li>信息论（熵，相对熵，最大熵模型）</li><li>数理统计和参数估计（中心极值定理，矩阵计算，最大似然估计）</li><li>机器学习算法（KNN,决策树，SVM）</li><li>编程语言（最好是python）</li></ul></blockquote><p><img src="/2018/06/28/20180628深度学习发展/d5.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;深度学习的发展历程&quot;&gt;&lt;a href=&quot;#深度学习的发展历程&quot; class=&quot;headerlink&quot; title=&quot;深度学习的发展历程&quot;&gt;&lt;/a&gt;深度学习的发展历程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;人工智能（爷爷）&lt;/li&gt;
&lt;li&gt;机器学习（爸爸）&lt;/li&gt;
&lt;li&gt;深度学习（儿子）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://moyingyao.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ML" scheme="http://moyingyao.github.io/tags/ML/"/>
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
      <category term="AI" scheme="http://moyingyao.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>深度学习常见模型-CNN</title>
    <link href="http://moyingyao.github.io/2018/06/21/20180621%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN/"/>
    <id>http://moyingyao.github.io/2018/06/21/20180621深度学习的常见模型-CNN/</id>
    <published>2018-06-21T14:27:24.000Z</published>
    <updated>2019-03-14T10:09:18.355Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CNN的来源"><a href="#CNN的来源" class="headerlink" title="CNN的来源"></a>CNN的来源</h3><blockquote><p>CNN由纽约大学的Yann LeCun于1989年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。</p></blockquote><a id="more"></a><blockquote><p>一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。</p></blockquote><blockquote><p>权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。</p></blockquote><h3 id="CNN的网络架构"><a href="#CNN的网络架构" class="headerlink" title="CNN的网络架构"></a>CNN的网络架构</h3><blockquote><p>卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。</p></blockquote><h4 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h4><blockquote><p>这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”名字来源。<br>在卷积层中，有两个关键操作：局部关联和窗口滑动。局部关联将每个神经元看作一个滤波器（filter），窗口滑动则使filter对局部数据进行计算。<br>除了这两个操作外，还有两个名词：步长和填充值。步长（stride）为窗口一次滑动的长度，而填充值请看下图的例子。比如有一个5x5像素大小的图片，步长取2，那么则有一个像素没有办法获取到，那应该怎么办呢？</p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv.png" alt="示例图片"></p><blockquote><p>再举一个例子，图片中输入的是一个3x4的矩阵，卷积核是一个2x2的矩阵。我们假设卷积是一次移动一个像素来操作的，那么我们首先对左上角2x2局部矩阵与卷积核进行卷积操作，即各个位置的元素相乘再相加，得到的输出矩阵S的S<sub>00</sub>的元素值为aw+bx+ey+fz。然后我们将卷积核向右平移一个像素，现在是（b,c,f,g）四个元素构成的矩阵和卷积核来卷进，得到了输出矩阵S的S<sub>01</sub>的元素，以此类推，可以得到矩阵S的S<sub>02</sub>，S<sub>10</sub>，S<sub>11</sub>，S<sub>12</sub>的元素，具体过程如下图所示。</p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv2.png" alt="示例图片"></p><blockquote><p>再举一个卷积过程的例子如下：我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3*3矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。 </p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv3.png" alt="示例图片"></p><blockquote><p>上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？</p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv5.png" alt="示例图片"></p><blockquote><p>这里实际输入的是3个5x5的矩阵，在原来输入的周围加上值为0的一层padding，则输入变为如图所示的7x7的矩阵。例子里面使用了两个卷积核，我们先关注与卷积核W<sub>0</sub>。由于输入的是3个7x7的矩阵，也可以说成7x7x3的张量，所以我们对应的卷积核W<sub>0</sub>的最后一个参数也必须是3的张量，这里卷积核W<sub>0</sub>的单独子矩阵维度为3x3.那么卷积核W<sub>0</sub>实际为一个3x3x3的张量。同时和上面的例子不同的是，这里的步长为2，即每次卷积后卷积核会向后移动2个像素的位置。<br><strong>蓝色矩阵（输入图像）</strong> 对 <strong>粉丝矩阵（filter）</strong> 进行矩阵内积计算并将三个内积运算的结果与偏移量b像加，比如上图中，3+0+0+0=3，计算后的值，即<strong>绿色矩阵</strong> 中的一个元素。</p></blockquote><h4 id="池化层（Pooling）"><a href="#池化层（Pooling）" class="headerlink" title="池化层（Pooling）"></a>池化层（Pooling）</h4><blockquote><p>池化层，又称为降采样层，使用的原因为：根据图像局部相关性的原理，对图像进行子采样可能减少计算量，同时保持图像的旋转不变性。相比卷积层的复杂，池化层简单的多，所谓的池化，个人理解就是对输入张量的各个子矩阵进行压缩。假如是2x2的池化滤波，那么就将子矩阵的每个2x2个元素变为一个元素；如果为3x3的池化滤波，就将子矩阵每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。<br>如果想将矩阵中每NxN个元素变成一个元素，则需要一个共同的池化标准。常见的池化标准有2个：MAX和Average。即取对应区域的最大值或者平均值作为池化后的元素值。下图的例子中采用的是最大池化方法，2x2的池化滤波，步长为2.首先对红色2x2区域进行池化，此区域中最大值对6，则对应池化输出的值为6。然后滤波进行移动，由于步长为2，则移动至图中绿色区域，输出最大值为8，以此类推，最终，输入的4x4的矩阵经过池化过程后，变为2x2的矩阵，得到了压缩。</p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv7.png" alt="示例图片"></p><h4 id="全连接层（Full-Connecting）"><a href="#全连接层（Full-Connecting）" class="headerlink" title="全连接层（Full Connecting）"></a>全连接层（Full Connecting）</h4><blockquote><p>每层之间的神经元都有权重连接，通常全连接层在卷积神经网络的尾部，是同传统神经网络神经元的连接方式是一样的。全连接层和卷积层比较相似，但全连接层的输出是一个Nx1大小的向量，并通过几个全连接层对向量进行将为操作，一般采用softmax全连接。</p></blockquote><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv8.png" alt="示例图片"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv9.png" alt="示例图片"></p><h4 id="一般CNN的结构依次为"><a href="#一般CNN的结构依次为" class="headerlink" title="一般CNN的结构依次为"></a>一般CNN的结构依次为</h4><blockquote><ol><li><strong>input</strong></li><li>((<strong>conv</strong> –&gt; <strong>relu</strong>) x <strong>N</strong>–&gt;pool?) x <strong>M</strong></li><li>(<strong>fc</strong> –&gt; <strong>relu</strong>) x <strong>K</strong></li><li><strong>fc</strong></li></ol></blockquote><h4 id="卷积神经网络的训练算法"><a href="#卷积神经网络的训练算法" class="headerlink" title="卷积神经网络的训练算法"></a>卷积神经网络的训练算法</h4><blockquote><ul><li>与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距；</li><li>找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。</li></ul></blockquote><h4 id="卷积神经网络的优缺点"><a href="#卷积神经网络的优缺点" class="headerlink" title="卷积神经网络的优缺点"></a>卷积神经网络的优缺点</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><blockquote><ul><li>共享卷积核，便于处理高维数据；</li><li>不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。</li></ul></blockquote><h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><blockquote><ul><li>需要大量训练样本和好的硬件支持（GPU、TPU…）;</li></ul></blockquote><blockquote><ul><li>物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。</li></ul></blockquote><h4 id="卷积神经网络的典型结构"><a href="#卷积神经网络的典型结构" class="headerlink" title="卷积神经网络的典型结构"></a>卷积神经网络的典型结构</h4><blockquote><ul><li>LeNet,最早用于手写体数字识别的卷积神经网络。</li><li>AlexNet，2012年ILSVRC比赛中获得第一名，远超过第二名，比LeNet更深，用多层小卷积层进行叠加替换大卷积层。</li><li>ZFNet，2013年ILSVRC比赛冠军</li><li>GoogleNet，2014年ILSVRC比赛冠军</li><li>VGGNet，2014年ILSVRC比赛中的模型，图像识别上略差于GoogleNet，但是在很多图像转化学习问题（比如object detection）上效果很好。</li></ul></blockquote><h3 id="实战演练"><a href="#实战演练" class="headerlink" title="实战演练"></a>实战演练</h3><p><strong>猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型</strong></p><blockquote><p>训练集（共25000张图片，猫狗各12500张）<br>测试集（共3000张图片，猫狗各1500张） </p></blockquote><p>我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。<br>模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。</p><p><img src="/2018/06/21/20180621深度学习的常见模型-CNN/conv11.png" alt="示例图片"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;CNN的来源&quot;&gt;&lt;a href=&quot;#CNN的来源&quot; class=&quot;headerlink&quot; title=&quot;CNN的来源&quot;&gt;&lt;/a&gt;CNN的来源&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;CNN由纽约大学的Yann LeCun于1989年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://moyingyao.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CNN" scheme="http://moyingyao.github.io/tags/CNN/"/>
    
      <category term="DL" scheme="http://moyingyao.github.io/tags/DL/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper</title>
    <link href="http://moyingyao.github.io/2018/06/08/20180608zookeeper/"/>
    <id>http://moyingyao.github.io/2018/06/08/20180608zookeeper/</id>
    <published>2018-06-08T13:54:25.000Z</published>
    <updated>2019-03-14T09:04:54.791Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。zookeeper主要是写分布式程序，可总结为：一致、有头、数据数。<br><a id="more"></a></p></blockquote><h1 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h1><blockquote><p>使用zookeeper开发自己的分布式系统要注意的问题：</p><ol><li>解决数据一致性的问题</li><li>协调各种“动物”<br>hadoop  小象<br>impala  黑斑羚<br>shark   鲨鱼<br>hive    蜂巢<br>mahout  象夫<br>zookeeper 动物园管理员</li></ol></blockquote><h1 id="google三论文"><a href="#google三论文" class="headerlink" title="google三论文"></a>google三论文</h1><blockquote><p>GFS → HDFS<br>BigTable → HBase<br>MapReduce → HadoopMR<br>chubby → zookeeper</p></blockquote><h1 id="zookeeper是什么？"><a href="#zookeeper是什么？" class="headerlink" title="zookeeper是什么？"></a>zookeeper是什么？</h1><blockquote><p>NoSQL数据库<br>CAP原理</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。zookeeper主要是写分布式程序，可总结为：一致、有头、数据数。&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://moyingyao.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://moyingyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="zookeeper" scheme="http://moyingyao.github.io/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>memcached安装及简单命令</title>
    <link href="http://moyingyao.github.io/2018/06/05/20180605memcached%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/"/>
    <id>http://moyingyao.github.io/2018/06/05/20180605memcached安装及简单命令/</id>
    <published>2018-06-05T12:09:14.000Z</published>
    <updated>2019-03-14T09:04:26.791Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>memcached是一套分布式的高速缓存系统，由LiveJournal的Brad Fitzpatrick开发，但目前被许多网站使用。</p></blockquote><h1 id="ubuntu16-0-4下memcached的安装"><a href="#ubuntu16-0-4下memcached的安装" class="headerlink" title="ubuntu16.0.4下memcached的安装"></a>ubuntu16.0.4下memcached的安装</h1><a id="more"></a><blockquote><p>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install libevent-deve</span><br><span class="line">sudo apt-<span class="builtin-name">get</span> install memcached</span><br></pre></td></tr></table></figure><blockquote><p>若linux系统为centos则命令为<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> libevent libevent-deve</span><br><span class="line">yum <span class="keyword">install</span> memcached</span><br></pre></td></tr></table></figure><h1 id="memcached的连接与关闭"><a href="#memcached的连接与关闭" class="headerlink" title="memcached的连接与关闭"></a>memcached的连接与关闭</h1><h2 id="启动memcached连接"><a href="#启动memcached连接" class="headerlink" title="启动memcached连接"></a>启动memcached连接</h2><blockquote><p>找到memcached的安装目录，自动安装memcached在/usr/local/bin/memcached路径下<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">memcached -u root -d -m <span class="number">128</span>m -p <span class="number">11211</span></span><br></pre></td></tr></table></figure><blockquote><p>连接memcached语法为：telnet HOST PORT<br>本实例的memcached服务运行的主机为127.0.0.1(本机)，端口为11211<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">11211</span></span><br></pre></td></tr></table></figure><blockquote><p>连接成功如下图所示：</p></blockquote><p><img src="https://i.imgur.com/jLWKbvn.png" alt=""><br><strong>退出命令</strong></p><blockquote><p>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">quit</span></span><br></pre></td></tr></table></figure><h2 id="关闭memcached"><a href="#关闭memcached" class="headerlink" title="关闭memcached"></a>关闭memcached</h2><blockquote><p>与windows直接输入memcached.exe -d stop关闭memcached不同<br>linux需先知道memcached的进程号，再将其杀死<br>查看进程号<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stats     或者     <span class="keyword">ps</span> -ef|<span class="keyword">grep</span> memcached</span><br></pre></td></tr></table></figure><blockquote><p>知道了memcached对应的进程号pid后，使用kill命令杀死进程即可。<br>注意：杀死进程前必须quit退出连接<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> 5070</span><br></pre></td></tr></table></figure><blockquote><p>杀死进城后再次连接memcached失败，说明memcached已经被关闭。</p></blockquote><h1 id="memcached的命令"><a href="#memcached的命令" class="headerlink" title="memcached的命令"></a>memcached的命令</h1><h2 id="存储命令"><a href="#存储命令" class="headerlink" title="存储命令"></a>存储命令</h2><h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><blockquote><p>set用于将value存储于key中，若set的key已经存在，该命令可以更新key所对应的原来的数据。语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">key</span> flag exptime <span class="keyword">bytes</span> [noreply]</span><br><span class="line"><span class="keyword">value</span></span><br></pre></td></tr></table></figure><blockquote><p>key：键值对中的key，用于查找缓存值<br>flag：可以包含键值对的整型参数，客户机使用它存储关于键值对的额外信息<br>exptime：再缓存中保存键值对的时间长度，以秒为单位，0表示永远<br>bytes：在缓存中存储的字节数<br>noreply：可选参数，该参数告知服务器不需要返回数据<br>value：存储的值，始终位于第二行</p></blockquote><h3 id="add"><a href="#add" class="headerlink" title="add"></a>add</h3><blockquote><p>add用于将value存储在指定的key中，如果add的key已经存在，则不会更新数据，与之前的值仍然保持相同，会得到NOT_STORED的响应，但是过期的key会更新。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add</span><span class="bash"> key flags exptime bytes [noreply]</span></span><br><span class="line"><span class="bash">value</span></span><br></pre></td></tr></table></figure><h3 id="replace"><a href="#replace" class="headerlink" title="replace"></a>replace</h3><blockquote><p>replace用于替换已经存在的key的value，如果可以不存在，则替换失败，并且得到NOT_STORED的响应<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">replace</span> <span class="keyword">key</span> flags exptime <span class="keyword">bytes</span> [noreply]</span><br><span class="line"><span class="keyword">value</span></span><br></pre></td></tr></table></figure><h3 id="append"><a href="#append" class="headerlink" title="append"></a>append</h3><blockquote><p>append用于向已经存在key的value后面追加数据<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">append key flags exptime <span class="keyword">bytes</span> [noreply]</span><br><span class="line"><span class="built_in">value</span></span><br></pre></td></tr></table></figure><h3 id="prepend"><a href="#prepend" class="headerlink" title="prepend"></a>prepend</h3><blockquote><p>prepend命令用于向已经存在key的value前面追加数据<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prepend key flags exptime <span class="keyword">bytes</span> [noreply]</span><br><span class="line"><span class="built_in">value</span></span><br></pre></td></tr></table></figure><h3 id="cas"><a href="#cas" class="headerlink" title="cas"></a>cas</h3><blockquote><p>cas用于执行一个“检查并设置”的操作，它仅在当前客户端最后一次取值后，该key对应的值没有被其他客户端修改的情况下才能够将值写入。检查是通过cas_token参数进行的，这个参数是memcached指定给已经存在的元素的一个唯一的64位值。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cas key flags exptime <span class="keyword">bytes</span> unique_cas_token [noreply]</span><br><span class="line"><span class="built_in">value</span></span><br></pre></td></tr></table></figure><blockquote><p>unique_cas_token是通过gets命令获取的一个唯一的64位值</p></blockquote><h2 id="查找命令"><a href="#查找命令" class="headerlink" title="查找命令"></a>查找命令</h2><h3 id="get"><a href="#get" class="headerlink" title="get"></a>get</h3><blockquote><p>get用于获取存储在key中的value，如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">get</span> key</span><br><span class="line"><span class="builtin-name">get</span> key1 key2 key3</span><br></pre></td></tr></table></figure><h3 id="gets"><a href="#gets" class="headerlink" title="gets"></a>gets</h3><blockquote><p>gets用于获取CAS令牌存的value,如果key不存在，则返回空。若获取多个key的value，则使用空格将其隔开即可。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">gets</span> key</span><br><span class="line"><span class="keyword">gets</span> key1 key2 key3</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><blockquote><p>delete命令用于删除已经存在的key。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="built_in">key</span> [noreply]</span><br></pre></td></tr></table></figure><h3 id="incr-decr"><a href="#incr-decr" class="headerlink" title="incr/decr"></a>incr/decr</h3><blockquote><p>incr和decr用于对已经存在的key的数字进行自增或自减操作。但是惭怍的数据必须是十进制的32位无符号整数，若key不存在，返回NOT_FOUND，若键的值不为数字，则返回CLIENT_ERROR，其他错误返回ERROR。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">incr key increment_values <span class="string">[noreply]</span></span><br><span class="line">decr key decrement_values <span class="string">[noreply]</span></span><br></pre></td></tr></table></figure><blockquote><p>increment_values为增加的数值<br>decrement_values为减少的数值</p></blockquote><h3 id="flush-all"><a href="#flush-all" class="headerlink" title="flush_all"></a>flush_all</h3><blockquote><p>flush_all用于清理缓存中的所有的键值对，该命令提供了一个可选参数time，用于在制定的时间后执行清理缓存操作。<br>语法格式如下：<br>linux系统安装memcached首先要安装libevent库</p></blockquote><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flush_all <span class="string">[time]</span> <span class="string">[noreply]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;memcached是一套分布式的高速缓存系统，由LiveJournal的Brad Fitzpatrick开发，但目前被许多网站使用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;ubuntu16-0-4下memcached的安装&quot;&gt;&lt;a href=&quot;#ubuntu16-0-4下memcached的安装&quot; class=&quot;headerlink&quot; title=&quot;ubuntu16.0.4下memcached的安装&quot;&gt;&lt;/a&gt;ubuntu16.0.4下memcached的安装&lt;/h1&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://moyingyao.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://moyingyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="memcached" scheme="http://moyingyao.github.io/tags/memcached/"/>
    
  </entry>
  
  <entry>
    <title>redis安装及简单命令</title>
    <link href="http://moyingyao.github.io/2018/06/02/20180602redis%E5%AE%89%E8%A3%85%E5%8F%8A%E7%AE%80%E5%8D%95%E5%91%BD%E4%BB%A4/"/>
    <id>http://moyingyao.github.io/2018/06/02/20180602redis安装及简单命令/</id>
    <published>2018-06-02T08:45:50.000Z</published>
    <updated>2019-03-14T09:02:14.906Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>redis是一个key-value存储系统。它支持存储的value类型相对更很多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。<br><a id="more"></a><br>Redis是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。</p></blockquote><h1 id="Redis：REmote-DIctionary-Server"><a href="#Redis：REmote-DIctionary-Server" class="headerlink" title="Redis：REmote DIctionary Server"></a>Redis：REmote DIctionary Server</h1><h2 id="Redis-远程字典服务器"><a href="#Redis-远程字典服务器" class="headerlink" title="Redis(远程字典服务器)"></a>Redis(远程字典服务器)</h2><blockquote><p>是完全开源免费的，用C语言编写，是一个高性能的（key/value）分布式内存数据库，基于内存运行并支持持久化的NoSQL数据库，是当前最热门的NoSQL数据库之一，也被人们称之为结构数据服务器。</p></blockquote><h2 id="Redis逐步取代memcached的原因"><a href="#Redis逐步取代memcached的原因" class="headerlink" title="Redis逐步取代memcached的原因"></a>Redis逐步取代memcached的原因</h2><blockquote><p>1.redis支持数据持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用<br>2.redis不仅仅直迟简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储<br>3.redis支持数据的备份，即master-slave魔术的数据备份</p></blockquote><h2 id="Redis能做什么？"><a href="#Redis能做什么？" class="headerlink" title="Redis能做什么？"></a>Redis能做什么？</h2><blockquote><p>1.内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务<br>2.取最新N个数据的操作，如：可以将最新的10条评论的ID放在redis的List集合里面<br>3.模拟类似于HttpSession这种需要设定过期时间的功能<br>4.发布、订阅消息系统<br>5.定时器、计数器</p></blockquote><h2 id="与memcached区别"><a href="#与memcached区别" class="headerlink" title="与memcached区别"></a>与memcached区别</h2><blockquote><p>都是key-value存储<br>memcached一旦服务关闭，数据会全部没有<br>redis服务关闭重启后数据还在</p></blockquote><h1 id="安装Redis"><a href="#安装Redis" class="headerlink" title="安装Redis"></a>安装Redis</h1><blockquote><p>我们的环境：<br>VMware Workplace<br>CentOS-6.5<br>redis-4.0.9<br>下载地址： Http://redis.io<br>将下载好的redis-4.0.9放入虚拟机中，并解压</p></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">tar</span> <span class="selector-tag">-zxvf</span> <span class="selector-tag">redis-4</span><span class="selector-class">.9</span><span class="selector-class">.0</span><span class="selector-class">.tar</span><span class="selector-class">.gz</span></span><br></pre></td></tr></table></figure><blockquote><p>redis内包含的文件</p></blockquote><p><img src="https://i.imgur.com/inuGfQx.png" alt=""></p><blockquote><p>输入命令</p></blockquote><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">make</span></span><br></pre></td></tr></table></figure><blockquote><p>运行运行makefile文件，要有GCC，没有则会报错。<br>安装gcc：</p></blockquote><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">yum</span> <span class="comment">install</span> <span class="comment">gcc</span><span class="literal">-</span><span class="comment">c</span><span class="literal">+</span><span class="literal">+</span></span><br></pre></td></tr></table></figure><blockquote><p>安装完成之后要进行二次make，但是之前要将上一次make不成功的残余文件清理，之后再make</p></blockquote><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">make</span> disclean</span><br><span class="line"><span class="built_in">make</span></span><br></pre></td></tr></table></figure><blockquote><p>make完成后，执行</p></blockquote><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make <span class="keyword">install</span></span><br></pre></td></tr></table></figure><blockquote><p>出现下图所示即安装成功</p></blockquote><p><img src="https://i.imgur.com/pbUT2FT.png" alt=""></p><h1 id="配置redis-conf"><a href="#配置redis-conf" class="headerlink" title="配置redis.conf"></a>配置redis.conf</h1><blockquote><p>进入redis-4.0.9,将redis.conf拷贝一份，在拷贝后的上面进行修改<br>将redis.conf拷贝至myredis文件夹下</p></blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> redis.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/ThxbeJV.png" alt=""></p><blockquote><p>将no修改为yes</p></blockquote><h1 id="启动redis服务"><a href="#启动redis服务" class="headerlink" title="启动redis服务"></a>启动redis服务</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/bin</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/kWyradG.png" alt=""><br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server /<span class="built_in">home</span>/myy/hadoop/myredis/redis.<span class="built_in">config</span></span><br></pre></td></tr></table></figure></p><p><img src="https://i.imgur.com/yJIzrIU.png" alt=""></p><h1 id="进入客户端"><a href="#进入客户端" class="headerlink" title="进入客户端"></a>进入客户端</h1><blockquote><p>客户端默认端口为6379</p></blockquote><p><img src="https://i.imgur.com/7tzwWOB.png" alt=""></p><blockquote><p>判断是否与服务端连接成功</p></blockquote><p><img src="https://i.imgur.com/w0CV6tg.png" alt=""></p><blockquote><p>查看服务状态</p></blockquote><p><img src="https://i.imgur.com/bli2295.png" alt=""></p><blockquote><p>关闭连接</p></blockquote><p><img src="https://i.imgur.com/CX3oEJZ.png" alt=""></p><blockquote><p>关闭后查看服务就没有了</p></blockquote><p><img src="https://i.imgur.com/IiXkIPf.png" alt=""></p><h1 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h1><h2 id="默认库"><a href="#默认库" class="headerlink" title="默认库"></a>默认库</h2><blockquote><p>Redis默认有16个库，进入时默认在0号库，角标从0开始，某些任务找一号库，某些找二号库，任务逻辑更清晰redis.conf中有说明</p></blockquote><p><img src="https://i.imgur.com/BU29JJY.png" alt=""></p><blockquote><p>转换库的命令，eg：切换为8号库</p></blockquote><p><img src="https://i.imgur.com/J2FGtCY.png" alt=""></p><h2 id="flushdb和flushall的区别"><a href="#flushdb和flushall的区别" class="headerlink" title="flushdb和flushall的区别"></a>flushdb和flushall的区别</h2><blockquote><p>flushdb是删除当前库，flushall是删除全部库</p></blockquote><h2 id="Benchmark查看本机状态"><a href="#Benchmark查看本机状态" class="headerlink" title="Benchmark查看本机状态"></a>Benchmark查看本机状态</h2><p><img src="https://i.imgur.com/Z2Q0Fv3.png" alt=""><img src="https://i.imgur.com/7z1VOWO.png" alt=""><img src="https://i.imgur.com/i9NWdjz.png" alt=""><img src="https://i.imgur.com/LmtfECg.png" alt=""><img src="https://i.imgur.com/MXCvvnS.png" alt=""></p><h1 id="常用五大数据类型"><a href="#常用五大数据类型" class="headerlink" title="常用五大数据类型"></a>常用五大数据类型</h1><h2 id="string字符串"><a href="#string字符串" class="headerlink" title="string字符串"></a>string字符串</h2><blockquote><p>String是redis最基本的类型，可以理解为与memcached一摸一样的类型，一个key对应一个value。<br>String类型是二进制安全的，即redis的string可以包含任何数据，比如jpg图片或者序列化对象<br>string是redis最基本的数据类型，一个redis中字符串最多可以是512M</p></blockquote><h2 id="hash哈希"><a href="#hash哈希" class="headerlink" title="hash哈希"></a>hash哈希</h2><blockquote><p>redis hash是一个键值对集合，是一个string类型的filed和value的映射表，适合用于存储对象。<br>类似java中的Map&lt;String,Object&gt;</p></blockquote><h2 id="list列表"><a href="#list列表" class="headerlink" title="list列表"></a>list列表</h2><blockquote><p>列表是简单的字符串列表，按照插入顺利排序。可以添加一个元素到列表的头部（左边）或者尾部（右边），它的底层实际是个链表</p></blockquote><h2 id="set集合"><a href="#set集合" class="headerlink" title="set集合"></a>set集合</h2><blockquote><p>set是string类型的无序集合。是通过HashTable实现的。</p></blockquote><h2 id="Zset有序集合"><a href="#Zset有序集合" class="headerlink" title="Zset有序集合"></a>Zset有序集合</h2><blockquote><p>Zset（sorted set）<br>zset和set一样也是string类型元素的集合，且不允许重复的成员。<br>不同的是每个元素都会关联一个double类型的分数。<br>redis正是通过分数来为集合中的成员进行从小到大的排序。zset的承宣是唯一的，但分数（score）是可以重复的。</p></blockquote><h1 id="常用关键字"><a href="#常用关键字" class="headerlink" title="常用关键字"></a>常用关键字</h1><h2 id="set-get-exists-keys-move-mset-mget"><a href="#set-get-exists-keys-move-mset-mget" class="headerlink" title="set/get/exists/keys/move/mset/mget"></a>set/get/exists/keys/move/mset/mget</h2><blockquote><p>set设置键值，也可以覆盖原来的值<br>get获取对应键的值<br>exists查看某个key是否存在<br>move移动到别的库内<br>mset/mget批量设置/获取键值</p></blockquote><p><img src="https://i.imgur.com/iM55rsZ.png" alt=""><img src="https://i.imgur.com/AiFg5Jl.png" alt=""><img src="https://i.imgur.com/L0raU11.png" alt=""><img src="https://i.imgur.com/6aAYxKL.png" alt=""></p><h2 id="expire-ttl-setex"><a href="#expire-ttl-setex" class="headerlink" title="expire/ttl/setex"></a>expire/ttl/setex</h2><blockquote><p>expire设置秒数，过期后自动消失<br>ttl 查看某个key还有多久时间<br>setex设置值时同时设置时间</p></blockquote><p><img src="https://i.imgur.com/qAMGzHT.png" alt=""><img src="https://i.imgur.com/AAWcCWc.png" alt=""></p><h2 id="append-strlen-getrange-setrange-incr-decr-incrby-decrby"><a href="#append-strlen-getrange-setrange-incr-decr-incrby-decrby" class="headerlink" title="append/strlen/getrange/setrange/incr/decr/incrby/decrby"></a>append/strlen/getrange/setrange/incr/decr/incrby/decrby</h2><blockquote><p>append补充字符串<br>strlen字符串的长度<br>getrange获取指定区域范围内的值，0到-1表示全部<br>setrange设置指定区域范围内的值<br>incr递增加1<br>decr递减少1<br>incrby   decrby自定义数量<br>string类型此命令不可用</p></blockquote><p><img src="https://i.imgur.com/UjpKfJa.png" alt=""><img src="https://i.imgur.com/0Tqgcyz.png" alt=""><img src="https://i.imgur.com/Erl8ku1.png" alt=""></p><h2 id="lpush-lrange-lpop-rpop-lidex-llen"><a href="#lpush-lrange-lpop-rpop-lidex-llen" class="headerlink" title="lpush/lrange/lpop/rpop/lidex/llen"></a>lpush/lrange/lpop/rpop/lidex/llen</h2><blockquote><p>lpush和rpush查看后顺序不同<br>lrange查看list<br>lpop栈顶出去<br>rpop栈底出去<br>lidex索引<br>llen查看list长度</p></blockquote><p><img src="https://i.imgur.com/SmiHEoI.png" alt=""><img src="https://i.imgur.com/QSJZtGe.png" alt=""><img src="https://i.imgur.com/13DBD6n.png" alt=""></p><h2 id="lren-ltrim-rpoplpush-lset-linsert"><a href="#lren-ltrim-rpoplpush-lset-linsert" class="headerlink" title="lren/ltrim/rpoplpush/lset/linsert"></a>lren/ltrim/rpoplpush/lset/linsert</h2><blockquote><p>lrem删除n和value<br>ltrim截取指定范围内的值再赋值给list<br>rpoplpush将list01栈底给list02栈顶<br>lset替换某位置的值<br>linsert某值之前或之后插入某值</p></blockquote><p><img src="https://i.imgur.com/hxDg2SU.png" alt=""><img src="https://i.imgur.com/mXG2x8p.png" alt=""><img src="https://i.imgur.com/RF5E1Vy.png" alt=""><img src="https://i.imgur.com/zguZHDx.png" alt=""><img src="https://i.imgur.com/nO5lZGc.png" alt=""></p><h2 id="sadd-smembers-sismember-scard-srem-srandmember"><a href="#sadd-smembers-sismember-scard-srem-srandmember" class="headerlink" title="sadd/smembers/sismember/scard/srem/srandmember"></a>sadd/smembers/sismember/scard/srem/srandmember</h2><blockquote><p>sadd设置set集合（重复自动留一个）<br>smembers查看set集合<br>sismember查看set内是否有某值<br>scard获取集合内元素个数<br>srem删除集合内某值<br>srandmember集合中随机出几个数</p></blockquote><p><img src="https://i.imgur.com/toMNESc.png" alt=""><img src="https://i.imgur.com/NeBJIHu.png" alt=""><img src="https://i.imgur.com/QSZbjLr.png" alt=""><img src="https://i.imgur.com/GZ37ulu.png" alt=""></p><h2 id="spop-smove-sdiff-sinter-sunion"><a href="#spop-smove-sdiff-sinter-sunion" class="headerlink" title="spop/smove/sdiff/sinter/sunion"></a>spop/smove/sdiff/sinter/sunion</h2><blockquote><p>spop随机出栈<br>smove将一set中某一值<br>赋给另一set<br>sdiff取两集合的差集：在第一个中而不在第二个中<br>sinter取两集合的交集<br>sunion取两集合的并集</p></blockquote><p><img src="https://i.imgur.com/XyQXNW7.png" alt=""><img src="https://i.imgur.com/dRWsJJV.png" alt=""><img src="https://i.imgur.com/u6TdF2U.png" alt=""></p><h2 id="zrange-zrevrange-zcount"><a href="#zrange-zrevrange-zcount" class="headerlink" title="zrange/zrevrange/zcount"></a>zrange/zrevrange/zcount</h2><blockquote><p>zrange同list相同<br>zrevrange从高到低排序<br>zincrby修改某个值的分数<br>zcount返回指定分数范围内值的个数</p></blockquote><p><img src="https://i.imgur.com/q7suFVE.png" alt=""><img src="https://i.imgur.com/EqTjeHX.png" alt=""><img src="https://i.imgur.com/wQCzN1l.png" alt=""><img src="https://i.imgur.com/10VijR6.png" alt=""></p><h2 id="hash相关的关键字"><a href="#hash相关的关键字" class="headerlink" title="hash相关的关键字"></a>hash相关的关键字</h2><blockquote><p>kv模式不变，但v是一个键值对</p></blockquote><p><img src="https://i.imgur.com/GGPPbYV.png" alt=""><img src="https://i.imgur.com/JnBAXGt.png" alt=""><img src="https://i.imgur.com/EUYBanW.png" alt=""><img src="https://i.imgur.com/KUlb3nd.png" alt=""><img src="https://i.imgur.com/jO0YeHh.png" alt=""><img src="https://i.imgur.com/JEnzQND.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;redis是一个key-value存储系统。它支持存储的value类型相对更很多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://moyingyao.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://moyingyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="redis" scheme="http://moyingyao.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>hadoop 分布式搭建</title>
    <link href="http://moyingyao.github.io/2018/05/30/20180530hadoop-%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA/"/>
    <id>http://moyingyao.github.io/2018/05/30/20180530hadoop-分布式搭建/</id>
    <published>2018-05-30T07:58:55.000Z</published>
    <updated>2019-03-14T08:57:39.715Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。<br>下面我们一起来搭建吧。<br><a id="more"></a></p></blockquote><h1 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h1><blockquote><p>linux环境下搭建hadoop集群需要准备：<br>VMware-workstation-10.0.1注册机<br>CentOS-6.5-x86_64-bin-DVD1<br>jdk-7u79-linux-x64<br>hadoop-2.6.4.tar</p></blockquote><p><img src="https://i.imgur.com/vrtN7JU.png" alt=""></p><h1 id="新建虚拟机"><a href="#新建虚拟机" class="headerlink" title="新建虚拟机"></a>新建虚拟机</h1><blockquote><p>解压VMware-workstation-10.0.1注册机,打开VMware Workstation主页,点击新建虚拟机,选择典型,如下:<br><img src="https://i.imgur.com/Wbzensp.png" alt=""><br>点击下一步,选择安装程序光盘映像文件,浏览找到你下载CentOS-6.5-x86_64-bin-DVD1的压缩包文件,如下:<br><img src="https://i.imgur.com/IzWxBjV.png" alt=""><br>继续点击下一步,填写用户名和密码(尽量简单),填好后点击下一步,为即将创建的虚拟机命名并选择安装路径(最好不要安装在C盘),如下所示:<br><img src="https://i.imgur.com/hjdvvqZ.png" alt=""><br>继续点击下一步至如下界面:<br><img src="https://i.imgur.com/chMAScN.png" alt=""><br>点击自定义硬件可以修改虚拟机的各项参数,如果电脑内存小于等于4GB,需要将内存改至512MB,否则严重卡顿。修改完成后点击完成，虚拟机就创建成功，打开后界面如下：<br><img src="https://i.imgur.com/RlJpVk3.png" alt=""><br>若要批量创建虚拟机，可以在创建好的虚拟机的基础上进行克隆操作，<br><img src="https://i.imgur.com/lbyJLai.png" alt=""></p></blockquote><h1 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h1><blockquote><p>打开一个虚拟机，右键单击桌面选择Open in Terminal，进入编辑界面：</p></blockquote><h2 id="假设用户名为wxx"><a href="#假设用户名为wxx" class="headerlink" title="假设用户名为wxx"></a>假设用户名为wxx</h2><h3 id="获取root权限"><a href="#获取root权限" class="headerlink" title="获取root权限"></a>获取root权限</h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su  </span><br><span class="line"><span class="keyword">cd</span> <span class="string">/etc</span></span><br><span class="line">vi sudoers</span><br></pre></td></tr></table></figure><blockquote><p>i 进入编辑状态，在</p></blockquote><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>)</span><br></pre></td></tr></table></figure><blockquote><p>ALL的下一行编辑</p></blockquote><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wxx  <span class="keyword">ALL</span>=(<span class="keyword">ALL</span>) <span class="keyword">ALL</span></span><br></pre></td></tr></table></figure><blockquote><p>按ESC键，退出编辑格式<br>按Shift + :<br>输入wq!保存并退出</p></blockquote><h3 id="创建hadoop文件夹"><a href="#创建hadoop文件夹" class="headerlink" title="创建hadoop文件夹"></a>创建hadoop文件夹</h3><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line"><span class="built_in">mkdir</span>  hadoop</span><br></pre></td></tr></table></figure><blockquote><p>将jdk-7u79-linux-x64安装包复制到hadoop文件目录下（与windows环境下类似）。</p></blockquote><h3 id="解压jdk-7u79-linux-x64-gz文件"><a href="#解压jdk-7u79-linux-x64-gz文件" class="headerlink" title="解压jdk-7u79-linux-x64.gz文件"></a>解压jdk-7u79-linux-x64.gz文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line"><span class="built_in">cd</span> hadoop</span><br><span class="line">tar-zxvf jdk-7u79-linux-x64.gz</span><br></pre></td></tr></table></figure><h3 id="设置jdk环境变量"><a href="#设置jdk环境变量" class="headerlink" title="设置jdk环境变量"></a>设置jdk环境变量</h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span></span><br><span class="line"><span class="keyword">cd</span>  hadoop</span><br><span class="line">su</span><br><span class="line">gedit <span class="string">/etc/profile</span></span><br></pre></td></tr></table></figure><blockquote><p>进入后在最后一行添加以下指令：</p></blockquote><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/home/by</span><span class="regexp">/hadoop/jdk</span>1.<span class="number">8.0_11</span></span><br><span class="line">export PATH=$JAVA_HOME/<span class="symbol">bin:</span>$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br></pre></td></tr></table></figure><blockquote><p>点击保存后关闭，输入以下指令使jdk生效：</p></blockquote><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure><h3 id="检查jdk是否安装成功"><a href="#检查jdk是否安装成功" class="headerlink" title="检查jdk是否安装成功"></a>检查jdk是否安装成功</h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="built_in">version</span></span><br></pre></td></tr></table></figure><blockquote><p>成功后显示如下信息：</p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version <span class="string">"1.7.0_79"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.7.0_79-b15)</span><br><span class="line">Java HotSpot(TM) 64-Bit<span class="built_in"> Server </span>VM (build 24.79-b02, mixed mode)</span><br></pre></td></tr></table></figure><h1 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h1><h2 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h2><blockquote><p>将已经安装好jdk的虚拟机克隆两个，创建三个虚拟机的集群。</p></blockquote><h2 id="修改hostname"><a href="#修改hostname" class="headerlink" title="修改hostname"></a>修改hostname</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su</span><br><span class="line">vi  <span class="regexp">/etc/</span>sysconfig<span class="regexp">/network</span></span><br></pre></td></tr></table></figure><blockquote><p>将三个虚拟机分别命名master、slave1、slave2<br>如图：</p></blockquote><p><img src="https://i.imgur.com/wUQ1xjq.png" alt=""></p><blockquote><p>完成后重启虚拟机reboot</p></blockquote><h2 id="将三个虚拟机的ip地址相互连接"><a href="#将三个虚拟机的ip地址相互连接" class="headerlink" title="将三个虚拟机的ip地址相互连接"></a>将三个虚拟机的ip地址相互连接</h2><blockquote><p>首先必须确保虚拟机联网，如果NET模式连不上网，则选中桥接模式。<br>网络通畅后执行以下操作:<br>1.查看三台虚拟机IP,分别对三个虚拟机执行指令ifconfig，查看各虚拟机ip地址</p></blockquote><blockquote><p>2.在master中执行以下指令</p></blockquote><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su</span><br><span class="line">cd/etc</span><br><span class="line">gedit /etc/hosts <span class="number">192.168</span><span class="number">.142</span><span class="number">.142</span> <span class="number">192.168</span><span class="number">.142</span><span class="number">.143</span></span><br></pre></td></tr></table></figure><blockquote><p>进入编辑界面后按“IP地址   hostname”填写信息，如图：</p></blockquote><p><img src="https://i.imgur.com/O1rxMcZ.png" alt=""></p><blockquote><p>填写完后按Save按钮，关闭编辑页。</p></blockquote><blockquote><p>3.将配置好的文件复制到slave1、slave2中,在master中执行以下指令：</p></blockquote><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/hosts root<span class="variable">@slave1</span><span class="symbol">:/etc/</span></span><br><span class="line">scp /etc/hosts root<span class="variable">@slave2</span><span class="symbol">:/etc/</span></span><br></pre></td></tr></table></figure><blockquote><p>4.检查各虚拟机是否互联,在master中执行以下指令：</p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ping slave1</span><br><span class="line">ping slave2</span><br></pre></td></tr></table></figure><blockquote><p>连通即完成</p></blockquote><h2 id="配置SSH无密钥登录"><a href="#配置SSH无密钥登录" class="headerlink" title="配置SSH无密钥登录"></a>配置SSH无密钥登录</h2><blockquote><p>1.关闭防火墙,对每个虚拟机进行如下操作：</p></blockquote><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su</span><br><span class="line"><span class="attribute">chkconfig</span> iptables <span class="literal">off</span></span><br></pre></td></tr></table></figure><blockquote><p>执行后重启虚拟机： </p></blockquote><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">reboot</span></span><br></pre></td></tr></table></figure><blockquote><p>2.关闭防火墙后在master下执行以下指令：</p></blockquote><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd</span><br><span class="line">ssh-keygen –t rsa</span><br><span class="line">cd  .ssh</span><br><span class="line">cat  id_rsa.pub  <span class="meta">&gt;&gt;  </span>authorized_keys</span><br><span class="line">chmod  <span class="number">600</span>  authorized_keys </span><br><span class="line">scp  authorized_keys  wxx@slave1<span class="symbol">:~/</span>.ssh/</span><br><span class="line">scp  authorized_keys  wxx@slave2<span class="symbol">:~/</span>.ssh/</span><br></pre></td></tr></table></figure><blockquote><p>3.检查无密钥登录是否成功</p></blockquote><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh slave1</span><br><span class="line">ssh slave2</span><br><span class="line">ssh  <span class="literal">master</span></span><br></pre></td></tr></table></figure><blockquote><p>成功后显示如下：</p></blockquote><p><img src="https://i.imgur.com/I25emLH.png" alt=""></p><h2 id="安装并配置hadoop-2-6-4-在master中"><a href="#安装并配置hadoop-2-6-4-在master中" class="headerlink" title="安装并配置hadoop-2.6.4(在master中)"></a>安装并配置hadoop-2.6.4(在master中)</h2><blockquote><p>1.将hadoop-2.6.4.tar.gz安装包复制到hadoop文件目录下（与windows环境下类似）。</p></blockquote><blockquote><p>2.解压hadoop-2.6.4.tar.gz</p></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">cd</span></span><br><span class="line"><span class="selector-tag">cd</span> <span class="selector-tag">hadoop</span></span><br><span class="line"><span class="selector-tag">tar</span> <span class="selector-tag">-zxvf</span> <span class="selector-tag">hadoop-2</span><span class="selector-class">.6</span><span class="selector-class">.4</span><span class="selector-class">.tar</span><span class="selector-class">.gz</span></span><br></pre></td></tr></table></figure><blockquote><p>3.配置hadoop-2.6.4的各项文件</p></blockquote><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span></span><br><span class="line"><span class="keyword">cd</span> hadoop/hadoop-2.7.4</span><br><span class="line"><span class="keyword">cd</span> etc/hadoop</span><br><span class="line">gedit hadoop-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><blockquote><p>在最后一行添加:</p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/home/by/hadoop/ jdk1.8.0_11</span><br></pre></td></tr></table></figure><h3 id="编辑core-site-xml"><a href="#编辑core-site-xml" class="headerlink" title="编辑core-site.xml"></a>编辑core-site.xml</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">gedit</span> <span class="selector-tag">core-site</span><span class="selector-class">.xml</span></span><br></pre></td></tr></table></figure><blockquote><p>添加代码：</p></blockquote><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>fs.default.name<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span>hdfs:<span class="comment">//master:9000&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;final&gt;</span>true<span class="params">&lt;/final&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>hadoop.tmp.dir<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span><span class="meta-keyword">/home/</span>by<span class="meta-keyword">/hadoop/</span>tmp<span class="params">&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>ds.default.name<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span>hdfs:<span class="comment">//master:54310&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;final&gt;</span>true<span class="params">&lt;/final&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编辑hdfs-site-xml"><a href="#编辑hdfs-site-xml" class="headerlink" title="编辑hdfs-site.xml"></a>编辑hdfs-site.xml</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">gedit</span> <span class="selector-tag">hdfs-site</span><span class="selector-class">.xml</span></span><br></pre></td></tr></table></figure><blockquote><p>添加代码：</p></blockquote><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>dfs.namenode.name.dir<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span>file:<span class="meta-keyword">/home/</span>by<span class="meta-keyword">/hadoop/</span>dfs/name<span class="params">&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;final&gt;</span>true<span class="params">&lt;/final&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>dfs.datanode.data.dir<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span>file:<span class="meta-keyword">/home/</span>by<span class="meta-keyword">/hadoop/</span>dfs/data<span class="params">&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;final&gt;</span>true<span class="params">&lt;/final&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br><span class="line"><span class="params">&lt;property&gt;</span></span><br><span class="line"><span class="params">&lt;name&gt;</span>dfs.replication<span class="params">&lt;/name&gt;</span></span><br><span class="line"><span class="params">&lt;value&gt;</span><span class="number">2</span><span class="params">&lt;/value&gt;</span></span><br><span class="line"><span class="params">&lt;/property&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编辑mapred-site-xml"><a href="#编辑mapred-site-xml" class="headerlink" title="编辑mapred-site.xml"></a>编辑mapred-site.xml</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">gedit</span> <span class="selector-tag">mapred-site</span><span class="selector-class">.xml</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：必须先复制mapred-site.xml.template文件更名为mapred-site.xml<br>添加代码：</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编辑yarn-site-xml"><a href="#编辑yarn-site-xml" class="headerlink" title="编辑yarn-site.xml"></a>编辑yarn-site.xml</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">gedit</span>  <span class="selector-tag">yarn-site</span><span class="selector-class">.xml</span></span><br></pre></td></tr></table></figure><blockquote><p>添加代码：</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="编辑master"><a href="#编辑master" class="headerlink" title="编辑master"></a>编辑master</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit <span class="literal">master</span></span><br></pre></td></tr></table></figure><blockquote><p>添加代码：</p></blockquote><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br></pre></td></tr></table></figure><h3 id="编辑slaves"><a href="#编辑slaves" class="headerlink" title="编辑slaves"></a>编辑slaves</h3><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">gedit slaves</span></span><br></pre></td></tr></table></figure><blockquote><p>添加代码：</p></blockquote><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">master</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><blockquote><p>4.将配置好的文件复制到slave1、slave2中</p></blockquote><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> </span><br><span class="line"><span class="keyword">cd</span> hadoop</span><br><span class="line">scp -r hadoop-2.7.4 slave1:~<span class="string">/hadoop</span></span><br><span class="line">scp -r hadoop-2.7.4 slave2:~<span class="string">/hadoop</span></span><br></pre></td></tr></table></figure><blockquote><p>5.启动集群</p></blockquote><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span></span><br><span class="line"><span class="built_in">cd</span>  hadoop/hadoop-<span class="number">2</span>.<span class="number">7</span>.<span class="number">4</span></span><br><span class="line">bin/hdfs namenode  -<span class="built_in">format</span></span><br><span class="line">sbin/<span class="built_in">start</span>-dfs.sh</span><br><span class="line">sbin/<span class="built_in">start</span>-yarn.sh</span><br><span class="line">sbin/hadoop-daemon.sh  <span class="built_in">start</span>  secondarynamenode</span><br></pre></td></tr></table></figure><blockquote><p>6.检查集群情况</p></blockquote><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">jps</span></span><br></pre></td></tr></table></figure><blockquote><p>三台虚拟机如下所示：</p></blockquote><p><img src="https://i.imgur.com/Y1KfeJY.png" alt=""><br><img src="https://i.imgur.com/VlP3QMy.png" alt=""><br><img src="https://i.imgur.com/7Nsazie.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。&lt;br&gt;下面我们一起来搭建吧。&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://moyingyao.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="hadoop" scheme="http://moyingyao.github.io/tags/hadoop/"/>
    
      <category term="大数据" scheme="http://moyingyao.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>LaTeX入门</title>
    <link href="http://moyingyao.github.io/2018/05/27/20180527LaTeX%E5%85%A5%E9%97%A8/"/>
    <id>http://moyingyao.github.io/2018/05/27/20180527LaTeX入门/</id>
    <published>2018-05-27T07:04:26.000Z</published>
    <updated>2019-03-14T08:54:53.745Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>LaTeX是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天，甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。我们在投稿论文的时候，会经常使用LaTex根据期刊的要求对文章进行排版，所以作为一名研究生学习这个是十分必要的。<br><a id="more"></a></p></blockquote><h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><blockquote><p>打开WinEdt，建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8。</p></blockquote><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span>  </span><br><span class="line">hello, world  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><blockquote><p>然后在WinEdt的工具栏中找到编译按钮（在垃圾桶和字母B中间），在下拉菜单中选择XeTeX，并点击编译。<br>如果顺利的话，就可以顺利生成出第一个pdf文件，点击工具栏中的放大镜按钮就可以快速打开生成的pdf文件。 </p></blockquote><h1 id="标题、作者、章节和段落"><a href="#标题、作者、章节和段落" class="headerlink" title="标题、作者、章节和段落"></a>标题、作者、章节和段落</h1><blockquote><p>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。</p></blockquote><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">author</span><span class="string">&#123;My Name&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">title</span><span class="string">&#123;The Title&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">maketitle</span></span>  </span><br><span class="line">hello, world <span class="comment">% This is comment  </span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><blockquote><p>效果图如下：</p></blockquote><p><img src="https://i.imgur.com/KPP0wzq.png" alt=""> </p><h1 id="加入目录"><a href="#加入目录" class="headerlink" title="加入目录"></a>加入目录</h1><blockquote><p>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，编译并观察现象。</p></blockquote><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">tableofcontents</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">section</span><span class="string">&#123;Hello China&#125;</span></span> China is in East Asia.  </span><br><span class="line"><span class="tag">\<span class="name">subsection</span><span class="string">&#123;Hello Beijing&#125;</span></span> Beijing is the capital of China.  </span><br><span class="line"><span class="tag">\<span class="name">subsubsection</span><span class="string">&#123;Hello Dongcheng District&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">paragraph</span><span class="string">&#123;Hello Tian'anmen Square&#125;</span></span>is in the center of Beijing  </span><br><span class="line"><span class="tag">\<span class="name">subparagraph</span><span class="string">&#123;Hello Chairman Mao&#125;</span></span> is in the center of Tian'anmen Square  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><blockquote><p>效果图如下：</p></blockquote><p><img src="https://i.imgur.com/5KAaLd6.png" alt=""> </p><h1 id="段落和换行"><a href="#段落和换行" class="headerlink" title="段落和换行"></a>段落和换行</h1><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">\documentclass&#123;article&#125;</span>  </span><br><span class="line"><span class="string">\begin&#123;document&#125;</span>  </span><br><span class="line">Beijing <span class="keyword">is</span>  </span><br><span class="line">the capital  </span><br><span class="line"><span class="keyword">of</span> China.  </span><br><span class="line">  </span><br><span class="line">New York <span class="keyword">is</span>  </span><br><span class="line">  </span><br><span class="line">the capital  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">of</span> America.  </span><br><span class="line">  </span><br><span class="line">Amsterdam <span class="keyword">is</span> <span class="string">\\</span> the capital <span class="string">\\</span>  </span><br><span class="line"><span class="keyword">of</span> Netherlands.  </span><br><span class="line"><span class="string">\end&#123;document&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>效果图如下：</p></blockquote><p><img src="https://i.imgur.com/Rpk6Hh2.png" alt=""></p><h1 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h1><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amsmath&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;amssymb&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span>  </span><br><span class="line">The Newton's second law is F=ma.  </span><br><span class="line">  </span><br><span class="line">The Newton's second law is <span class="formula">$F=ma$</span>.  </span><br><span class="line">  </span><br><span class="line">The Newton's second law is  </span><br><span class="line">F=ma</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">The Newton's second law is  </span><br><span class="line">F=ma</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">Greek Letters <span class="formula">$<span class="tag">\<span class="name">eta</span></span>$</span> and <span class="formula">$<span class="tag">\<span class="name">mu</span></span>$</span>  </span><br><span class="line">  </span><br><span class="line">Fraction <span class="formula">$<span class="tag">\<span class="name">frac</span><span class="string">&#123;a&#125;</span><span class="string">&#123;b&#125;</span></span>$</span>  </span><br><span class="line">  </span><br><span class="line">Power <span class="formula">$a^b$</span>  </span><br><span class="line">  </span><br><span class="line">Subscript <span class="formula">$a_b$</span>  </span><br><span class="line">  </span><br><span class="line">Derivate <span class="formula">$<span class="tag">\<span class="name">frac</span><span class="string">&#123;\partial y&#125;</span><span class="string">&#123;\partial t&#125;</span></span> $</span>  </span><br><span class="line">  </span><br><span class="line">Vector <span class="formula">$<span class="tag">\<span class="name">vec</span><span class="string">&#123;n&#125;</span></span>$</span>  </span><br><span class="line">  </span><br><span class="line">Bold <span class="formula">$<span class="tag">\<span class="name">mathbf</span><span class="string">&#123;n&#125;</span></span>$</span>  </span><br><span class="line">  </span><br><span class="line">To time differential <span class="formula">$<span class="tag">\<span class="name">dot</span><span class="string">&#123;F&#125;</span></span>$</span>  </span><br><span class="line">  </span><br><span class="line">Matrix (lcr here means left, center or right for each column)  </span><br><span class="line"><span class="tag">\<span class="name">[</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">left</span><span class="string">[  </span></span></span><br><span class="line"><span class="tag"><span class="string">\begin&#123;array&#125;&#123;lcr&#125;  </span></span></span><br><span class="line"><span class="tag"><span class="string">a1 &amp; b22 &amp; c333 \\  </span></span></span><br><span class="line"><span class="tag"><span class="string">d444 &amp; e555555 &amp; f6  </span></span></span><br><span class="line"><span class="tag"><span class="string">\end&#123;array&#125;  </span></span></span><br><span class="line"><span class="tag"><span class="string">\right]</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">]</span></span>  </span><br><span class="line">  </span><br><span class="line">Equations(here <span class="tag">\<span class="name">&amp;</span></span> is the symbol for aligning different rows)  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;align&#125;</span></span>  </span><br><span class="line">a+b&amp;=c<span class="tag">\<span class="name">\</span></span>  </span><br><span class="line">d&amp;=e+f+g  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;align&#125;</span></span>  </span><br><span class="line">  </span><br><span class="line"><span class="tag">\<span class="name">[</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">left</span></span><span class="tag">\<span class="name">&#123;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;aligned&#125;</span></span>  </span><br><span class="line">&amp;a+b=c<span class="tag">\<span class="name">\</span></span>  </span><br><span class="line">&amp;d=e+f+g  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;aligned&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">right</span></span>.  </span><br><span class="line"><span class="tag">\<span class="name">]</span></span>  </span><br><span class="line">  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><blockquote><p>效果图如下：</p></blockquote><p><img src="https://i.imgur.com/JCSYPxO.png" alt=""></p><h1 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h1><blockquote><p>先搜索到一个将图片转成eps文件的软件，很容易找的，然后将图片保存为一个名字如figure1.eps。<br>建立一个新文档，将以下内容复制进入文档中，保存，保存类型选择为UTF-8，放在和图片文件同一个文件夹里，编<br>译并观察现象。</p></blockquote><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">usepackage</span><span class="string">&#123;graphicx&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">includegraphics</span><span class="string">[width=4.00in,height=3.00in]</span><span class="string">&#123;figure1.eps&#125;</span></span>  </span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><h1 id="简单表格"><a href="#简单表格" class="headerlink" title="简单表格"></a>简单表格</h1><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">&#123;article&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;document&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;tabular&#125;</span><span class="string">&#123;|c|c|&#125;</span></span></span><br><span class="line">a &amp; b <span class="tag">\<span class="name">\</span></span></span><br><span class="line">c &amp; d<span class="tag">\<span class="name">\</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;tabular&#125;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;tabular&#125;</span><span class="string">&#123;|c|c|&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">hline</span></span></span><br><span class="line">a &amp; b <span class="tag">\<span class="name">\</span></span></span><br><span class="line"><span class="tag">\<span class="name">hline</span></span></span><br><span class="line">c &amp; d<span class="tag">\<span class="name">\</span></span></span><br><span class="line"><span class="tag">\<span class="name">hline</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;tabular&#125;</span></span></span><br><span class="line"></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;center&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">begin</span><span class="string">&#123;tabular&#125;</span><span class="string">&#123;|c|c|&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">hline</span></span></span><br><span class="line">a &amp; b <span class="tag">\<span class="name">\</span></span> <span class="tag">\<span class="name">hline</span></span></span><br><span class="line">c &amp; d<span class="tag">\<span class="name">\</span></span></span><br><span class="line"><span class="tag">\<span class="name">hline</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;tabular&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;center&#125;</span></span></span><br><span class="line"><span class="tag">\<span class="name">end</span><span class="string">&#123;document&#125;</span></span></span><br></pre></td></tr></table></figure><blockquote><p>效果图如下：</p></blockquote><p><img src="https://i.imgur.com/JoYKa5s.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;LaTeX是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天，甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。我们在投稿论文的时候，会经常使用LaTex根据期刊的要求对文章进行排版，所以作为一名研究生学习这个是十分必要的。&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="LaTeX" scheme="http://moyingyao.github.io/categories/LaTeX/"/>
    
    
      <category term="LaTeX" scheme="http://moyingyao.github.io/tags/LaTeX/"/>
    
      <category term="文档编写" scheme="http://moyingyao.github.io/tags/%E6%96%87%E6%A1%A3%E7%BC%96%E5%86%99/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战--决策树</title>
    <link href="http://moyingyao.github.io/2018/05/25/20180525%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://moyingyao.github.io/2018/05/25/20180525机器学习实战-决策树/</id>
    <published>2018-05-25T02:32:28.000Z</published>
    <updated>2019-03-14T08:53:15.098Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><blockquote><p>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据<br>缺点：可能会产生过度匹配问题<br>使用数据类型：数值型和标称型<br>专家系统中，经常使用决策树<br><a id="more"></a></p></blockquote><h1 id="trees-py"><a href="#trees-py" class="headerlink" title="trees.py"></a>trees.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br></pre></td></tr></table></figure><h2 id="createDataSet"><a href="#createDataSet" class="headerlink" title="createDataSet()"></a>createDataSet()</h2><blockquote><p>创建数据集</p></blockquote><h1 id="trees-py-1"><a href="#trees-py-1" class="headerlink" title="trees.py"></a>trees.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 数据集中两个特征'no surfacing','flippers', 数据的两个类标签'yes','no</span></span><br><span class="line">    <span class="comment">#dataSet是个list</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure><h2 id="calcShannonEnt-dataSet"><a href="#calcShannonEnt-dataSet" class="headerlink" title="calcShannonEnt(dataSet)"></a>calcShannonEnt(dataSet)</h2><blockquote><p>计算给定数据集的熵</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)   <span class="comment">#计算数据集中实例的总数</span></span><br><span class="line">    labelCounts = &#123;&#125;            <span class="comment">#创建空字典</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:     <span class="comment">#提取数据集每一行的特征向量</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]  <span class="comment">#获取特征向量最后一列的标签</span></span><br><span class="line">        <span class="comment"># 检测字典的关键字key中是否存在该标签，如果不存在keys()关键字，将当前标签/0键值对存入字典中,并赋值为0</span></span><br><span class="line">        <span class="comment">#print(labelCounts.keys())</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        <span class="comment">#print(labelCounts)</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>  <span class="comment">#否则将当前标签对应的键值加1</span></span><br><span class="line">        <span class="comment">#print("%s="%currentLabel,labelCounts[currentLabel])</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span>    <span class="comment">#初始化熵为0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries   <span class="comment">#计算各值出现的频率</span></span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)    <span class="comment">#以2为底求对数再乘以出现的频率，即信息期望值</span></span><br><span class="line">        <span class="comment">#print("%s="%labelCounts[key],shannonEnt)</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><h2 id="splitDataSet-dataSet-axis-value"><a href="#splitDataSet-dataSet-axis-value" class="headerlink" title="splitDataSet(dataSet, axis, value)"></a>splitDataSet(dataSet, axis, value)</h2><blockquote><p>按照给定特征划分数据集<br>得到熵之后，还需划分数据集，以便判断当前是否正确地划分了数据集，三个输入参数分别为：带划分的数据集，划分数据集的特征，需要返回的特征得值，挑选出dataSet中axis位置值为value的剩余部分。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:  <span class="comment">#筛选出dataSet中axis位置值为value</span></span><br><span class="line">           <span class="comment">#列表的索引中冒号的作用，a[1: ]表示该列表中的第1个元素到最后一个元素，而a[ : n]表示从第0歌元素到第n个元素(不包括n)</span></span><br><span class="line">           reducedFeatVec = featVec[:axis] <span class="comment">#取出特定位置前面部分并赋值给reducedFeatVec</span></span><br><span class="line">           <span class="comment">#print(featVec[axis+1:])</span></span><br><span class="line">           <span class="comment">#print(reducedFeatVec)</span></span><br><span class="line">           reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])     <span class="comment">#取出特定位置后面部分并赋值给reducedFeatVec</span></span><br><span class="line">           retDataSet.append(reducedFeatVec)</span><br><span class="line">           <span class="comment">#print(retDataSet)</span></span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure><h2 id="chooseBestFeatureToSplit-dataSet"><a href="#chooseBestFeatureToSplit-dataSet" class="headerlink" title="chooseBestFeatureToSplit(dataSet)"></a>chooseBestFeatureToSplit(dataSet)</h2><blockquote><p>选择最好的数据集划分方式<br>选取特征，划分数据集，计算得出最好的划分数据集的特征</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>      <span class="comment">#计算特征数量，即每一列表元素具有的列数，再减去最后一列为标签，故需减去1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)       <span class="comment">#计算信息熵，此处值为0.9709505944546686，此值将与划分之后的数据集计算的信息熵进行比较</span></span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>;bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]      <span class="comment">#创建标签列表</span></span><br><span class="line">        <span class="comment">#print(featList)</span></span><br><span class="line">        uniqueVals = set(featList)       <span class="comment">#确定某一特征下所有可能的取值,set集合类型中的每个值互不相同</span></span><br><span class="line">        <span class="comment">#print(uniqueVals)</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:        <span class="comment">#计算每种划分方式的信息熵</span></span><br><span class="line">           subDataSet = splitDataSet(dataSet, i, value)        <span class="comment">#抽取该特征的每个取值下其他特征的值组成新的子数据集</span></span><br><span class="line">           prob = len(subDataSet)/float(len(dataSet))      <span class="comment">#计算该特征下的每一个取值对应的概率（或者说所占的比重）</span></span><br><span class="line">           newEntropy += prob * calcShannonEnt(subDataSet)     <span class="comment">#计算该特征下每一个取值的子数据集的信息熵，并求和</span></span><br><span class="line">        infoGain = baseEntropy - newEntropy     <span class="comment">#计算每个特征的信息增益</span></span><br><span class="line">        <span class="comment">#print("第%d个特征是的取值是%s，对应的信息增益值是%f"%((i+1),uniqueVals,infoGain))</span></span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">           bestInfoGain = infoGain</span><br><span class="line">           bestFeature = i</span><br><span class="line">           <span class="comment">#print("第%d个特征的信息增益最大，所以选择它作为划分的依据，其特征的取值为%s,对应的信息增益值是%f"%((i+1),uniqueVals,infoGain))</span></span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure><h2 id="majorityCnt-classList"><a href="#majorityCnt-classList" class="headerlink" title="majorityCnt(classList)"></a>majorityCnt(classList)</h2><blockquote><p>递归构建决策树，返回出现次数最多的分类名称</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">   classCount=&#123;&#125;</span><br><span class="line">   <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">       <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): classCount[vote] = <span class="number">0</span></span><br><span class="line">       classCount[vote] += <span class="number">1</span></span><br><span class="line">   sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">   <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="createTree-dataSet-labels"><a href="#createTree-dataSet-labels" class="headerlink" title="createTree(dataSet,labels)"></a>createTree(dataSet,labels)</h2><blockquote><p>创建树,参数为数据集和标签列表</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet,labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]       <span class="comment">#提取dataset中的最后一列——种类标签</span></span><br><span class="line">    <span class="comment">#print(classList)</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):    <span class="comment">#计算classlist[0]出现的次数,如果相等，说明都是属于一类，不用继续往下划分</span></span><br><span class="line">       <span class="keyword">return</span> classList[<span class="number">0</span>]     <span class="comment">#递归结束的第一个条件是所有的类标签完全相同，则直接返回该类标签</span></span><br><span class="line">   <span class="comment">#print(dataSet[0])</span></span><br><span class="line">   <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>: <span class="comment">#看还剩下多少个属性，如果只有一个属性，但是类别标签有多个，就直接用majoritycnt()进行整理，选取类别最多的作为返回值</span></span><br><span class="line">       <span class="keyword">return</span> majorityCnt(classList)   <span class="comment">#递归结束的第二个条件是使用完了所有的特征，仍然不能将数据集划分成仅包含唯一类别的分组，则返回出现次数最多的类别</span></span><br><span class="line">   bestFeat = chooseBestFeatureToSplit(dataSet)    <span class="comment">#选取信息增益最大的特征作为下一次分类的依据</span></span><br><span class="line">   bestFeatLabel = labels[bestFeat]     <span class="comment">#选取特征对应的标签</span></span><br><span class="line">   <span class="comment">#print(bestFeatLabel)</span></span><br><span class="line">   myTree = &#123;bestFeatLabel:&#123;&#125;&#125;  <span class="comment">#创建tree字典，下一个特征位于第二个大括号内，循环递归</span></span><br><span class="line">   <span class="keyword">del</span>(labels[bestFeat])   <span class="comment">#删除使用过的特征</span></span><br><span class="line">   featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]     <span class="comment">#特征值对应的该栏数据</span></span><br><span class="line">   <span class="comment">#print(featValues)</span></span><br><span class="line">    uniqueVals = set(featValues)    <span class="comment">#找到featvalues所包含的所有元素，去重复</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">       subLabels = labels[:]        <span class="comment">#将使用过的标签删除更新后，赋值给新的列表，进行迭代</span></span><br><span class="line">       <span class="comment">#print(subLabels)</span></span><br><span class="line">       myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat,value),subLabels) <span class="comment">#循环递归生成树</span></span><br><span class="line">   <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure><h2 id="classify-inputTree-featLabels-testVec"><a href="#classify-inputTree-featLabels-testVec" class="headerlink" title="classify(inputTree,featLabels,testVec):"></a>classify(inputTree,featLabels,testVec):</h2><blockquote><p>测试算法，使用决策树执行分类</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">   firstStr = list(inputTree.keys())[<span class="number">0</span>]    <span class="comment">#找到树的第一个分类特征，或者说根节点'no surfacing'</span></span><br><span class="line">   <span class="comment">#print(firstStr)</span></span><br><span class="line">   secondDict = inputTree[firstStr]    <span class="comment">#从树中得到该分类特征的分支，有0和1</span></span><br><span class="line">   <span class="comment">#print(secondDict)</span></span><br><span class="line">   featIndex = featLabels.index(firstStr)  <span class="comment">#根据分类特征的索引找到对应的标称型数据值，'no surfacing'对应的索引为0</span></span><br><span class="line">    <span class="comment">#print(featIndex)</span></span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">       classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure><h2 id="storeTree-inputTree-filename"><a href="#storeTree-inputTree-filename" class="headerlink" title="storeTree(inputTree,filename)"></a>storeTree(inputTree,filename)</h2><blockquote><p>决策树的存储，使用pickle序列化对象，可在磁盘中保存对象。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">   <span class="keyword">import</span> pickle</span><br><span class="line">   fw = open(filename,<span class="string">'wb'</span>)    <span class="comment">#二进制写入'wb'</span></span><br><span class="line">   pickle.dump(inputTree,fw)   <span class="comment">#pickle的dump函数将决策树写入文件中</span></span><br><span class="line">   fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">   <span class="keyword">import</span> pickle</span><br><span class="line">   fr = open(filename,<span class="string">'rb'</span>)    <span class="comment">#对应于二进制方式写入数据，'rb'采用二进制形式读出数据</span></span><br><span class="line">   <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><h1 id="trees-main-py"><a href="#trees-main-py" class="headerlink" title="trees_main.py"></a>trees_main.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> trees</span><br><span class="line"><span class="keyword">from</span> imp <span class="keyword">import</span> reload</span><br><span class="line"><span class="keyword">import</span> treePlotter</span><br></pre></td></tr></table></figure><h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myDat,labels=trees.createDataSet()</span><br><span class="line"><span class="comment">#print(myDat)</span></span><br><span class="line"><span class="comment">#print(labels)</span></span><br><span class="line"><span class="comment">#print(trees.calcShannonEnt(myDat))</span></span><br></pre></td></tr></table></figure><h2 id="熵增大的原因"><a href="#熵增大的原因" class="headerlink" title="熵增大的原因"></a>熵增大的原因</h2><blockquote><p>熵越高，混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#myDat[1][-1]='maybe'#更改list中某一元素的值（除yes和no外的值），即为添加更多的分类，中括号中为对应元素行列的位置</span></span><br><span class="line"><span class="comment">#print(myDat)</span></span><br><span class="line"><span class="comment">#print(trees.calcShannonEnt(myDat))  #分类变多，熵增大</span></span><br></pre></td></tr></table></figure><h2 id="append-和extend-两类方法的区别"><a href="#append-和extend-两类方法的区别" class="headerlink" title="append()和extend()两类方法的区别"></a>append()和extend()两类方法的区别</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">a.append(b)</span><br><span class="line"><span class="comment">#print(a)#[1, 2, 3, [4, 5, 6]]</span></span><br><span class="line">a.extend(b)</span><br><span class="line"><span class="comment">#print(a)#[1, 2, 3, [4, 5, 6], 4, 5, 6]</span></span><br></pre></td></tr></table></figure><h2 id="按照给定特征划分数据集"><a href="#按照给定特征划分数据集" class="headerlink" title="按照给定特征划分数据集"></a>按照给定特征划分数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(myDat)</span></span><br><span class="line"><span class="comment">#print(trees.splitDataSet(myDat,0,1))</span></span><br><span class="line"><span class="comment">#print(trees.splitDataSet(myDat,0,0))</span></span><br></pre></td></tr></table></figure><h2 id="选择最好的数据集划分方式"><a href="#选择最好的数据集划分方式" class="headerlink" title="选择最好的数据集划分方式"></a>选择最好的数据集划分方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#print(myDat)</span></span><br><span class="line"><span class="comment">#print(trees.chooseBestFeatureToSplit(myDat))</span></span><br></pre></td></tr></table></figure><h2 id="创建树-参数为数据集和标签列表"><a href="#创建树-参数为数据集和标签列表" class="headerlink" title="创建树,参数为数据集和标签列表"></a>创建树,参数为数据集和标签列表</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">myTree=trees.createTree(myDat,labels)</span><br><span class="line"><span class="comment">#print(myTree)</span></span><br><span class="line"></span><br><span class="line">myDat,labels=trees.createDataSet()</span><br><span class="line">myTree1=treePlotter.retrieveTree(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#print(myTree1)</span></span><br><span class="line"><span class="comment">#print(trees.classify(myTree1,labels,[1,0]))</span></span><br><span class="line"><span class="comment">#print(trees.classify(myTree,labels,[1,1]))</span></span><br></pre></td></tr></table></figure><h2 id="决策树的存储"><a href="#决策树的存储" class="headerlink" title="决策树的存储"></a>决策树的存储</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trees.storeTree(myTree,<span class="string">'classifierStorage.txt'</span>)</span><br><span class="line"><span class="comment">#print(trees.grabTree('classifierStorage.txt'))</span></span><br></pre></td></tr></table></figure><h2 id="使用决策树预测隐形眼镜类型"><a href="#使用决策树预测隐形眼镜类型" class="headerlink" title="使用决策树预测隐形眼镜类型"></a>使用决策树预测隐形眼镜类型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fr=open(<span class="string">'lenses.txt'</span>)</span><br><span class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]  <span class="comment">#将文本数据的每一个数据行按照tab键分割，并依次存入lenses</span></span><br><span class="line">lensesLabels = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]   <span class="comment"># 创建并存入特征标签列表</span></span><br><span class="line">lensesTree = trees.createTree(lenses, lensesLabels)   <span class="comment"># 根据继续文件得到的数据集和特征标签列表创建决策树</span></span><br><span class="line">print(lensesTree)</span><br><span class="line">treePlotter.createPlot(lensesTree)</span><br></pre></td></tr></table></figure><h1 id="treePlotter-py"><a href="#treePlotter-py" class="headerlink" title="treePlotter.py"></a>treePlotter.py</h1><blockquote><p>python中使用Matplotlib注解绘制树形图</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="定义文本框和箭头格式"><a href="#定义文本框和箭头格式" class="headerlink" title="定义文本框和箭头格式"></a>定义文本框和箭头格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc=<span class="string">"0.8"</span>)  <span class="comment"># boxstyle为文本框的类型，sawtooth是锯齿形，fc是边框线粗细,pad指的是外边框锯齿形（圆形等）的大小</span></span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)    <span class="comment">#定义决策树的叶子结点的描述属性，round4表示圆形</span></span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)  <span class="comment">#定义箭头属性</span></span><br></pre></td></tr></table></figure><h2 id="plotNode-nodeTxt-centerPt-parentPt-nodeType"><a href="#plotNode-nodeTxt-centerPt-parentPt-nodeType" class="headerlink" title="plotNode(nodeTxt, centerPt, parentPt, nodeType)"></a>plotNode(nodeTxt, centerPt, parentPt, nodeType)</h2><blockquote><p>绘制带箭头的注解<br>annotate是关于一个数据点的文本<br>nodeTxt为要显示的文本，centerPt为文本的中心点，箭头所在的点，parentPt为指向文本的点<br>annotate的作用是添加注释，nodetxt是注释的内容<br>nodetype指的是输入的节点（边框）的形状</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">            xytext=centerPt, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">            va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=nodeType, arrowprops=arrow_args )</span><br></pre></td></tr></table></figure><h2 id="def-createPlot"><a href="#def-createPlot" class="headerlink" title="def createPlot():"></a>def createPlot():</h2><blockquote><p>第一版构造树函数，后面会改进，所以这里要注释上</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#fig = plt.figure(1, facecolor='white')</span></span><br><span class="line"><span class="comment">#fig.clf()</span></span><br><span class="line"><span class="comment">#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses</span></span><br><span class="line"><span class="comment">#plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)</span></span><br><span class="line"><span class="comment">#plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><h2 id="getNumLeafs-myTree"><a href="#getNumLeafs-myTree" class="headerlink" title="getNumLeafs(myTree)"></a>getNumLeafs(myTree)</h2><blockquote><p>计算叶子节点的个数<br>构造注解树，需要知道叶节点的个数，以便可以正确确定x轴的长度；要知道树的层数，可以确定y轴的高度。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span>    </span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]  <span class="comment">#获得myTree的第一个键值，即第一个特征，分割的标签</span></span><br><span class="line">    <span class="comment">#print(firstStr)</span></span><br><span class="line">    secondDict = myTree[firstStr]   <span class="comment">#根据键值得到对应的值，即根据第一个特征分类的结果</span></span><br><span class="line">    <span class="comment">#print(secondDict)</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():   <span class="comment">#获取第二个小字典中的key</span></span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">            <span class="comment">#判断是否小字典中是否还包含新的字典（即新的分支）</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])    <span class="comment">#包含的话进行递归从而继续循环获得新的分支所包含的叶节点的数量</span></span><br><span class="line">        <span class="keyword">else</span>:   numLeafs +=<span class="number">1</span>    <span class="comment">#不包含的话就停止迭代并把现在的小字典加一表示这边有一个分支</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span>   <span class="comment">#计算判断节点的个数</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">   <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">       <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">           thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">       <span class="keyword">else</span>:   thisDepth = <span class="number">1</span></span><br><span class="line">       <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br></pre></td></tr></table></figure><h2 id="retrieveTree-i"><a href="#retrieveTree-i" class="headerlink" title="retrieveTree(i)"></a>retrieveTree(i)</h2><blockquote><p>预先存储树信息</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieveTree</span><span class="params">(i)</span>:</span></span><br><span class="line">    listOfTrees =[&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                 ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br></pre></td></tr></table></figure><h2 id="plotMidText-cntrPt-parentPt-txtString"><a href="#plotMidText-cntrPt-parentPt-txtString" class="headerlink" title="plotMidText(cntrPt, parentPt, txtString)"></a>plotMidText(cntrPt, parentPt, txtString)</h2><blockquote><p>作用是计算tree的中间位置，cntrPt起始位置,parentPt终止位置,txtString文本标签信息</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>]-cntrPt[<span class="number">0</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">0</span>]  <span class="comment">#cntrPt起点坐标，子节点坐标，parentPt结束坐标，父节点坐标</span></span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>]-cntrPt[<span class="number">1</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">1</span>]  <span class="comment">#找到x和y的中间位置</span></span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">   cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs))/<span class="number">2.0</span>/plotTree.totalW, plotTree.yOff)   <span class="comment">#计算子节点的坐标</span></span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)      <span class="comment">#绘制线上的文字</span></span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)      <span class="comment">#绘制节点</span></span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span>/plotTree.totalD     <span class="comment">#每绘制一次图，将y的坐标减少1.0/plottree.totald，间接保证y坐标上深度的</span></span><br><span class="line">   <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">       <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">           plotTree(secondDict[key],cntrPt,str(key))</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           plotTree.xOff = plotTree.xOff + <span class="number">1.0</span>/plotTree.totalW</span><br><span class="line">           plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">           plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span>/plotTree.totalD</span><br></pre></td></tr></table></figure><h2 id="createPlot-inTree"><a href="#createPlot-inTree" class="headerlink" title="createPlot(inTree)"></a>createPlot(inTree)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)  <span class="comment">#类似于Matlab的figure，定义一个画布，背景为白色</span></span><br><span class="line">    fig.clf()   <span class="comment"># 把画布清空</span></span><br><span class="line">    axprops = dict(xticks=[], yticks=[])    <span class="comment">#subplot定义了一个绘图</span></span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)    <span class="comment">#no ticks</span></span><br><span class="line">    <span class="comment">#createPlot.ax1为全局变量，绘制图像的句柄，111表示figure中的图有1行1列，即1个，最后的1代表第一个图,frameon表示是否绘制坐标轴矩形</span></span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = <span class="number">-0.5</span>/plotTree.totalW; plotTree.yOff = <span class="number">1.0</span>;</span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>,<span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h1 id="treePlotter-main-py"><a href="#treePlotter-main-py" class="headerlink" title="treePlotter_main.py"></a>treePlotter_main.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  treePlotter</span><br><span class="line"><span class="comment">#treePlotter.createPlot()</span></span><br><span class="line"><span class="comment">#print(treePlotter.retrieveTree(1))</span></span><br><span class="line">myTree=treePlotter.retrieveTree(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#print(treePlotter.getNumLeafs(myTree))</span></span><br><span class="line"><span class="comment">#print(treePlotter.getTreeDepth(myTree))</span></span><br><span class="line">myTree[<span class="string">'no surfacing'</span>][<span class="number">3</span>]=<span class="string">'maybe'</span></span><br><span class="line">treePlotter.createPlot(myTree)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特诊数据&lt;br&gt;缺点：可能会产生过度匹配问题&lt;br&gt;使用数据类型：数值型和标称型&lt;br&gt;专家系统中，经常使用决策树&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://moyingyao.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="《机器学习实战》" scheme="http://moyingyao.github.io/tags/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B/"/>
    
      <category term="ML" scheme="http://moyingyao.github.io/tags/ML/"/>
    
      <category term="决策树" scheme="http://moyingyao.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战--KNN</title>
    <link href="http://moyingyao.github.io/2018/05/23/20180523%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-KNN/"/>
    <id>http://moyingyao.github.io/2018/05/23/20180523机器学习实战-KNN/</id>
    <published>2018-05-23T12:24:18.000Z</published>
    <updated>2019-03-14T08:50:40.252Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0</p></blockquote><h1 id="knn"><a href="#knn" class="headerlink" title="knn"></a>knn</h1><blockquote><p>优       点：精度高，对异常值不敏感，无数据输入假定<br>缺       点：计算复杂度高，空间复杂度高，无法给出数据的内在含义<br>使用数据范围：数值型和标称型 </p></blockquote><a id="more"></a><p><img src="https://i.imgur.com/1diiQIB.png" alt=""></p><p>————————————————————————————-下面进入正题————————————————————————————-</p><h1 id="kNN-py"><a href="#kNN-py" class="headerlink" title="kNN.py"></a>kNN.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *  </span><br><span class="line"><span class="keyword">import</span> operator  </span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br></pre></td></tr></table></figure><h2 id="createDataSet"><a href="#createDataSet" class="headerlink" title="createDataSet()"></a>createDataSet()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span>  </span><br><span class="line">    group = array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])  </span><br><span class="line">    labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]  </span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br></pre></td></tr></table></figure><h2 id="classify0"><a href="#classify0" class="headerlink" title="classify0()"></a>classify0()</h2><blockquote><p>inX用于分类的输入向量,是一个向量<br>dataSet输入的训练样本集，是一个矩阵<br>labels标签向量<br>k用于选择最近邻居的数目<br>labels数目与dataSet的行数相同 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]  <span class="comment">#返回的是dataSet的行数，行数就是样本的数量</span></span><br><span class="line">    diffMat = tile(inX, (dataSetSize,<span class="number">1</span>)) - dataSet  <span class="comment">#矩阵相减</span></span><br><span class="line">    <span class="comment">#inX是个向量，而dataset是个矩阵，两者之间要进行相减的运算，需要把这个向量也补成一个和dataset有相同行数列数的矩阵，</span></span><br><span class="line">    <span class="comment">#tile()的第二个参数，就是(datasetsize,1)，这个参数的意思就是把inX补成有datasetsize行数的矩阵。</span></span><br><span class="line">    <span class="comment">#假如inX是（1，2），datasetsize =3，那么经过tile()转换后产生了一个这样的矩阵（[1,2],[1,2],[1,2]）</span></span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span>  <span class="comment">#平方</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>) <span class="comment">#按行求和</span></span><br><span class="line">    <span class="comment"># sqdiffMat是([1,2],[0,1],[3,4])，axis这个参数影响了对矩阵求和时候的顺序，axis=0是按照列求和，结果为([3.1.7])</span></span><br><span class="line">    <span class="comment"># axis=1是按照行进行求和，结果是([4,7])。</span></span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span>    <span class="comment">#开方，得到欧氏距离</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()     <span class="comment">#把向量中每个元素进行排序，结果是元素的索引形成的向量</span></span><br><span class="line">    <span class="comment">#例子distance([1,4,3])，经过distance.argsort()之后的结果是([0,2,1]</span></span><br><span class="line">    classCount=&#123;&#125;       <span class="comment">#存放最终的分类结果及相应的结果投票数</span></span><br><span class="line">    <span class="comment">#投票过程，就是统计前k个最近的样本所属类别包含的样本个数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        <span class="comment"># index = sortedDistIndicies[i]是第i个最相近的元素索引，即样本下标</span></span><br><span class="line">        <span class="comment"># voteIlabel = labels[index]是样本index对应的分类结果('A' or 'B')</span></span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        <span class="comment"># classCount.get(voteIlabel, 0)返回voteIlabel的值，如果不存在，则返回0</span></span><br><span class="line">        <span class="comment"># 然后将票数增1</span></span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 把分类结果进行排序，然后返回得票数最多的分类结果</span></span><br><span class="line">    <span class="comment"># key=operator.itemgetter(1)的意思是按照字典里的第一个排序</span></span><br><span class="line">    <span class="comment">#例子a = [1, 2, 3]，b = operator.itemgetter(1)，b(a)返回为2</span></span><br><span class="line">    <span class="comment">#b = operator.itemgetter(1, 0)，b(a)，定义函数b，获取对象的第1个域和第0个的值，返回 (2, 1)</span></span><br><span class="line">    <span class="comment"># reverse=True是降序排序</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>] <span class="comment">#返回类别最多的类别</span></span><br></pre></td></tr></table></figure><h2 id="file2matrix"><a href="#file2matrix" class="headerlink" title="file2matrix()"></a>file2matrix()</h2><blockquote><p>将文本记录转换为NumPy<br>将文本记录转换为NumPy的解析程序<br>输入为矩阵，输出为训练样本矩阵和类标签向量</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename)     <span class="comment">#打开文档</span></span><br><span class="line">    numberOfLines = len(fr.readlines())        <span class="comment">#得到文件行数</span></span><br><span class="line">    <span class="comment">#fr.readlines()读取行数,存在数组中,导入后每行中用\t隔开,两行之间用\n换行得到文件行数</span></span><br><span class="line">    returnMat = zeros((numberOfLines,<span class="number">3</span>))        <span class="comment">#创建返回NumPy矩阵，numberoflines行，3列的初始化零的矩阵</span></span><br><span class="line">    classLabelVector = []<span class="comment">#定义一个空的数组</span></span><br><span class="line">    fr = open(filename)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        line = line.strip() <span class="comment">#删除（）中的内容，这里表示删除空格</span></span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)<span class="comment">#以\t分割</span></span><br><span class="line">        <span class="comment">#print(listFromLine)</span></span><br><span class="line">        returnMat[index,:] = listFromLine[<span class="number">0</span>:<span class="number">3</span>]<span class="comment">#把每行前三个元素存入returnMat矩阵中，每行中存储三个</span></span><br><span class="line">        classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))<span class="comment">#存储第四列元素即标签，在数组中append添加，-1表示最后一列</span></span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnMat,classLabelVector</span><br></pre></td></tr></table></figure><h2 id="autoNorm"><a href="#autoNorm" class="headerlink" title="autoNorm()"></a>autoNorm()</h2><blockquote><p>归一化数值，避免某特征值过大，使得权重比例不均匀，对计算结果产生影响。<br>autoNorm可以自动将数字特征值转化为0到1区间</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)<span class="comment">#一维数组，值为各项特征（列）中的最小值。参数0使得函数从列中选取最小值</span></span><br><span class="line">    <span class="comment">#print(minVals)</span></span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(maxVals)</span></span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">   normDataSet = zeros(shape(dataSet)) <span class="comment">#创建与样本集一样大小的零矩阵</span></span><br><span class="line">   <span class="comment">#print(normDataSet)</span></span><br><span class="line">   m = dataSet.shape[<span class="number">0</span>]<span class="comment">#dataSet的行数</span></span><br><span class="line">   normDataSet = dataSet - tile(minVals, (m,<span class="number">1</span>))<span class="comment">#矩阵中所有的值减去最小值</span></span><br><span class="line">   <span class="comment">#tile将原来的一个数组minVals，扩充成了m行1列的数组</span></span><br><span class="line">   normDataSet = normDataSet/tile(ranges, (m,<span class="number">1</span>))   <span class="comment">#矩阵中所有的值除以最大取值范围进行归一化</span></span><br><span class="line">   <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure><h2 id="datingClassTest"><a href="#datingClassTest" class="headerlink" title="datingClassTest()"></a>datingClassTest()</h2><blockquote><p>测试算法，样本集中百分之九十的数据用来训练样本，百分之十的样本用来测试分类器kNN.classify0()。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hoRatio = <span class="number">0.10</span>      <span class="comment">#百分之十的数据用于测试分类器，更改该变量的值可更改参加测试分类器的数据量</span></span><br><span class="line">    datingDataMat,datingLabels = file2matrix(<span class="string">'datingTestSet2.txt'</span>)       <span class="comment">#导入数据</span></span><br><span class="line">   normMat, ranges, minVals = autoNorm(datingDataMat)      <span class="comment">#归一化数值</span></span><br><span class="line">   m = normMat.shape[<span class="number">0</span>]    <span class="comment">#得到总行数</span></span><br><span class="line">   numTestVecs = int(m*hoRatio)    <span class="comment">#测试总数据数量，m*hoRatio是一个浮点型，需转化成整形</span></span><br><span class="line">   errorCount = <span class="number">0.0</span>    <span class="comment">#初试错误率为0</span></span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">       classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],<span class="number">3</span>)</span><br><span class="line">       <span class="comment">#分类器（需要测试的向量，训练样本集(90%)，标签集合，K）</span></span><br><span class="line">       print(<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, datingLabels[i]))</span><br><span class="line">       <span class="keyword">if</span> (classifierResult != datingLabels[i]): errorCount += <span class="number">1.0</span>     <span class="comment">#计数，错误的个数</span></span><br><span class="line">   print(<span class="string">"the total error rate is: %f"</span> % (errorCount/float(numTestVecs)))      <span class="comment">#错误率</span></span><br><span class="line">   print(errorCount)</span><br></pre></td></tr></table></figure><h2 id="classifyPerson"><a href="#classifyPerson" class="headerlink" title="classifyPerson()"></a>classifyPerson()</h2><blockquote><p>约会数据,对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span><span class="params">()</span>:</span></span><br><span class="line">    resultList=[<span class="string">'not at all'</span>,<span class="string">'in samll doses'</span>,<span class="string">'in large doses'</span>] <span class="comment">#三种感兴趣程度</span></span><br><span class="line">    percentTats=float(input(<span class="string">"percentage of time spent playing video games?"</span>))</span><br><span class="line">    ffMiles=floats=float(input(<span class="string">"frequent flier miles earned per year?"</span>))</span><br><span class="line">    iceCream=float(input(<span class="string">"liters of ice cream consuned per year?"</span>))<span class="comment">#input键盘输入</span></span><br><span class="line">    datingDataMat,datingLabels=file2matrix(<span class="string">'datingTestSet2.txt'</span>) <span class="comment"># 导入数据</span></span><br><span class="line">    normMat,ranges,minvals=autoNorm(datingDataMat) <span class="comment"># 归一化，ranges是归一化的分母</span></span><br><span class="line">    inArr=array([ffMiles,percentTats,iceCream]) <span class="comment"># inArr是归一化之前的datingDataMat数组中的行</span></span><br><span class="line">    classifierResult=classify0((inArr-minvals)/ranges,normMat,datingLabels,<span class="number">3</span>)<span class="comment">#先归一化，然后调用分类函数</span></span><br><span class="line">    <span class="comment">#print(classifierResult)</span></span><br><span class="line">    print(<span class="string">"you will probably like this person:%s"</span>%resultList[classifierResult<span class="number">-1</span>])</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">## img2vector()</span></span><br><span class="line">&gt; 图片转向量</span><br><span class="line">手写体：<span class="number">32</span>*<span class="number">32</span>的黑白图像</span><br><span class="line">图片转向量，将<span class="number">32</span>*<span class="number">32</span>的二进制图像矩阵转换为<span class="number">1</span>*<span class="number">1024</span>的向量</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#循环读出文件的前32行</span></span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):<span class="comment">#将每行的前32个字符存储在NumPy数组中</span></span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect<span class="comment">#返回数组</span></span><br></pre></td></tr></table></figure><h2 id="handwritingClassTest"><a href="#handwritingClassTest" class="headerlink" title="handwritingClassTest()"></a>handwritingClassTest()</h2><blockquote><p>手写体测试</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(<span class="string">'trainingDigits'</span>)           <span class="comment">#导入训练数据</span></span><br><span class="line">    <span class="comment">#print(trainingFileList)</span></span><br><span class="line">    m = len(trainingFileList)   <span class="comment">#训练数据的总数</span></span><br><span class="line">    <span class="comment">#print(m)</span></span><br><span class="line">    trainingMat = zeros((m,<span class="number">1024</span>))   <span class="comment">#m行1024列的零向量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">       fileNameStr = trainingFileList[i]   <span class="comment">#文件名</span></span><br><span class="line">       fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment">#取文件名.之前的名字</span></span><br><span class="line">       classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])    <span class="comment">#取文件名_之前的名字</span></span><br><span class="line">       hwLabels.append(classNumStr)</span><br><span class="line">       trainingMat[i,:] = img2vector(<span class="string">'trainingDigits/%s'</span> % fileNameStr)    <span class="comment">#将对应数据集下的文件一个个的转为向量</span></span><br><span class="line">       <span class="comment">#print(trainingMat[i,:])</span></span><br><span class="line">   testFileList = listdir(<span class="string">'testDigits'</span>)        <span class="comment">#测试数据</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">       fileNameStr = testFileList[i]</span><br><span class="line">       fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">       classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">'testDigits/%s'</span> % fileNameStr)</span><br><span class="line">       classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>) <span class="comment">#利用训练的trainingMat测试</span></span><br><span class="line">       print(<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, classNumStr))</span><br><span class="line">       <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"\nthe total number of errors is: %d"</span> % errorCount)</span><br><span class="line">    print(<span class="string">"\nthe total error rate is: %f"</span> % (errorCount/float(mTest)))</span><br></pre></td></tr></table></figure><h1 id="knn-main-py"><a href="#knn-main-py" class="headerlink" title="knn_main.py"></a>knn_main.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kNN  </span><br><span class="line"><span class="keyword">import</span> matplotlib  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> imp <span class="keyword">import</span> reload  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">group,labels=kNN.createDataSet()  </span><br><span class="line"><span class="comment">#print(group)  </span></span><br><span class="line"><span class="comment">#print(labels)  </span></span><br><span class="line"><span class="comment">#print(kNN.classify0([0,0],group,labels,3))</span></span><br></pre></td></tr></table></figure><h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig=plt.figure()                <span class="comment">#建立画板  </span></span><br><span class="line">ax=fig.add_subplot(<span class="number">111</span>)         <span class="comment">#添加一个子图，一行一列第一个子块，若括号内为349，则三行四列第9个子块  </span></span><br><span class="line">reload(kNN)  </span><br><span class="line">datingDataMat,datingLabels=kNN.file2matrix(<span class="string">'datingTestSet2.txt'</span>)  </span><br><span class="line"><span class="comment">#print(datingDataMat)  </span></span><br><span class="line"><span class="comment">#ax.scatter(datingDataMat[:,1],datingDataMat[:,2])        # scatter绘制散点图,使用第二列第三列数据  </span></span><br><span class="line"><span class="comment">#ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*np.array(datingLabels),15.0*np.array(datingLabels))  </span></span><br><span class="line">ax.scatter(datingDataMat[:,<span class="number">0</span>],datingDataMat[:,<span class="number">1</span>],<span class="number">15.0</span>*np.array(datingLabels),<span class="number">15.0</span>*np.array(datingLabels))  </span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><h2 id="归一化数值"><a href="#归一化数值" class="headerlink" title="归一化数值"></a>归一化数值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">normMat,ranges,minVals=kNN.autoNorm(datingDataMat)  </span><br><span class="line"><span class="comment">#print(normMat)  </span></span><br><span class="line"><span class="comment">#print(ranges)  </span></span><br><span class="line"><span class="comment">#print(minVals)</span></span><br></pre></td></tr></table></figure><h2 id="测试算法"><a href="#测试算法" class="headerlink" title="测试算法"></a>测试算法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kNN.datingClassTest()</span></span><br></pre></td></tr></table></figure><h2 id="约会预测"><a href="#约会预测" class="headerlink" title="约会预测"></a>约会预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对于未来的约会预测函数，输入飞行里程数，玩视频游戏的百分比和冰激凌公升数，可以得到一个是否对他感兴趣的预测，  </span></span><br><span class="line"><span class="comment">#输入10   10000   0.5  </span></span><br><span class="line"><span class="comment">#kNN.classifyPerson()</span></span><br></pre></td></tr></table></figure><h2 id="手写体"><a href="#手写体" class="headerlink" title="手写体"></a>手写体</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#trainingDigits包含大约2000个例子，每个数字约有200个样本  </span></span><br><span class="line"><span class="comment">#testDigits包含大约900个测试数据  </span></span><br><span class="line">testVector=kNN.img2vector(<span class="string">'trainingDigits/0_13.txt'</span>)  </span><br><span class="line"><span class="comment">#print(testVector[0,0:31])  </span></span><br><span class="line"><span class="comment">#print(testVector[0,32:63])  </span></span><br><span class="line"><span class="comment">#print(testVector[0,64:95])  </span></span><br><span class="line">kNN.handwritingClassTest()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;这本书的好就不多说的，其实如果不是因为机器学习那门学位课的作业是这个，我想我会错过这本书0.0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;knn&quot;&gt;&lt;a href=&quot;#knn&quot; class=&quot;headerlink&quot; title=&quot;knn&quot;&gt;&lt;/a&gt;knn&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;优       点：精度高，对异常值不敏感，无数据输入假定&lt;br&gt;缺       点：计算复杂度高，空间复杂度高，无法给出数据的内在含义&lt;br&gt;使用数据范围：数值型和标称型 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://moyingyao.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="《机器学习实战》" scheme="http://moyingyao.github.io/tags/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8B/"/>
    
      <category term="KNN" scheme="http://moyingyao.github.io/tags/KNN/"/>
    
      <category term="ML" scheme="http://moyingyao.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>伊始</title>
    <link href="http://moyingyao.github.io/2018/05/17/20180517%E4%BC%8A%E5%A7%8B/"/>
    <id>http://moyingyao.github.io/2018/05/17/20180517伊始/</id>
    <published>2018-05-17T14:27:24.000Z</published>
    <updated>2019-03-14T08:48:23.957Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈<br><a id="more"></a></p></blockquote><h2 id="简单随便说说建站方法"><a href="#简单随便说说建站方法" class="headerlink" title="简单随便说说建站方法"></a>简单随便说说建站方法</h2><h3 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h3><blockquote><p>就是以Hexo为主，剩下的自行百度吧，毕竟我要回寝室，没时间写了</p></blockquote><h3 id="Github-Pages"><a href="#Github-Pages" class="headerlink" title="Github Pages"></a>Github Pages</h3><blockquote><p>以github为载体实现的，也百度吧，啊啊啊，实验室就剩我自己了。</p></blockquote><h3 id="配置域名"><a href="#配置域名" class="headerlink" title="配置域名"></a>配置域名</h3><blockquote><p>在博客的根目录下source文件中(例如：C:\hexo\source)新建一个名为CNAME的文件，注意没有任何后缀，用于github进行读取。在文件中添加自己的域名并保存，例如</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">amberwu.top</span><br></pre></td></tr></table></figure><blockquote><p>然后，重新生成静态文件并部署。CNAME文件也会被上传到github仓库当中，此时在浏览器中输入自己的域名，回车之后，你会第一次遇见自己的小天地~</p></blockquote><h3 id="Hexo的一些基本命令"><a href="#Hexo的一些基本命令" class="headerlink" title="Hexo的一些基本命令"></a>Hexo的一些基本命令</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean <span class="comment">#用于清除配置文件</span></span><br><span class="line">hexo g <span class="comment">#完整命令为hexo generate,用于生成静态文件</span></span><br><span class="line">hexo s <span class="comment">#完整命令为hexo server,用于启动服务器，主要用来本地预览</span></span><br></pre></td></tr></table></figure><blockquote><p>在浏览器地址栏输入<a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000/</a>, 按下回车键，熟悉的界面又出现了。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo d <span class="comment">#完整命令为hexo deploy,用于将本地文件发布到github等git仓库上</span></span><br><span class="line">hexo n <span class="string">"my article"</span> <span class="comment">#完整命令为hexo new,用于新建一篇名为“my article”的文章</span></span><br></pre></td></tr></table></figure><blockquote><p>这样就会在博客目录下source_posts中生成相应的 my article.md文件( 例如 C:\blog\source_posts\my article.md )</p></blockquote><h3 id="Hexo修改及配置主题"><a href="#Hexo修改及配置主题" class="headerlink" title="Hexo修改及配置主题"></a>Hexo修改及配置主题</h3><blockquote><p>hexo初始化之后默认的主题是landscape , 然后你可以去这个地址 <a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a> 里面找到你想要的主题。在github中搜索你要的主题名称，里面都会有该主题的如何使用的介绍，按着来就好了，反正就是改改改！我选的是next,看起来挺不错，至少是我喜欢的类型。</p></blockquote><p><strong>更改主题需要修改配置文件</strong></p><blockquote><p>更改主题需要修改配置文件，就是根目录下的_config.yml文件，找到 theme 字段，并将其值更改为next即可</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: next</span><br></pre></td></tr></table></figure><p><strong>配置next主题</strong></p><blockquote><p>next主题共分三种，在站点根目录/themes/next/_congig.yml 文件中修改，找到scheme关键字即可选择。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line">scheme: Pisces</span><br></pre></td></tr></table></figure><blockquote><p>当然，你完全可以进行很多的自定义设置甚至修改源码，定制自己的主题。小女子能力有限，更多的设置请参考官方文档<a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="noopener">http://theme-next.iissnan.com/getting-started.html</a> </p></blockquote><p><strong>添加背景图片</strong></p><blockquote><p>将背景图片命名为background.jpg并放入主题根目录/source/images文件夹中<br>打开博客根目录/themes/next/source/css/_custom/custom.styl文件<br>加入如下代码：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line">body &#123; </span><br><span class="line">background-image: url(/images/background.jpg);</span><br><span class="line">background-attachment: fixed;</span><br><span class="line">background-repeat: no-repeat;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="更多资料"><a href="#更多资料" class="headerlink" title="更多资料"></a>更多资料</h3>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Welcome to AmberWu’s Blog! 为什么会想弄这么一个博客呢，还不是因为有那么一个研究僧程序猿且男屌丝，哦不不不，大神，嗯，大神0.0。第一次接触建站域名，随便弄弄。还挺有意思的，本以为这个很难，离自己很远，动起手来，真的蛮简单的，毕竟，本学渣弄得下来，哈哈哈&lt;br&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="hexo" scheme="http://moyingyao.github.io/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://moyingyao.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
